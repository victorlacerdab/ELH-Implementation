{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import owlready2 as owl\n",
    "from owlready2 import *\n",
    "import types\n",
    "owlready2.reasoning.JAVA_MEMORY = 6000\n",
    "\n",
    "import scipy\n",
    "#from scipy.spatial import ConvexHull\n",
    "#import cdd\n",
    "#from cdd import RepType, Matrix, Polyhedron\n",
    "#from fractions import Fraction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/goslimyeast.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/galennorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/gonorm.xml.owl'\n",
    "dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/family_ontology.owl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "onto = get_ontology(dir)\n",
    "onto = onto.load()\n",
    "\n",
    "individuals = list(onto.individuals())\n",
    "roles = list(onto.properties())\n",
    "classes = list(onto.classes())\n",
    "gcas = list(onto.general_class_axioms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = list(onto.classes())\n",
    "\n",
    "for cls in classes:\n",
    "    print(f'{cls} Ancestors: {cls.ancestors(include_self = True, include_constructs = True)}')\n",
    "    print(f'{cls} Constructs: {list(cls.constructs())}')\n",
    "    print(f'{cls} Properties: {list(cls.get_class_properties())}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with onto:\n",
    "    sync_reasoner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = list(onto.classes())\n",
    "\n",
    "for cls in classes:\n",
    "    print(f'{cls} Ancestors: {cls.ancestors(include_self = True, include_constructs = True)}')\n",
    "    print(f'{cls} Constructs: {list(cls.constructs())}')\n",
    "    print(f'{cls} Properties: {list(cls.get_class_properties())}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class for creating entities to\n",
    "populate the creation of the\n",
    "canonical models.\n",
    "\n",
    "The .name attribute is used to\n",
    "create a single representation\n",
    "for concepts like A and B / \n",
    "B and A, as they are the same.\n",
    "'''\n",
    "\n",
    "class CanonicalModelElements:\n",
    "\n",
    "    concept_names = {}\n",
    "    concept_intersections = {}\n",
    "    concept_restrictions = {}\n",
    "    all_concepts = {}\n",
    "\n",
    "    def __init__(self, concept):\n",
    "        self.concept = concept\n",
    "        self.name = self.get_name()\n",
    "        self.get_element_dict()\n",
    "\n",
    "    def get_name(self):\n",
    "\n",
    "        # add \\Top\n",
    "        \n",
    "        if type(self.concept) == ThingClass:\n",
    "            return self.concept.name\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            return 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "        \n",
    "        else:\n",
    "            return 'And_' + ''.join(sorted(self.concept.Classes[0].name + self.concept.Classes[1].name)) # The name is sorted to avoid that (e.g) (A \\and B) and (B \\and A) are treated as different concepts\n",
    "        \n",
    "    def get_element_dict(self):\n",
    "\n",
    "        if type(self.concept) == ThingClass:\n",
    "            CanonicalModelElements.concept_names[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            CanonicalModelElements.concept_restrictions[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == And:\n",
    "            CanonicalModelElements.concept_intersections[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_model_elements(concept_names_iter, role_names_iter, ontology):\n",
    "    \n",
    "    onto = ontology\n",
    "    top = owl.Thing\n",
    "    bottom = owl.Nothing\n",
    "\n",
    "    CanonicalModelElements(top)\n",
    "    CanonicalModelElements(bottom)\n",
    "\n",
    "    for concept_name in concept_names_iter:\n",
    "        \n",
    "        CanonicalModelElements(concept_name)\n",
    "        for concept_name2 in concept_names_iter:\n",
    "        \n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(concept_name & concept_name2)\n",
    "                gca.is_a.append(concept_name & concept_name2)\n",
    "            \n",
    "            CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('===========================================================================================================')\n",
    "    print('All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.')\n",
    "    print('===========================================================================================================')\n",
    "\n",
    "    concept_names_iter.append(top)\n",
    "    # concept_names_iter.append(bottom)\n",
    "\n",
    "    for role_name in role_names_iter:\n",
    "        for concept_name in concept_names_iter:\n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(role_name.some(concept_name))\n",
    "                gca.is_a.append(role_name.some(concept_name))\n",
    "\n",
    "            CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('All restrictions have been preprocessed for the creation of the canonical model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main class for creating the canonical model for the ontology.\n",
    "\n",
    "The canonical model is stored in dictionaries available as class variables 'concept_canonical_interpretation'\n",
    "and 'role_canonical_interpretation'. \n",
    "\n",
    "Args:\n",
    "    concept_names_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_intersection_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_restrictions_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    all_concepts_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    role_names_iter (list): a list containing all role names in the loaded ontology.\n",
    "'''\n",
    "\n",
    "class CanonicalModel:\n",
    "\n",
    "    concept_canonical_interpretation = {}\n",
    "    role_canonical_interpretation = {}\n",
    "\n",
    "    def __init__(self, concept_names_dict, concept_intersections_dict, concept_restrictions_dict, all_concepts_dict, role_names_iter):\n",
    "        \n",
    "        self.domain = all_concepts_dict\n",
    "        self.concept_names_dict = concept_names_dict\n",
    "        self.concept_restrictions_dict = concept_restrictions_dict\n",
    "        self.concept_intersections_dict = concept_intersections_dict\n",
    "\n",
    "        self.role_names_iter = role_names_iter\n",
    "\n",
    "        self.concept_canonical_interp = self.get_concept_name_caninterp() # These are only used to build the concept_canonical_interpretation and role_canonical_interpretation class attributes\n",
    "        self.role_canonical_interp = self.get_role_name_caninterp()       # The functions do not return anything, they just update their corresponding class variables \n",
    "\n",
    "    def get_concept_name_caninterp(self):\n",
    "\n",
    "        # The variable concept is a string containing the name of an element of the domain of the canonical model\n",
    "        # The key to the concept_names_dict variable corresponds to concept.name\n",
    "        # This name can be used to access the concept in owlready2's format\n",
    "\n",
    "        for concept in self.concept_names_dict.keys():\n",
    "\n",
    "            CanonicalModel.concept_canonical_interpretation[concept] = []\n",
    "            superclasses = self.domain[concept].concept.ancestors(include_self=True, include_constructs=True) # The self.domain[concept] is used to access the CanonicalModelElements type of object,\n",
    "                                                                                                               # and the attribute .concept is used to access the concept in owlready2 format\n",
    "                                                                                                              \n",
    "            for superclass in superclasses:\n",
    "\n",
    "                if type(superclass) == ThingClass:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append(superclass.name)\n",
    "\n",
    "                elif type(superclass) == Restriction:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append('exists_' + superclass.property.name + '.' + superclass.value.name)\n",
    "\n",
    "                elif type(superclass) == And:\n",
    "                    if 'And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)) in CanonicalModel.concept_canonical_interpretation[concept]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        CanonicalModel.concept_canonical_interpretation[concept].append('And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)))\n",
    "\n",
    "            \n",
    "    def get_role_name_caninterp(self):\n",
    "\n",
    "        # Second case from Definition 10\n",
    "\n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            role_name_str = role_name.name # Accesses the property type object's name as a string\n",
    "            CanonicalModel.role_canonical_interpretation[role_name_str] = []\n",
    "\n",
    "            for restriction_name in self.concept_restrictions_dict.keys(): # Where restriction_name denotes a \\exists r.B type of concept 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "                c_B = self.concept_restrictions_dict[restriction_name].concept.value.name\n",
    "\n",
    "                if role_name_str in restriction_name:\n",
    "                    \n",
    "                    superclasses = self.domain[restriction_name].concept.ancestors(include_self=True, include_constructs=False) # Include_constructs is turned to false due to the definition of canonical model\n",
    "\n",
    "                    for superclass in superclasses:\n",
    "                        super_superclasses = superclass.ancestors(include_self=True, include_constructs=True)\n",
    "\n",
    "                        for super_superclass in super_superclasses:\n",
    "\n",
    "                            if type(super_superclass) == ThingClass:\n",
    "                                c_D = super_superclass.name\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "                            elif type(super_superclass) == Restriction:\n",
    "                                c_D = 'exists_' + super_superclass.property.name + '.' + super_superclass.value.name\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "                            elif type(super_superclass) == And:\n",
    "                                c_D = 'And_' + ''.join(sorted(super_superclass.Classes[0].name + super_superclass.Classes[1].name))\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "        # First case from Definition 10\n",
    "\n",
    "        for restriction_name in self.concept_restrictions_dict.keys():\n",
    "\n",
    "            concept_name_str = self.concept_restrictions_dict[restriction_name].concept.value.name\n",
    "            role_name_str = self.concept_restrictions_dict[restriction_name].concept.property.name\n",
    "\n",
    "            role_name = self.concept_restrictions_dict[restriction_name].concept.property\n",
    "\n",
    "            superroles = list(role_name.ancestors(include_self=True))\n",
    "            \n",
    "            pair = (restriction_name, concept_name_str)\n",
    "\n",
    "            for superrole in superroles:\n",
    "                if superrole.name in CanonicalModel.role_canonical_interpretation.keys():\n",
    "                    CanonicalModel.role_canonical_interpretation[superrole.name].append(pair)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "        # Second case from Definition 10: r \\sqsubset s \n",
    "            \n",
    "        #for role_name in self.role_names_iter:\n",
    "\n",
    "        #    superroles = role_name.ancestors(include_self=True)\n",
    "        #    role_name_str = role_name.name\n",
    "            \n",
    "        #    for superrole in superroles:\n",
    "        #        for restriction_name in self.concept_restrictions_dict.keys():\n",
    "        #            if superrole.name in restriction_name:\n",
    "        #                pair = tuple((restriction_name, self.domain[restriction_name].concept.value.name))\n",
    "        #                CanonicalModel.role_canonical_interpretation[role_name_str].append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main function for creating the canonical model.\n",
    "\n",
    "    Args:\n",
    "        onto_dir (str): a string pointing to the directory where the ontology is stored.\n",
    "\n",
    "    Returns:\n",
    "        canmodel (CanonicalModel): returns a variable containing the canonical model. \n",
    "        \n",
    "        Attention: the interpretations of concept names and role names can also be accessed via class variables\n",
    "        from the CanonicalModel class.\n",
    "'''\n",
    "\n",
    "def create_canonical_model(onto_dir):\n",
    "\n",
    "    onto = get_ontology(onto_dir)\n",
    "    onto = onto.load()\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "    individuals_iter = list(onto.individuals())\n",
    "\n",
    "    get_canonical_model_elements(concept_names_iter, role_names_iter, onto)\n",
    "\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Starting to reason.')\n",
    "\n",
    "    with onto:\n",
    "        sync_reasoner()\n",
    "        \n",
    "    #onto.save(\"inferences_goslimyeast.owl\")\n",
    "\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "    individuals_iter = list(onto.individuals())\n",
    "\n",
    "    print('')\n",
    "    print('============================================================================')\n",
    "    print('Done reasoning. Creating the canonical model.')\n",
    "    canmodel = CanonicalModel(CanonicalModelElements.concept_names, CanonicalModelElements.concept_intersections, CanonicalModelElements.concept_restrictions, CanonicalModelElements.all_concepts, role_names_iter)\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Concluded creating canonical model.')\n",
    "\n",
    "    return canmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================================================\n",
      "All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.\n",
      "===========================================================================================================\n",
      "\n",
      "\n",
      "All restrictions have been preprocessed for the creation of the canonical model.\n",
      "\n",
      "============================================================================\n",
      "\n",
      "Starting to reason.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx6000M -cp /opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit:/opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////var/folders/wg/g5861gcs6k5d3rbq_rncztjw0000gn/T/tmppuxqy85y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================\n",
      "\n",
      "Done reasoning. Creating the canonical model.\n",
      "\n",
      "============================================================================\n",
      "\n",
      "Concluded creating canonical model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * HermiT took 0.5708737373352051 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the canonical model\n",
    "\n",
    "canmodel = create_canonical_model(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for initializing\n",
    "the class EntityEmbedding. They\n",
    "allow us to access dictionaries\n",
    "containing indexes and canonical\n",
    "interpretation of concepts\n",
    "and roles as class.\n",
    "'''\n",
    "\n",
    "def get_concept_names_idx_dict(canmodel):\n",
    "   conceptnames_idx_dict = {concept_name: idx for idx, concept_name in enumerate(CanonicalModel.concept_canonical_interpretation.keys())}\n",
    "   return conceptnames_idx_dict\n",
    "\n",
    "def get_role_names_idx_dict(canmodel):\n",
    "    rolenames_idx_dict = {role_name: idx for idx, role_name in enumerate(CanonicalModel.role_canonical_interpretation.keys())}\n",
    "    return rolenames_idx_dict\n",
    "\n",
    "def get_entities_idx_dict(canmodel):\n",
    "    entities_idx_dict = {entity: idx for idx, entity in enumerate(canmodel.domain.keys())}\n",
    "    return entities_idx_dict\n",
    "\n",
    "def get_domain_dict(canmodel):\n",
    "    return canmodel.domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atenção: a função mu está com complexidade alta devido aos for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Class for obtaining the positional \n",
    "embedding for each entity in the domain\n",
    "of the canonical interpretation.\n",
    "It represents the Mu Function from the\n",
    "paper.\n",
    "'''\n",
    "\n",
    "class EntityEmbedding:\n",
    "\n",
    "    # Dictionaries for storing the indices of concept names and role names, entities pairs, respectively\n",
    "    # Keys are strings and values are integers\n",
    "    \n",
    "    concept_names_idx_dict = get_concept_names_idx_dict(canmodel)\n",
    "    role_names_idx_dict = get_role_names_idx_dict(canmodel)\n",
    "    entities_idx_dict = get_entities_idx_dict(canmodel)\n",
    "\n",
    "    # Dictionaries accessing the canonical interpretation of concepts and roles\n",
    "    # Keys and values are strings\n",
    "    \n",
    "    concept_canonical_interpretation_dict = CanonicalModel.concept_canonical_interpretation\n",
    "    role_canonical_interpretation_dict = CanonicalModel.role_canonical_interpretation\n",
    "\n",
    "    # Dictionary storing the domain of the canonical model being embedded\n",
    "    # IMPORTANT: Keys are strings and values are CanonicalModelElements type objects\n",
    "    \n",
    "    domain_dict = get_domain_dict(canmodel)\n",
    "\n",
    "    # Dictionary for easy access to entity embeddings\n",
    "    # It is initialized with empty values, iteratively built by the .get_embedding_vector() method\n",
    "    # Key (str): Domain Entity / Value (np.array): EntityEmbedding.embedding_vector\n",
    "\n",
    "    entity_entityvector_dict = dict.fromkeys(domain_dict.keys())\n",
    "\n",
    "    def __init__(self, entity_name, emb_dim):\n",
    "        self.name = entity_name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.in_interpretation_of = []\n",
    "        self.embedding_vector = self.get_embedding_vector()\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        \n",
    "        embedding_vector = np.zeros((self.emb_dim,))\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = []\n",
    "\n",
    "        # Applies the embedding function to the concept names portion of the definition\n",
    "\n",
    "        for concept_name in EntityEmbedding.concept_canonical_interpretation_dict:\n",
    "            concept_name_idx = EntityEmbedding.concept_names_idx_dict[concept_name]\n",
    "        \n",
    "            if self.name in EntityEmbedding.concept_canonical_interpretation_dict[concept_name]:\n",
    "                embedding_vector[concept_name_idx] = 1\n",
    "                self.in_interpretation_of.append(concept_name)\n",
    "\n",
    "        # Applies the embedding function to the role names portion of the definition\n",
    "\n",
    "        for role_name in EntityEmbedding.role_canonical_interpretation_dict:\n",
    "            \n",
    "            role_name_idx = len(EntityEmbedding.concept_names_idx_dict) + (EntityEmbedding.role_names_idx_dict[role_name] * len(EntityEmbedding.entities_idx_dict))\n",
    "            role_name_caninterp = EntityEmbedding.role_canonical_interpretation_dict[role_name]\n",
    "\n",
    "            for pair in role_name_caninterp:\n",
    "\n",
    "                entity_2 = pair[1]\n",
    "\n",
    "                if (self.name, entity_2) == pair:\n",
    "                    entity_2_idx = EntityEmbedding.entities_idx_dict[entity_2]\n",
    "                    final_role_entity_pair_idx = role_name_idx + entity_2_idx\n",
    "                    embedding_vector[final_role_entity_pair_idx] = 1\n",
    "\n",
    "        # EntityEmbedding.entity_entityvector_dict[self.name].append(embedding_vector)\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = embedding_vector\n",
    "\n",
    "        return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the binary vectors representing\n",
    "each element of the domain of the canonical interpretation.\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int/float): the number of dimensions of the embedding space.\n",
    "\n",
    "    Returns:\n",
    "        embedded_entities (list): a list containing all embeddings of the entities\n",
    "                                  in the domain. \n",
    "    \n",
    "    The embedded_entities are also available in the dictionary EntityEmbeddings.entity_entityvector_dict\n",
    "'''\n",
    "\n",
    "def get_domain_embeddings(emb_dim):\n",
    "\n",
    "    embedded_entities = []\n",
    "    counter = 0\n",
    "\n",
    "   # The entities in the domain are strings\n",
    "    \n",
    "    for entity_name in EntityEmbedding.domain_dict:\n",
    "       embedded_entity = EntityEmbedding(entity_name, emb_dim)\n",
    "       embedded_entities.append(embedded_entity)\n",
    "       counter += 1\n",
    "       \n",
    "       if counter % 1000 == 0:\n",
    "           print(counter)\n",
    "       \n",
    "    return embedded_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final class for creating the dataset.\n",
    "\n",
    "Inputs: concept or role names, generated\n",
    "embeddings for entities in the domain of\n",
    "the canonical model.\n",
    "\n",
    "Outputs: geometrical interpretation of\n",
    "concepts and role names, represented\n",
    "by vertices defining a region.\n",
    "\n",
    "One can access the GeometricInterpretation\n",
    "objects either as elements in a list, or as\n",
    "values in a class variable dictionary.\n",
    "'''\n",
    "\n",
    "class GeometricInterpretation:\n",
    "\n",
    "    concept_geointerps_dict = dict.fromkeys(CanonicalModel.concept_canonical_interpretation.keys())\n",
    "    role_geointerps_dict = dict.fromkeys(CanonicalModel.role_canonical_interpretation.keys())\n",
    "\n",
    "    def __init__(self, name, emb_dim):\n",
    "        self.name = name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vertices = []\n",
    "        self.centroid = None\n",
    "\n",
    "    def get_centroid_naive(self):\n",
    "        if len(self.vertices) == 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) == 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim * 2,)) # The centroid for the regions needs to be doubled due to the concat operation\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There has to be a more efficient way of doing the creating of geometric interpretations for concepts and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_finder(emb_dim, concept_name_idx_dict, role_name_idx_dict, domain_idx_dict):\n",
    "\n",
    "    index_dict = {k: None for k in range(emb_dim)}\n",
    "\n",
    "    for k,v in concept_name_idx_dict.items():\n",
    "\n",
    "        index_dict[v] = k\n",
    "\n",
    "    for role in role_name_idx_dict:\n",
    "        role_init_idx = len(concept_name_idx_dict) + (role_name_idx_dict[role] * len(domain_idx_dict))\n",
    "\n",
    "        for entity in domain_idx_dict:\n",
    "            entity_init_idx = domain_idx_dict[entity]\n",
    "            final_role_entity_pair_idx = role_init_idx + entity_init_idx\n",
    "            index_dict[final_role_entity_pair_idx] = (role, entity)\n",
    "        \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "\n",
    "    faithful_concept_geometric_interps = []\n",
    "\n",
    "    for concept_name in concept_names_interps.keys():\n",
    "        concept_name = GeometricInterpretation(concept_name, emb_dim)\n",
    "\n",
    "        for embedding in domain_embeddings_list:\n",
    "            if concept_name.name in embedding.in_interpretation_of:\n",
    "                concept_name.vertices.append(embedding.embedding_vector)\n",
    "            \n",
    "        GeometricInterpretation.concept_geointerps_dict[concept_name.name] = concept_name\n",
    "        concept_name.centroid = concept_name.get_centroid_naive()\n",
    "        \n",
    "        faithful_concept_geometric_interps.append(concept_name)\n",
    "\n",
    "    return faithful_concept_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_role_geometric_interps(role_names_interps, entity_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "    \n",
    "    faithful_role_geometric_interps = []\n",
    "    idx_entity_dict = entity_dims_index_dict\n",
    "    #entity_idx_dict = {v: k for k,v in entity_dims_index_dict}\n",
    "\n",
    "    relevant_idxs = len(canmodel.concept_names_dict)-1\n",
    "\n",
    "    for role_name in role_names_interps.keys():\n",
    "        role_name_str = role_name\n",
    "        role_name = GeometricInterpretation(role_name_str, emb_dim)\n",
    "\n",
    "        for entity in entity_embeddings_list:\n",
    "\n",
    "            onehot_idx_list = np.where(entity.embedding_vector == 1)[0]\n",
    "            #print(f'This is the entity: {entity} and this is the onehot_idx_list: {onehot_idx_list}')\n",
    "\n",
    "            for idx in onehot_idx_list: # I could just look at the TRULY relevant indexes\n",
    "                if idx > relevant_idxs:\n",
    "                    role_entity_pair = idx_entity_dict[idx]\n",
    "                    r_name_str = role_entity_pair[0]\n",
    "                    e_name_str = role_entity_pair[1]\n",
    "\n",
    "                    if r_name_str == role_name_str:\n",
    "                        e_embedding = EntityEmbedding.entity_entityvector_dict[e_name_str]\n",
    "                        role_name.vertices.append(np.concatenate((entity.embedding_vector, e_embedding)))\n",
    "\n",
    "        GeometricInterpretation.role_geointerps_dict[role_name_str] = role_name\n",
    "        role_name.centroid = role_name.get_centroid_naive()\n",
    "        faithful_role_geometric_interps.append(role_name)\n",
    "\n",
    "    return faithful_role_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exists_P22.Father': <__main__.CanonicalModelElements at 0x15cebd710>,\n",
       " 'exists_P22.Mother': <__main__.CanonicalModelElements at 0x15cf0ff90>,\n",
       " 'exists_P22.Spouse': <__main__.CanonicalModelElements at 0x15c9263d0>,\n",
       " 'exists_P22.Sibling': <__main__.CanonicalModelElements at 0x15cdcdb50>,\n",
       " 'exists_P22.Child': <__main__.CanonicalModelElements at 0x15cea95d0>,\n",
       " 'exists_P22.Q5': <__main__.CanonicalModelElements at 0x15ceaa250>,\n",
       " 'exists_P22.Female': <__main__.CanonicalModelElements at 0x15cea9450>,\n",
       " 'exists_P22.Male': <__main__.CanonicalModelElements at 0x15c779150>,\n",
       " 'exists_P22.Thing': <__main__.CanonicalModelElements at 0x15ceab910>,\n",
       " 'exists_P25.Father': <__main__.CanonicalModelElements at 0x15c9117d0>,\n",
       " 'exists_P25.Mother': <__main__.CanonicalModelElements at 0x15cee6210>,\n",
       " 'exists_P25.Spouse': <__main__.CanonicalModelElements at 0x15cee64d0>,\n",
       " 'exists_P25.Sibling': <__main__.CanonicalModelElements at 0x15cf0f850>,\n",
       " 'exists_P25.Child': <__main__.CanonicalModelElements at 0x15cf0f650>,\n",
       " 'exists_P25.Q5': <__main__.CanonicalModelElements at 0x15cf0f450>,\n",
       " 'exists_P25.Female': <__main__.CanonicalModelElements at 0x15cf0f190>,\n",
       " 'exists_P25.Male': <__main__.CanonicalModelElements at 0x15cf0f090>,\n",
       " 'exists_P25.Thing': <__main__.CanonicalModelElements at 0x15cf0ec50>,\n",
       " 'exists_P26.Father': <__main__.CanonicalModelElements at 0x15cf0e750>,\n",
       " 'exists_P26.Mother': <__main__.CanonicalModelElements at 0x15cf0e610>,\n",
       " 'exists_P26.Spouse': <__main__.CanonicalModelElements at 0x15cf0e290>,\n",
       " 'exists_P26.Sibling': <__main__.CanonicalModelElements at 0x15cf0dc50>,\n",
       " 'exists_P26.Child': <__main__.CanonicalModelElements at 0x15cf0dad0>,\n",
       " 'exists_P26.Q5': <__main__.CanonicalModelElements at 0x15cf0d710>,\n",
       " 'exists_P26.Female': <__main__.CanonicalModelElements at 0x15cf0d550>,\n",
       " 'exists_P26.Male': <__main__.CanonicalModelElements at 0x15cf0d110>,\n",
       " 'exists_P26.Thing': <__main__.CanonicalModelElements at 0x15cf0ccd0>,\n",
       " 'exists_P3373.Father': <__main__.CanonicalModelElements at 0x15cf0c9d0>,\n",
       " 'exists_P3373.Mother': <__main__.CanonicalModelElements at 0x15cf0cb10>,\n",
       " 'exists_P3373.Spouse': <__main__.CanonicalModelElements at 0x15cf0d1d0>,\n",
       " 'exists_P3373.Sibling': <__main__.CanonicalModelElements at 0x15cf0c790>,\n",
       " 'exists_P3373.Child': <__main__.CanonicalModelElements at 0x15cf0c510>,\n",
       " 'exists_P3373.Q5': <__main__.CanonicalModelElements at 0x15cb6f110>,\n",
       " 'exists_P3373.Female': <__main__.CanonicalModelElements at 0x15cf0c110>,\n",
       " 'exists_P3373.Male': <__main__.CanonicalModelElements at 0x15cedbf90>,\n",
       " 'exists_P3373.Thing': <__main__.CanonicalModelElements at 0x15ceda010>,\n",
       " 'exists_P40.Father': <__main__.CanonicalModelElements at 0x15ce74c90>,\n",
       " 'exists_P40.Mother': <__main__.CanonicalModelElements at 0x13fdf8ed0>,\n",
       " 'exists_P40.Spouse': <__main__.CanonicalModelElements at 0x108384890>,\n",
       " 'exists_P40.Sibling': <__main__.CanonicalModelElements at 0x15cec9410>,\n",
       " 'exists_P40.Child': <__main__.CanonicalModelElements at 0x15cec8f90>,\n",
       " 'exists_P40.Q5': <__main__.CanonicalModelElements at 0x15cec9110>,\n",
       " 'exists_P40.Female': <__main__.CanonicalModelElements at 0x15cec9850>,\n",
       " 'exists_P40.Male': <__main__.CanonicalModelElements at 0x15ceca290>,\n",
       " 'exists_P40.Thing': <__main__.CanonicalModelElements at 0x15cee0250>}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canmodel.concept_restrictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tbox_embeddings(canonical_model: CanonicalModel):\n",
    "\n",
    "    domain = canonical_model.domain # Keys are strings and values are CanonicalModelElements type objects\n",
    "    concept_names_interps = canonical_model.concept_canonical_interpretation # Keys are strings and values are lists of strings.\n",
    "    role_names_interps = canonical_model.role_canonical_interpretation # Keys are strings and values are lists of tuples. Tuples are of form ('C', 'D'), with C and D strings.\n",
    "\n",
    "    EMB_DIM = len(concept_names_interps) + len(role_names_interps) * len(domain)\n",
    "\n",
    "    print('================EMBEDDING DIMENSION================')\n",
    "    print(f'Concept Name dimensions: {len(concept_names_interps)}')\n",
    "    print(f'The number of role names is: {len(role_names_interps)}')\n",
    "    print(f'The size of the domain is: {len(domain)}')\n",
    "    print(f'Role names dimensions: {len(role_names_interps) * len(domain)}')\n",
    "    print('===================================================')\n",
    "    print('')\n",
    "    print(f'Final embedding dimension: {EMB_DIM}')\n",
    "    print(f'The final dimension for role regions is: {EMB_DIM * 2}')\n",
    "\n",
    "    domain_embeddings_list = get_domain_embeddings(EMB_DIM)\n",
    "    \n",
    "    concept_names_ordering = EntityEmbedding.concept_names_idx_dict\n",
    "    role_names_ordering = EntityEmbedding.role_names_idx_dict\n",
    "    entities_ordering = EntityEmbedding.entities_idx_dict\n",
    "    \n",
    "    print('')\n",
    "    print('===============FINISHED EMBEDDINGS===============')\n",
    "    print(f'There are {len(domain_embeddings_list)} vector embeddings.')\n",
    "    print('')\n",
    "\n",
    "    index_finder_dict = index_finder(EMB_DIM, concept_names_ordering, role_names_ordering, entities_ordering)\n",
    "\n",
    "    faithful_concept_geometric_interps = get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('============FINISHED INTERPS CONCEPT=============')\n",
    "    print(f'There are {len(faithful_concept_geometric_interps)} regions for concept names.')\n",
    "    print('')\n",
    "\n",
    "    faithful_role_geometric_interps = get_faithful_role_geometric_interps(role_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('=============FINISHED INTERPS ROLES==============')\n",
    "    print(f'There are {len(faithful_role_geometric_interps)} regions for role names.')\n",
    "    print('')\n",
    "\n",
    "    return domain_embeddings_list, faithful_concept_geometric_interps, faithful_role_geometric_interps, index_finder_dict, int(EMB_DIM) # Returns the faithful geometric interpretations for concepts and roles as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================EMBEDDING DIMENSION================\n",
      "Concept Name dimensions: 10\n",
      "The number of role names is: 5\n",
      "The size of the domain is: 91\n",
      "Role names dimensions: 455\n",
      "===================================================\n",
      "\n",
      "Final embedding dimension: 465\n",
      "The final dimension for role regions is: 930\n",
      "\n",
      "===============FINISHED EMBEDDINGS===============\n",
      "There are 91 vector embeddings.\n",
      "\n",
      "============FINISHED INTERPS CONCEPT=============\n",
      "There are 10 regions for concept names.\n",
      "\n",
      "=============FINISHED INTERPS ROLES==============\n",
      "There are 5 regions for role names.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "domain_embeddings, concept_geointerps, role_geointerps, idx_finder_dict, EMB_DIM = create_tbox_embeddings(canmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P22': [('exists_P22.Father', 'Father'),\n",
       "  ('exists_P22.Mother', 'Mother'),\n",
       "  ('exists_P22.Spouse', 'Spouse'),\n",
       "  ('exists_P22.Sibling', 'Sibling'),\n",
       "  ('exists_P22.Child', 'Child'),\n",
       "  ('exists_P22.Q5', 'Q5'),\n",
       "  ('exists_P22.Female', 'Female'),\n",
       "  ('exists_P22.Male', 'Male'),\n",
       "  ('exists_P22.Thing', 'Thing')],\n",
       " 'P25': [('exists_P25.Father', 'Father'),\n",
       "  ('exists_P25.Mother', 'Mother'),\n",
       "  ('exists_P25.Spouse', 'Spouse'),\n",
       "  ('exists_P25.Sibling', 'Sibling'),\n",
       "  ('exists_P25.Child', 'Child'),\n",
       "  ('exists_P25.Q5', 'Q5'),\n",
       "  ('exists_P25.Female', 'Female'),\n",
       "  ('exists_P25.Male', 'Male'),\n",
       "  ('exists_P25.Thing', 'Thing')],\n",
       " 'P26': [('exists_P26.Father', 'Father'),\n",
       "  ('exists_P26.Mother', 'Mother'),\n",
       "  ('exists_P26.Spouse', 'Spouse'),\n",
       "  ('exists_P26.Sibling', 'Sibling'),\n",
       "  ('exists_P26.Child', 'Child'),\n",
       "  ('exists_P26.Q5', 'Q5'),\n",
       "  ('exists_P26.Female', 'Female'),\n",
       "  ('exists_P26.Male', 'Male'),\n",
       "  ('exists_P26.Thing', 'Thing')],\n",
       " 'P3373': [('exists_P3373.Father', 'Father'),\n",
       "  ('exists_P3373.Mother', 'Mother'),\n",
       "  ('exists_P3373.Spouse', 'Spouse'),\n",
       "  ('exists_P3373.Sibling', 'Sibling'),\n",
       "  ('exists_P3373.Child', 'Child'),\n",
       "  ('exists_P3373.Q5', 'Q5'),\n",
       "  ('exists_P3373.Female', 'Female'),\n",
       "  ('exists_P3373.Male', 'Male'),\n",
       "  ('exists_P3373.Thing', 'Thing')],\n",
       " 'P40': [('exists_P40.Father', 'Father'),\n",
       "  ('exists_P40.Mother', 'Mother'),\n",
       "  ('exists_P40.Spouse', 'Spouse'),\n",
       "  ('exists_P40.Sibling', 'Sibling'),\n",
       "  ('exists_P40.Child', 'Child'),\n",
       "  ('exists_P40.Q5', 'Q5'),\n",
       "  ('exists_P40.Female', 'Female'),\n",
       "  ('exists_P40.Male', 'Male'),\n",
       "  ('exists_P40.Thing', 'Thing')]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CanonicalModel.role_canonical_interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the pre-split dataset containing facts from the ontology.\n",
    "Distinguishes between concept assertions and role assertions.\n",
    "\n",
    "\n",
    "    Args: ontology_dir (str): the directory from the ontology\n",
    "          concept_geointerps_dict (dict): the geometrical interpretations for concepts generated by create_tbox_embeddings()\n",
    "          role_geointerps_dict (dict): the geometrical interpretations for roles generated by create_tbox_embeddings()\n",
    "\n",
    "    Returns:\n",
    "          X_concepts (np.array): A dataframe with columns 'Concept', 'Entity', 'y_true' (equivalent to concept.centroid())\n",
    "          X_roles (np.array): A dataframe with columns 'SubjectEntity', 'Role', 'ObjectEntity', 'y_true' (equivalent to role.centroid())\n",
    "          y_concepts (np.array):\n",
    "          y_roles (np.array):\n",
    "          vocabulary_dict (dict): A vocabulary with key (int): value (str) for entities in the domain.\n",
    "'''\n",
    "\n",
    "def get_abox_dataset(ontology_dir: str,\n",
    "                     concept_geointerps_dict: dict, role_geointerps_dict: dict,\n",
    "                     concept_to_idx: dict, role_to_idx: dict,\n",
    "                     index_finder_dict: dict, emb_dim = int):\n",
    "    \n",
    "    ontology = get_ontology(ontology_dir)\n",
    "    ontology = ontology.load()\n",
    "    \n",
    "    X_concepts = []\n",
    "    X_roles = []\n",
    "    y_concepts = []\n",
    "    y_roles = []\n",
    "\n",
    "    entities = [entity.name for entity in list(ontology.individuals())]\n",
    "    \n",
    "    concept_to_idx_vocab = concept_to_idx\n",
    "    idx_to_concept_vocab = {value: key for key, value in concept_to_idx_vocab.items()}\n",
    "\n",
    "    role_to_idx_vocab = role_to_idx\n",
    "    idx_to_role_vocab = {value: key for key, value in role_to_idx_vocab.items()}\n",
    "    \n",
    "    entity_to_idx_vocab = {value: index for index, value in enumerate(entities)}\n",
    "    idx_to_entity_vocab = {value: key for key, value in entity_to_idx_vocab.items()}\n",
    "\n",
    "    for individual in list(ontology.individuals()):\n",
    "\n",
    "        all_facts = individual.INDIRECT_is_a # Is this actually getting all assertions?\n",
    "        #all_facts = individual.is_a\n",
    "\n",
    "        for concept in all_facts:\n",
    "            # Handles concepts of the type A\n",
    "            if type(concept) == ThingClass:\n",
    "                concept = concept_geointerps_dict[concept.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "                \n",
    "            # Handles concepts of the type A \\and B\n",
    "            elif type(concept) == And:\n",
    "                concept1 = concept_geointerps_dict[concept.Classes[0]]\n",
    "                concept2 = concept_geointerps_dict[concept.Classes[1]]\n",
    "                intersection_name = 'And_' + ''.join(sorted(concept1.name + concept2.name))\n",
    "\n",
    "                if concept_to_idx_vocab.get(intersection_name) is not None:\n",
    "                    fact = np.array([concept_to_idx_vocab[intersection_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "                else:\n",
    "                    concept_to_idx_vocab[intersection_name] = len(concept_to_idx_vocab)\n",
    "                    idx_to_concept_vocab[len(concept_to_idx_vocab)] = intersection_name\n",
    "                    fact = np.array([concept_to_idx_vocab[intersection_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "            \n",
    "            # Handles concepts of the type \\exists r.B\n",
    "            elif type(concept) == Restriction:\n",
    "                print('restriction')\n",
    "                concept_name = concept.value\n",
    "                role_name = concept.property\n",
    "                restriction_name = 'exists_' + role_name.name + '.' + concept_name.name\n",
    "\n",
    "                if concept_to_idx_vocab.get(restriction_name) is not None:\n",
    "                    fact = np.array([concept_to_idx_vocab[restriction_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array(concept.centroid)\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "                else:\n",
    "                    concept_to_idx_vocab[restriction_name] = len(concept_to_idx_vocab)\n",
    "                    idx_to_concept_vocab[len(concept_to_idx_vocab)] = restriction_name\n",
    "                    restriction_concept = GeometricInterpretation(restriction_name, EMB_DIM)\n",
    "                    restriction_concept.vertices = get_restriction_vertices(restriction_concept, concept_geointerps_dict,\n",
    "                                                                            role_geointerps_dict, index_finder_dict)\n",
    "\n",
    "                    fact = np.array([concept_to_idx_vocab[concept_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = None # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "        relevant_roles = individual.get_properties()\n",
    "        individual_name = individual.name\n",
    "\n",
    "        for role in relevant_roles:\n",
    "            role_geo = role_geointerps_dict[role.name]\n",
    "            subject_list = role[individual] # This syntax is from the owlready2 library\n",
    "            for subject in subject_list:\n",
    "                fact = np.array([entity_to_idx_vocab[individual.name], role_to_idx_vocab[role.name], entity_to_idx_vocab[subject.name]])\n",
    "                X_roles.append(fact)\n",
    "                y_label = y_roles.append(np.array(role_geo.centroid))\n",
    "\n",
    "    return np.array(X_concepts), np.array(X_roles), np.array(y_concepts), np.array(y_roles), entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concepts, X_roles, y_concepts, y_roles, entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab = get_abox_dataset(dir,\n",
    "                                                                                                                                                                                        GeometricInterpretation.concept_geointerps_dict,\n",
    "                                                                                                                                                                                        GeometricInterpretation.role_geointerps_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.concept_names_idx_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.role_names_idx_dict,\n",
    "                                                                                                                                                                                        idx_finder_dict,\n",
    "                                                                                                                                                                                        EMB_DIM,\n",
    "                                                                                                                                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = torch.tensor(data, dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].long(), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE_PROPORTION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "ConceptDataset = OntologyDataset(X_concepts, y_concepts)\n",
    "\n",
    "dataset_size = len(ConceptDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainConceptDataset, testConceptDataset = torch.utils.data.random_split(ConceptDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "RoleDataset = OntologyDataset(X_roles, y_roles)\n",
    "\n",
    "dataset_size = len(RoleDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainRoleDataset, testRoleDataset = torch.utils.data.random_split(RoleDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ConceptDataLoader = DataLoader(trainConceptDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_ConceptDataLoader = DataLoader(testConceptDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_RoleDataLoader = DataLoader(trainRoleDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_RoleDataLoader = DataLoader(testRoleDataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithEL(nn.Module):\n",
    "    def __init__(self, emb_dim, phi, individual_vocabulary, concept_vocabulary, role_vocabulary, init_concept_param_to_centroid = False, init_role_param_to_centroid = False):\n",
    "        super(FaithEL, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.phi = phi\n",
    "        \n",
    "        self.individual_embedding_dict = nn.Embedding(len(individual_vocabulary),\n",
    "                                                      emb_dim\n",
    "                                                      )\n",
    "        \n",
    "        self.concept_embedding_dict = nn.Embedding(len(concept_vocabulary),\n",
    "                                                   emb_dim\n",
    "                                                   )\n",
    "        \n",
    "        # Initializes the moving parameter for concepts at the concept's respective centroid\n",
    "        if init_concept_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for concept_name, concept_idx in concept_vocabulary.items():\n",
    "                    self.concept_embedding_dict.weight[concept_idx] = torch.tensor(GeometricInterpretation.concept_geointerps_dict[concept_name].centroid)\n",
    "\n",
    "        self.role_embedding_dict = nn.Embedding(len(role_vocabulary),\n",
    "                                                emb_dim * 2\n",
    "                                                )\n",
    "        \n",
    "        # Initializes the moving parameter for roles at the role's respective centroid\n",
    "        if init_role_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for role_name, role_idx in role_vocabulary.items():\n",
    "                    self.role_embedding_dict.weight[role_idx] = torch.tensor(GeometricInterpretation.role_geointerps_dict[role_name].centroid)\n",
    "    \n",
    "    def forward(self, data):\n",
    "    \n",
    "        # Concept assertions are of the form ['Concept', 'Entity']\n",
    "        # Role assertions are of the form ['SubjectEntity', 'Role', 'ObjectEntity']\n",
    "        \n",
    "        subj_entity_idx = 1 if len(data[0]) == 2 else 0 # Checks whether the model has received a C assert or R assert\n",
    "\n",
    "        if subj_entity_idx == 1:\n",
    "            concept_idx = 0\n",
    "\n",
    "            subj_entity = data[:, subj_entity_idx]\n",
    "            concept = data[:, concept_idx]\n",
    "\n",
    "            c_assertion_out1 = self.concept_embedding_dict(concept) # Outputs the moving parameter for the concept\n",
    "            c_assertion_out2 = self.individual_embedding_dict(subj_entity) # Outputs the embedding for the individual\n",
    "            \n",
    "            out1 = c_assertion_out1\n",
    "            out2 = c_assertion_out2\n",
    "            \n",
    "            return out1, out2\n",
    "\n",
    "        elif subj_entity_idx == 0:\n",
    "            role_idx = 1\n",
    "            obj_entity_idx = 2\n",
    "        \n",
    "            subject_entity = self.individual_embedding_dict(data[:, subj_entity_idx])\n",
    "            object_entity = self.individual_embedding_dict(data[:, obj_entity_idx])\n",
    "            role = self.role_embedding_dict(data[:, role_idx])\n",
    "\n",
    "            r_assertion_out1 = role # Role parameter embedding\n",
    "            r_assertion_out2 = torch.cat((subject_entity, object_entity), 1) # Concatenation of subject and object\n",
    "\n",
    "            out1 = r_assertion_out1\n",
    "            out2 = r_assertion_out2\n",
    "            \n",
    "            return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_dists(model, dictionary_concept_to_idx, role = False):\n",
    "\n",
    "    dist_dict = {}\n",
    "\n",
    "    for predicate, idx in dictionary_concept_to_idx.items():\n",
    "        with torch.no_grad():\n",
    "            if role == False:\n",
    "                dist = torch.dist(torch.tensor(GeometricInterpretation.concept_geointerps_dict[predicate].centroid), model.concept_embedding_dict.weight[idx])\n",
    "                dist_dict[predicate] = dist\n",
    "            else:\n",
    "                dist = torch.dist(torch.tensor(GeometricInterpretation.role_geointerps_dict[predicate].centroid), model.concept_embedding_dict.weight[idx])\n",
    "                dist_dict[predicate] = dist\n",
    "\n",
    "    return dist_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, concept_dataloader, role_dataloader, loss_fn, optimizer, alternate_training = False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "    \n",
    "    if alternate_training == False:\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "            loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2)  + model.phi * loss_fn(outputs1, labels) # Phi describe\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "            loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, concept_dataloader, role_dataloader, loss_fn, alternate_training = False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    if alternate_training == False:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(concept_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs)\n",
    "                loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            for i, data in enumerate(role_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs)\n",
    "                loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, data in enumerate(concept_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "                loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2)  + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "                loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_concept_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    relevant_concept_idx = []\n",
    "\n",
    "    # Gathers only concepts appearing in the test set (it is not guaranteed that if a concept appears in the dataset, then it appears here)\n",
    "\n",
    "    for assertion in test_concept_assertions:\n",
    "        inputs, _ = assertion\n",
    "        if inputs[0] not in relevant_concept_idx:\n",
    "            relevant_concept_idx.append(inputs[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        print(f'Number of concepts appearing on the test set: {len(relevant_concept_idx)}')\n",
    "        print('')\n",
    "\n",
    "        for concept_idx in relevant_concept_idx:\n",
    "            assertion_scores = []\n",
    "\n",
    "            for _, entity_idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([concept_idx, entity_idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample) # out1 = Concept parameter, out2 = Individual parameter\n",
    "\n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                else:\n",
    "                    assertion_score = torch.dist(torch.tensor(GeometricInterpretation.concept_geointerps_dict[idx_to_concept_vocab[int(concept_idx)]].centroid), outputs2, p=2) # Spaghetti, Italians should be proud of my code BUGGED DO NOT USE\n",
    "\n",
    "                assertion_scores.append((torch.tensor([concept_idx, entity_idx]), assertion_score.item()))\n",
    "            \n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "            hit_k_values = []\n",
    "\n",
    "            true_samples = [inputs for inputs, _ in test_concept_assertions if inputs[0] == concept_idx] # This is problematic when dealing with big datasets\n",
    "\n",
    "            for k in k_list:\n",
    "                hit_k = any(torch.equal(scored_sample[0], true_sample) for true_sample in true_samples for scored_sample in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "            \n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            top1 += int(hit_k_values[0])\n",
    "            top3 += int(hit_k_values[1])\n",
    "            top10 += int(hit_k_values[2])\n",
    "            top100 += int(hit_k_values[3])\n",
    "            top_all += int(hit_k_values[4])\n",
    "\n",
    "    hits_at_k = [sum(hit_values) / len(hit_values) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    return hits_at_k, [top1, top3, top10, top100, top_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_role_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "    relevant_assertions = []\n",
    "\n",
    "    # Convert PyTorch dataset to a numpy array for vectorization\n",
    "    assertions_array = [assertion[0].numpy() for assertion in test_role_assertions]\n",
    "    assertions_array = np.stack(assertions_array)\n",
    "\n",
    "    ''' \n",
    "    The array below is used to disregard duplicate queries.\n",
    "    For ex., if we have two assertions r(a,b) and r(a,c), the function\n",
    "    will treat r(a, ?) as a query with b and c as positive answers. It\n",
    "    will then disregard any other.\n",
    "    '''\n",
    "\n",
    "    filter_array = np.ones((assertions_array.shape), dtype=int)\n",
    "    filter_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for assertion_idx, assertion in enumerate(assertions_array):\n",
    "\n",
    "            filter_counter = assertion_idx\n",
    "\n",
    "            if np.all(filter_array[filter_counter] == 1):\n",
    "\n",
    "                head_entity_idx = assertion[0]\n",
    "                role_entity_idx = assertion[1]\n",
    "                filter_arr = (assertions_array[:, 0] == head_entity_idx) & (assertions_array[:, 1] == role_entity_idx)\n",
    "                relevant_assertions_idcs = np.where(filter_arr)[0]\n",
    "                relevant_assertions = torch.tensor(np.array([assertions_array[idx] for idx in relevant_assertions_idcs]))\n",
    "                filter_array[relevant_assertions_idcs] = 0\n",
    "\n",
    "                assertion_scores = []\n",
    "\n",
    "                for _, tail_entity_idx in entity_to_idx_vocab.items():\n",
    "                    eval_sample = torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]).unsqueeze(0)\n",
    "                    outputs1, outputs2 = model(eval_sample)\n",
    "                    if centroid_score == False:\n",
    "                        assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                    assertion_scores.append((torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]), assertion_score.item()))\n",
    "\n",
    "                sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "                k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "                hit_k_values = []\n",
    "\n",
    "                for k in k_list:\n",
    "                    hit_k = any(torch.equal(scored_sample[0], assertion) for assertion in relevant_assertions for scored_sample in sorted_scores[:k])\n",
    "                    hit_k_values.append(hit_k)\n",
    "            \n",
    "                hits.append(hit_k_values)\n",
    "\n",
    "                top1 += int(hit_k_values[0])\n",
    "                top3 += int(hit_k_values[1])\n",
    "                top10 += int(hit_k_values[2])\n",
    "                top100 += int(hit_k_values[3])\n",
    "                top_all += int(hit_k_values[4])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "    hits_at_k = [sum(hit_values) / len(hit_values) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "    print(hits_at_k)\n",
    "\n",
    "    return hits_at_k, [top1, top3, top10, top100, top_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(hit_ratios, hit_list):\n",
    "\n",
    "    k_values = [1, 3, 10, 100, 469]\n",
    "    \n",
    "    for i in range(len(hit_ratios)):\n",
    "            print(f'Top{str(k_values[i])} hits: {hit_list[i]}')\n",
    "            print(f'Hits@{str(k_values[i])}: {hit_ratios[i]}')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "\n",
    "CENTROID_SCORE = False # When set to True, model scores assertion w.r.t distance to the centroid instead of to the moving parameter for concepts/roles\n",
    "# EPOCHS = 100\n",
    "LR = 0.0001\n",
    "PHI = 0.0\n",
    "EMB_DIM = 465\n",
    "\n",
    "model = FaithEL(EMB_DIM, PHI,\n",
    "                entity_to_idx_vocab, concept_to_idx_vocab, role_to_idx_vocab,\n",
    "                init_concept_param_to_centroid = True, init_role_param_to_centroid = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_epoch = 10\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = get_param_dists(model, concept_to_idx_vocab, role = False)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_ConceptDataLoader, train_RoleDataLoader, loss_fn, optimizer, alternate_training=False)\n",
    "    test_loss = test(model, test_ConceptDataLoader, test_RoleDataLoader, loss_fn, alternate_training=False)\n",
    "\n",
    "    if epoch % log_epoch == 0:\n",
    "        print(f'Epoch {epoch}/{EPOCHS} -> Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = get_param_dists(model, concept_to_idx_vocab, role = False)\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k_concept, hit_list_concept = get_hits_at_k_concept_assertions(model,\n",
    "                                             testConceptDataset, testRoleDataset,\n",
    "                                             entity_to_idx_vocab, idx_to_entity_vocab,\n",
    "                                             idx_to_concept_vocab, role_to_idx_vocab,\n",
    "                                             centroid_score = CENTROID_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(hits_at_k_concept, hit_list_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hit_list_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k_role, hit_list_role = get_hits_at_k_role_assertions(model,\n",
    "                                             testConceptDataset, testRoleDataset,\n",
    "                                             entity_to_idx_vocab, idx_to_entity_vocab,\n",
    "                                             idx_to_concept_vocab, role_to_idx_vocab,\n",
    "                                             centroid_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(hits_at_k_role, hit_list_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hit_list_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corruption for training with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_concept_assertions(test_data_concept_assertions,\n",
    "                               num_corrupt = int,\n",
    "                               entity_to_idx_vocab = dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "    \n",
    "    original_assertions = torch.tensor([sample[0] for sample, label in list(test_data_concept_assertions)]) # Gets rid of the individual\n",
    "\n",
    "    num_samples = len(original_assertions)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 2), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_assertions\n",
    "    corrupted_assertions[:, 1] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_role_assertions(test_data_role_assertions,\n",
    "                            num_corrupt = int,\n",
    "                            entity_to_idx_vocab=dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "\n",
    "    original_head_entities = torch.tensor([sample[0] for sample, label in list(test_data_role_assertions)])\n",
    "    original_roles = torch.tensor([sample[1] for sample, label in list(test_data_role_assertions)])\n",
    "    original_tail_entities = torch.tensor([sample[2] for sample, label in list(test_data_role_assertions)])\n",
    "\n",
    "    num_samples = len(original_head_entities)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 3), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_head_entities # The original head entities\n",
    "    corrupted_assertions[:, 1] = original_roles # The original roles\n",
    "    corrupted_assertions[:, 2] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
