{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import owlready2 as owl\n",
    "from owlready2 import *\n",
    "import types\n",
    "\n",
    "#import scipy\n",
    "#from scipy.spatial import ConvexHull\n",
    "#import cdd\n",
    "#from cdd import RepType, Matrix, Polyhedron\n",
    "#from fractions import Fraction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/goslimyeast.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/galennorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/gonorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/'\n",
    "dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/family_ontology.owl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto = get_ontology(dir)\n",
    "onto = onto.load()\n",
    "gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "concept_names_iter = list(onto.classes())\n",
    "role_names_iter = list(onto.properties())\n",
    "individuals = list(onto.individuals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class for creating entities to\n",
    "populate the creation of the\n",
    "canonical models.\n",
    "\n",
    "The .name attribute is used to\n",
    "create a single representation\n",
    "for concepts like A and B / \n",
    "B and A, as they are the same.\n",
    "'''\n",
    "\n",
    "class CanonicalModelElements:\n",
    "\n",
    "    concept_names = {}\n",
    "    concept_intersections = {}\n",
    "    concept_restrictions = {}\n",
    "    all_concepts = {}\n",
    "\n",
    "    def __init__(self, concept):\n",
    "        self.concept = concept\n",
    "        self.name = self.get_name()\n",
    "        self.get_element_dict()\n",
    "\n",
    "    def get_name(self):\n",
    "\n",
    "        # add \\Top\n",
    "        \n",
    "        if type(self.concept) == ThingClass:\n",
    "            return self.concept.name\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            return 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "        \n",
    "        else:\n",
    "            return 'And_' + ''.join(sorted(self.concept.Classes[0].name + self.concept.Classes[1].name)) # The name is sorted to avoid that (e.g) (A \\and B) and (B \\and A) are treated as different concepts\n",
    "        \n",
    "    def get_element_dict(self):\n",
    "\n",
    "        if type(self.concept) == ThingClass:\n",
    "            CanonicalModelElements.concept_names[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            CanonicalModelElements.concept_restrictions[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == And:\n",
    "            CanonicalModelElements.concept_intersections[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_model_elements(concept_names_iter, role_names_iter):\n",
    "    \n",
    "    top = owl.Thing\n",
    "    bottom = owl.Nothing\n",
    "\n",
    "    CanonicalModelElements(top)\n",
    "    CanonicalModelElements(bottom)\n",
    "\n",
    "    for concept_name in concept_names_iter:\n",
    "        \n",
    "        CanonicalModelElements(concept_name)\n",
    "        for concept_name2 in concept_names_iter:\n",
    "        \n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(concept_name & concept_name2)\n",
    "                gca.is_a.append(concept_name & concept_name2)\n",
    "            \n",
    "            CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.')\n",
    "\n",
    "    concept_names_iter.append(top)\n",
    "    concept_names_iter.append(bottom)\n",
    "\n",
    "    for role_name in role_names_iter:\n",
    "        for concept_name in concept_names_iter:\n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(role_name.some(concept_name))\n",
    "                gca.is_a.append(role_name.some(concept_name))\n",
    "\n",
    "            CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('All restrictions have been preprocessed for the creation of the canonical model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main class for creating the canonical model for the ontology.\n",
    "\n",
    "The canonical model is stored in dictionaries available as class variables 'concept_canonical_interpretation'\n",
    "and 'role_canonical_interpretation'. \n",
    "\n",
    "Args:\n",
    "    concept_names_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_intersection_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_restrictions_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    all_concepts_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    role_names_iter (list): a list containing all role names in the loaded ontology.\n",
    "'''\n",
    "\n",
    "class CanonicalModel:\n",
    "\n",
    "    concept_canonical_interpretation = {}\n",
    "    role_canonical_interpretation = {}\n",
    "\n",
    "    def __init__(self, concept_names_dict, concept_intersections_dict, concept_restrictions_dict, all_concepts_dict, role_names_iter):\n",
    "        \n",
    "        self.domain = all_concepts_dict\n",
    "        self.concept_names_dict = concept_names_dict\n",
    "        self.concept_restrictions_dict = concept_restrictions_dict\n",
    "        self.concept_intersections_dict = concept_intersections_dict\n",
    "\n",
    "        self.role_names_iter = role_names_iter\n",
    "\n",
    "        self.concept_canonical_interp = self.get_concept_name_caninterp() # These are only used to build the concept_canonical_interpretation and role_canonical_interpretation class attributes\n",
    "        self.role_canonical_interp = self.get_role_name_caninterp()       # The functions do not return anything, they just update the class variables\n",
    "\n",
    "    def get_concept_name_caninterp(self):\n",
    "\n",
    "        # The variable concept is a string containing the name of an element of the domain of the canonical model\n",
    "        # The key to the concept_names_dict variable corresponds to concept.name\n",
    "        # This name can be used to access the concept in owlready2's format\n",
    "\n",
    "        for concept in self.concept_names_dict.keys():\n",
    "\n",
    "            CanonicalModel.concept_canonical_interpretation[concept] = []\n",
    "            superclasses = self.domain[concept].concept.ancestors(include_self=True, include_constructs=True) # The self.domain[concept] is used to access the CanonicalModelElements type of object,\n",
    "                                                                                                               # and the attribute .concept is used to access the concept in owlready2 format\n",
    "                                                                                                              \n",
    "            for superclass in superclasses:\n",
    "\n",
    "                if type(superclass) == ThingClass:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append(superclass.name)\n",
    "\n",
    "                elif type(superclass) == Restriction:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append('exists_' + superclass.property.name + '.' + superclass.value.name)\n",
    "\n",
    "                elif type(superclass) == And:\n",
    "                    if 'And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)) in CanonicalModel.concept_canonical_interpretation[concept]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        CanonicalModel.concept_canonical_interpretation[concept].append('And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)))\n",
    "\n",
    "    def get_role_name_caninterp(self):\n",
    "\n",
    "        # First case from Definition 10\n",
    "\n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            role_name_str = role_name.name # Accesses the property type object's name as a string\n",
    "            CanonicalModel.role_canonical_interpretation[role_name_str] = []\n",
    "\n",
    "            for restriction_name in self.concept_restrictions_dict.keys(): # Where restriction_name denotes a \\exists r.B type of concept 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "                c_B = self.concept_restrictions_dict[restriction_name].concept.value.name\n",
    "\n",
    "                if role_name_str in restriction_name:\n",
    "                    superclasses = self.domain[restriction_name].concept.ancestors(include_self=True, include_constructs=False)\n",
    "\n",
    "                    for superclass in superclasses:\n",
    "                        super_superclasses = superclass.ancestors(include_self=True, include_constructs=True)\n",
    "\n",
    "                        for super_superclass in super_superclasses:\n",
    "\n",
    "                            if type(super_superclass) == ThingClass:\n",
    "                                c_D = super_superclass.name\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append(tuple(c_D, c_B))\n",
    "\n",
    "                            elif type(super_superclass) == Restriction:\n",
    "                                c_D = 'exists_' + super_superclass.property.name + '.' + super_superclass.value.name\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append(tuple(c_D, c_B))\n",
    "\n",
    "                            elif type(super_superclass) == And:\n",
    "                                c_D = 'And_' + ''.join(sorted(super_superclass.Classes[0].name + super_superclass.Classes[1].name))\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append(tuple(c_D, c_B))\n",
    "\n",
    "        # Second case from Definition 10: r \\sqsubset s \n",
    "            \n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            superroles = role_name.ancestors(include_self=True)\n",
    "            role_name_str = role_name.name\n",
    "            \n",
    "            for superrole in superroles:\n",
    "                for restriction_name in self.concept_restrictions_dict.keys():\n",
    "                    if superrole.name in restriction_name:\n",
    "                        pair = tuple((restriction_name, self.domain[restriction_name].concept.value.name))\n",
    "                        CanonicalModel.role_canonical_interpretation[role_name_str].append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main function for creating the canonical model.\n",
    "\n",
    "    Args:\n",
    "        onto_dir (str): a string pointing to the directory where the ontology is stored.\n",
    "\n",
    "    Returns:\n",
    "        canmodel (CanonicalModel): returns a variable containing the canonical model. \n",
    "        \n",
    "        Attention: the interpretations of concept names and role names can also be accessed via class variables\n",
    "        from the CanonicalModel class.\n",
    "'''\n",
    "\n",
    "def create_canonical_model(onto_dir):\n",
    "\n",
    "    onto = get_ontology(onto_dir)\n",
    "    onto = onto.load()\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "\n",
    "    get_canonical_model_elements(concept_names_iter, role_names_iter)\n",
    "\n",
    "    print('')\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Starting to reason.')\n",
    "    print('')\n",
    "\n",
    "    with onto:\n",
    "        sync_reasoner()\n",
    "        \n",
    "    # onto.save(\"inferences_goslimyeast.owl\")\n",
    "\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "\n",
    "    print('')\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Done reasoning. Creating the canonical model.')\n",
    "    print('')\n",
    "    canmodel = CanonicalModel(CanonicalModelElements.concept_names, CanonicalModelElements.concept_intersections, CanonicalModelElements.concept_restrictions, CanonicalModelElements.all_concepts, role_names_iter)\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Concluded creating canonical model.')\n",
    "\n",
    "    return canmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.\n",
      "\n",
      "\n",
      "All restrictions have been preprocessed for the creation of the canonical model.\n",
      "\n",
      "============================================================================\n",
      "\n",
      "Starting to reason.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx2000M -cp /opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit:/opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////var/folders/wg/g5861gcs6k5d3rbq_rncztjw0000gn/T/tmp36ctrxm2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================\n",
      "\n",
      "Done reasoning. Creating the canonical model.\n",
      "\n",
      "============================================================================\n",
      "\n",
      "Concluded creating canonical model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * HermiT took 0.5299110412597656 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the canonical model\n",
    "\n",
    "canmodel = create_canonical_model(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for initializing\n",
    "the class EntityEmbedding. They\n",
    "allow us to access dictionaries\n",
    "containing indexes and canonical\n",
    "interpretation of concepts\n",
    "and roles as class.\n",
    "'''\n",
    "\n",
    "def get_concept_names_idx_dict(canmodel):\n",
    "   conceptnames_idx_dict = {concept_name: idx for idx, concept_name in enumerate(CanonicalModel.concept_canonical_interpretation.keys())}\n",
    "   return conceptnames_idx_dict\n",
    "\n",
    "def get_role_names_idx_dict(canmodel):\n",
    "    rolenames_idx_dict = {role_name: idx for idx, role_name in enumerate(CanonicalModel.role_canonical_interpretation.keys())}\n",
    "    return rolenames_idx_dict\n",
    "\n",
    "def get_entities_idx_dict(canmodel):\n",
    "    entities_idx_dict = {entity: idx for idx, entity in enumerate(canmodel.domain.keys())}\n",
    "    return entities_idx_dict\n",
    "\n",
    "def get_domain_dict(canmodel):\n",
    "    return canmodel.domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atenção: a função mu está com complexidade alta devido aos for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Class for obtaining the positional \n",
    "embedding for each entity in the domain\n",
    "of the canonical interpretation.\n",
    "It represents the Mu Function from the\n",
    "paper.\n",
    "'''\n",
    "\n",
    "class EntityEmbedding:\n",
    "\n",
    "    # Dictionaries for storing the indices of concept names and role names, entities pairs, respectively\n",
    "    # Keys are strings and values are integers\n",
    "    \n",
    "    concept_names_idx_dict = get_concept_names_idx_dict(canmodel)\n",
    "    role_names_idx_dict = get_role_names_idx_dict(canmodel)\n",
    "    entities_idx_dict = get_entities_idx_dict(canmodel)\n",
    "\n",
    "    # Dictionaries accessing the canonical interpretation of concepts and roles\n",
    "    # Keys and values are strings\n",
    "    \n",
    "    concept_canonical_interpretation_dict = CanonicalModel.concept_canonical_interpretation\n",
    "    role_canonical_interpretation_dict = CanonicalModel.role_canonical_interpretation\n",
    "\n",
    "    # Dictionary storing the domain of the canonical model being embedded\n",
    "    # IMPORTANT: Keys are strings and values are CanonicalModelElements type objects\n",
    "    \n",
    "    domain_dict = get_domain_dict(canmodel)\n",
    "\n",
    "    # Dictionary for easy access to entity embeddings\n",
    "    # It is initialized with empty values, iteratively built by the .get_embedding_vector() method\n",
    "    # Key (str): Domain Entity / Value (np.array): EntityEmbedding.embedding_vector\n",
    "\n",
    "    entity_entityvector_dict = dict.fromkeys(domain_dict.keys())\n",
    "\n",
    "    def __init__(self, entity_name, emb_dim):\n",
    "        self.name = entity_name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.in_interpretation_of = []\n",
    "        self.embedding_vector = self.get_embedding_vector()\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        \n",
    "        embedding_vector = np.zeros((self.emb_dim,))\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = []\n",
    "\n",
    "        # Applies the embedding function to the concept names portion of the definition\n",
    "\n",
    "        for concept_name in EntityEmbedding.concept_canonical_interpretation_dict:\n",
    "            concept_name_idx = EntityEmbedding.concept_names_idx_dict[concept_name]\n",
    "        \n",
    "            if self.name in EntityEmbedding.concept_canonical_interpretation_dict[concept_name]:\n",
    "                embedding_vector[concept_name_idx] = 1\n",
    "                self.in_interpretation_of.append(concept_name)\n",
    "\n",
    "        # Applies the embedding function to the role names portion of the definition\n",
    "\n",
    "        for role_name in EntityEmbedding.role_canonical_interpretation_dict:\n",
    "            \n",
    "            role_name_idx = len(EntityEmbedding.concept_names_idx_dict) + (EntityEmbedding.role_names_idx_dict[role_name] * len(EntityEmbedding.entities_idx_dict))\n",
    "            role_name_caninterp = EntityEmbedding.role_canonical_interpretation_dict[role_name]\n",
    "\n",
    "            for pair in role_name_caninterp:\n",
    "\n",
    "                entity_2 = pair[1]\n",
    "\n",
    "                if (self.name, entity_2) == pair:\n",
    "                    entity_2_idx = EntityEmbedding.entities_idx_dict[entity_2]\n",
    "                    final_role_entity_pair_idx = role_name_idx + entity_2_idx\n",
    "                    embedding_vector[final_role_entity_pair_idx] = 1\n",
    "\n",
    "        # EntityEmbedding.entity_entityvector_dict[self.name].append(embedding_vector)\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = embedding_vector\n",
    "\n",
    "        return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the binary vectors representing\n",
    "each element of the domain of the canonical interpretation.\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int/float): the number of dimensions of the embedding space.\n",
    "\n",
    "    Returns:\n",
    "        embedded_entities (list): a list containing all embeddings of the entities\n",
    "                                  in the domain. \n",
    "    \n",
    "    The embedded_entities are also available in the dictionary EntityEmbeddings.entity_entityvector_dict\n",
    "'''\n",
    "\n",
    "def get_domain_embeddings(emb_dim):\n",
    "\n",
    "    embedded_entities = []\n",
    "    counter = 0\n",
    "\n",
    "   # The entities in the domain are strings\n",
    "    \n",
    "    for entity_name in EntityEmbedding.domain_dict:\n",
    "       embedded_entity = EntityEmbedding(entity_name, emb_dim)\n",
    "       embedded_entities.append(embedded_entity)\n",
    "       counter += 1\n",
    "       \n",
    "       if counter % 1000 == 0:\n",
    "           print(counter)\n",
    "       \n",
    "    return embedded_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final class for creating the dataset.\n",
    "\n",
    "Inputs: concept or role names, generated\n",
    "embeddings for entities in the domain of\n",
    "the canonical model.\n",
    "\n",
    "Outputs: geometrical interpretation of\n",
    "concepts and role names, represented\n",
    "by vertices defining a region.\n",
    "\n",
    "One can access the GeometricInterpretation\n",
    "objects either as elements in a list, or as\n",
    "values in a class variable dictionary.\n",
    "'''\n",
    "\n",
    "class GeometricInterpretation:\n",
    "\n",
    "    concept_geointerps_dict = dict.fromkeys(CanonicalModel.concept_canonical_interpretation.keys())\n",
    "    role_geointerps_dict = dict.fromkeys(CanonicalModel.role_canonical_interpretation.keys())\n",
    "\n",
    "    def __init__(self, name, emb_dim):\n",
    "        self.name = name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vertices = []\n",
    "        self.centroid = None\n",
    "\n",
    "    def get_centroid(self):\n",
    "        pass\n",
    "\n",
    "    def get_centroid_naive(self):\n",
    "        if len(self.vertices) == 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) == 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim * 2,))\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There has to be a more efficient way of doing the creating of geometric interpretations for concepts and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_finder(emb_dim, concept_name_idx_dict, role_name_idx_dict, domain_idx_dict):\n",
    "\n",
    "    index_dict = {k: None for k in range(emb_dim)}\n",
    "\n",
    "    for k,v in concept_name_idx_dict.items():\n",
    "\n",
    "        index_dict[v] = k\n",
    "\n",
    "    for role in role_name_idx_dict:\n",
    "        role_init_idx = len(concept_name_idx_dict) + (role_name_idx_dict[role] * len(domain_idx_dict))\n",
    "\n",
    "        for entity in domain_idx_dict:\n",
    "            entity_init_idx = domain_idx_dict[entity]\n",
    "            final_role_entity_pair_idx = role_init_idx + entity_init_idx\n",
    "            index_dict[final_role_entity_pair_idx] = (role, entity)\n",
    "        \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "\n",
    "    faithful_concept_geometric_interps = []\n",
    "\n",
    "    for concept_name in concept_names_interps.keys():\n",
    "        concept_name = GeometricInterpretation(concept_name, emb_dim)\n",
    "\n",
    "        for embedding in domain_embeddings_list:\n",
    "            if concept_name.name in embedding.in_interpretation_of:\n",
    "                concept_name.vertices.append(embedding.embedding_vector)\n",
    "            \n",
    "        GeometricInterpretation.concept_geointerps_dict[concept_name.name] = concept_name\n",
    "        concept_name.centroid = concept_name.get_centroid_naive()\n",
    "        \n",
    "        faithful_concept_geometric_interps.append(concept_name)\n",
    "\n",
    "    return faithful_concept_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_role_geometric_interps(role_names_interps, entity_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "    \n",
    "    faithful_role_geometric_interps = []\n",
    "    idx_entity_dict = entity_dims_index_dict\n",
    "    #entity_idx_dict = {v: k for k,v in entity_dims_index_dict}\n",
    "\n",
    "    relevant_idxs = len(canmodel.concept_names_dict)-1\n",
    "\n",
    "    for role_name in role_names_interps.keys():\n",
    "        role_name_str = role_name\n",
    "        role_name = GeometricInterpretation(role_name_str, emb_dim)\n",
    "\n",
    "        for entity in entity_embeddings_list:\n",
    "\n",
    "            onehot_idx_list = np.where(entity.embedding_vector == 1)[0]\n",
    "            #print(f'This is the entity: {entity} and this is the onehot_idx_list: {onehot_idx_list}')\n",
    "\n",
    "            for idx in onehot_idx_list: # I could just look at the TRULY relevant indexes\n",
    "                if idx > relevant_idxs:\n",
    "                    role_entity_pair = idx_entity_dict[idx]\n",
    "                    r_name_str = role_entity_pair[0]\n",
    "                    e_name_str = role_entity_pair[1]\n",
    "\n",
    "                    if r_name_str == role_name_str:\n",
    "                        e_embedding = EntityEmbedding.entity_entityvector_dict[e_name_str]\n",
    "                        role_name.vertices.append(np.concatenate((entity.embedding_vector, e_embedding)))\n",
    "\n",
    "        GeometricInterpretation.role_geointerps_dict[role_name_str] = role_name\n",
    "        role_name.centroid = role_name.get_centroid_naive()\n",
    "        faithful_role_geometric_interps.append(role_name)\n",
    "\n",
    "    return faithful_role_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tbox_embeddings(canonical_model: CanonicalModel):\n",
    "\n",
    "    domain = canonical_model.domain # Keys are strings and values are CanonicalModelElements type objects\n",
    "    concept_names_interps = canonical_model.concept_canonical_interpretation # Keys are strings and values are lists of strings.\n",
    "    role_names_interps = canonical_model.role_canonical_interpretation # Keys are strings and values are lists of tuples. Tuples are of form ('C', 'D'), with C and D strings.\n",
    "\n",
    "    EMB_DIM = len(concept_names_interps) + len(role_names_interps) * len(domain)\n",
    "\n",
    "    print('================EMBEDDING DIMENSION================')\n",
    "    print(f'Concept Name dimensions: {len(concept_names_interps)}')\n",
    "    print(f'The number of role names is: {len(role_names_interps)}')\n",
    "    print(f'The size of the domain is: {len(domain)}')\n",
    "    print(f'Role names dimensions: {len(role_names_interps) * len(domain)}')\n",
    "    print('===================================================')\n",
    "    print('')\n",
    "    print(f'Final embedding dimension: {EMB_DIM}')\n",
    "\n",
    "    domain_embeddings_list = get_domain_embeddings(EMB_DIM)\n",
    "    \n",
    "    concept_names_ordering = EntityEmbedding.concept_names_idx_dict\n",
    "    role_names_ordering = EntityEmbedding.role_names_idx_dict\n",
    "    entities_ordering = EntityEmbedding.entities_idx_dict\n",
    "    \n",
    "    print('')\n",
    "    print('===============FINISHED EMBEDDINGS===============')\n",
    "    print(f'There are {len(domain_embeddings_list)} vector embeddings.')\n",
    "    print('')\n",
    "\n",
    "    index_finder_dict = index_finder(EMB_DIM, concept_names_ordering, role_names_ordering, entities_ordering)\n",
    "\n",
    "    faithful_concept_geometric_interps = get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('============FINISHED INTERPS CONCEPT=============')\n",
    "    print(f'There are {len(faithful_concept_geometric_interps)} regions for concept names.')\n",
    "    print('')\n",
    "\n",
    "    faithful_role_geometric_interps = get_faithful_role_geometric_interps(role_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('=============FINISHED INTERPS ROLES==============')\n",
    "    print(f'There are {len(faithful_role_geometric_interps)} regions for role names.')\n",
    "    print('')\n",
    "\n",
    "    return domain_embeddings_list, faithful_concept_geometric_interps, faithful_role_geometric_interps, index_finder_dict # Returns the faithful geometric interpretations for concepts and roles as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================EMBEDDING DIMENSION================\n",
      "Concept Name dimensions: 10\n",
      "The number of role names is: 5\n",
      "The size of the domain is: 96\n",
      "Role names dimensions: 480\n",
      "===================================================\n",
      "\n",
      "Final embedding dimension: 490\n",
      "\n",
      "===============FINISHED EMBEDDINGS===============\n",
      "There are 96 vector embeddings.\n",
      "\n",
      "============FINISHED INTERPS CONCEPT=============\n",
      "There are 10 regions for concept names.\n",
      "\n",
      "=============FINISHED INTERPS ROLES==============\n",
      "There are 5 regions for role names.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "domain_embeddings, concept_geointerps, role_geointerps, idx_finder_dict = create_tbox_embeddings(canmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the pre-split dataset containing facts from the ontology.\n",
    "Distinguishes between concept assertions and role assertions.\n",
    "\n",
    "\n",
    "    Args: ontology_dir (str): the directory from the ontology\n",
    "          concept_geointerps_dict (dict): the geometrical interpretations for concepts generated by create_tbox_embeddings()\n",
    "          role_geointerps_dict (dict): the geometrical interpretations for roles generated by create_tbox_embeddings()\n",
    "\n",
    "    Returns:\n",
    "          X_concepts (np.array): A dataframe with columns 'Concept', 'Entity', 'y_true' (equivalent to concept.centroid())\n",
    "          X_roles (np.array): A dataframe with columns 'SubjectEntity', 'Role', 'ObjectEntity', 'y_true' (equivalent to role.centroid())\n",
    "          y_concepts (np.array):\n",
    "          y_roles (np.array):\n",
    "          vocabulary_dict (dict): A vocabulary with key (int): value (str) for entities in the domain.\n",
    "'''\n",
    "\n",
    "def get_abox_dataset(ontology_dir: str, concept_geointerps_dict: dict, role_geointerps_dict: dict, concept_to_idx: dict, role_to_idx: dict):\n",
    "    \n",
    "    ontology = get_ontology(ontology_dir)\n",
    "    ontology = ontology.load()\n",
    "    \n",
    "    X_concepts = []\n",
    "    X_roles = []\n",
    "    y_concepts = []\n",
    "    y_roles = []\n",
    "\n",
    "    entities = [entity.name for entity in list(ontology.individuals())]\n",
    "\n",
    "    concept_to_idx_vocab = concept_to_idx\n",
    "    idx_to_concept_vocab = {value: key for key, value in concept_to_idx_vocab.items()}\n",
    "\n",
    "    role_to_idx_vocab = role_to_idx\n",
    "    idx_to_role_vocab = {value: key for key, value in role_to_idx_vocab.items()}\n",
    "    \n",
    "    entity_to_idx_vocab = {value: index for index, value in enumerate(entities)}\n",
    "    idx_to_entity_vocab = {value: key for key, value in entity_to_idx_vocab.items()}\n",
    "\n",
    "    for individual in list(ontology.individuals()):\n",
    "        \n",
    "        all_facts = individual.is_a\n",
    "\n",
    "        for concept in all_facts:\n",
    "            if type(concept) == ThingClass:\n",
    "                concept = concept_geointerps_dict[concept.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "\n",
    "            if type(concept) == And:\n",
    "                concept1 = concept_geointerps_dict[concept.Classes[0].name]\n",
    "                concept2 = concept_geointerps_dict[concept.Classes[1].name]\n",
    "                concept_name = 'A#nd_' + ''.join(sorted(concept1.name + concept2.name))\n",
    "                fact = np.array([concept_to_idx_vocab[concept_name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "\n",
    "            if type(concept) == Restriction:\n",
    "                concept = concept_geointerps_dict['exists_' + concept.property.name + '.' + concept.value.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "\n",
    "        relevant_roles = individual.get_properties()\n",
    "        individual_name = individual.name\n",
    "\n",
    "        for role in relevant_roles:\n",
    "            role_geo = role_geointerps_dict[role.name]\n",
    "            subject_list = role[individual] # This syntax is from the owlready2 library\n",
    "            for subject in subject_list:\n",
    "                fact = np.array([entity_to_idx_vocab[individual.name], role_to_idx_vocab[role.name], entity_to_idx_vocab[subject.name]])\n",
    "                X_roles.append(fact)\n",
    "                y_label = y_roles.append(np.array(role_geo.centroid))\n",
    "\n",
    "    return np.array(X_concepts), np.array(X_roles), np.array(y_concepts), np.array(y_roles), entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concepts, X_roles, y_concepts, y_roles, entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab = get_abox_dataset(dir,\n",
    "                                                                                                                                                                                        GeometricInterpretation.concept_geointerps_dict,\n",
    "                                                                                                                                                                                        GeometricInterpretation.role_geointerps_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.concept_names_idx_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.role_names_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0],\n",
       "       [  0,   1],\n",
       "       [  0,   2],\n",
       "       [  0,   3],\n",
       "       [  0,   4],\n",
       "       [  0,   5],\n",
       "       [  0,   6],\n",
       "       [  0,   7],\n",
       "       [  0,   8],\n",
       "       [  0,   9],\n",
       "       [  0,  10],\n",
       "       [  0,  11],\n",
       "       [  0,  12],\n",
       "       [  0,  13],\n",
       "       [  0,  14],\n",
       "       [  0,  15],\n",
       "       [  0,  16],\n",
       "       [  0,  17],\n",
       "       [  0,  18],\n",
       "       [  0,  19],\n",
       "       [  0,  20],\n",
       "       [  0,  21],\n",
       "       [  0,  22],\n",
       "       [  0,  23],\n",
       "       [  0,  24],\n",
       "       [  0,  25],\n",
       "       [  0,  26],\n",
       "       [  0,  27],\n",
       "       [  0,  28],\n",
       "       [  0,  29],\n",
       "       [  0,  30],\n",
       "       [  0,  31],\n",
       "       [  0,  32],\n",
       "       [  0,  33],\n",
       "       [  0,  34],\n",
       "       [  0,  35],\n",
       "       [  0,  36],\n",
       "       [  0,  37],\n",
       "       [  0,  38],\n",
       "       [  0,  39],\n",
       "       [  0,  40],\n",
       "       [  0,  41],\n",
       "       [  0,  42],\n",
       "       [  0,  43],\n",
       "       [  0,  44],\n",
       "       [  0,  45],\n",
       "       [  0,  46],\n",
       "       [  0,  47],\n",
       "       [  0,  48],\n",
       "       [  0,  49],\n",
       "       [  0,  50],\n",
       "       [  0,  51],\n",
       "       [  0,  52],\n",
       "       [  0,  53],\n",
       "       [  0,  54],\n",
       "       [  0,  55],\n",
       "       [  0,  56],\n",
       "       [  0,  57],\n",
       "       [  0,  58],\n",
       "       [  0,  59],\n",
       "       [  0,  60],\n",
       "       [  0,  61],\n",
       "       [  0,  62],\n",
       "       [  0,  63],\n",
       "       [  0,  64],\n",
       "       [  0,  65],\n",
       "       [  0,  66],\n",
       "       [  0,  67],\n",
       "       [  0,  68],\n",
       "       [  0,  69],\n",
       "       [  0,  70],\n",
       "       [  0,  71],\n",
       "       [  0,  72],\n",
       "       [  0,  73],\n",
       "       [  0,  74],\n",
       "       [  0,  75],\n",
       "       [  0,  76],\n",
       "       [  0,  77],\n",
       "       [  0,  78],\n",
       "       [  0,  79],\n",
       "       [  0,  80],\n",
       "       [  0,  81],\n",
       "       [  0,  82],\n",
       "       [  0,  83],\n",
       "       [  0,  84],\n",
       "       [  0,  85],\n",
       "       [  0,  86],\n",
       "       [  0,  87],\n",
       "       [  0,  88],\n",
       "       [  0,  89],\n",
       "       [  0,  90],\n",
       "       [  0,  91],\n",
       "       [  0,  92],\n",
       "       [  0,  93],\n",
       "       [  0,  94],\n",
       "       [  0,  95],\n",
       "       [  0,  96],\n",
       "       [  0,  97],\n",
       "       [  0,  98],\n",
       "       [  0,  99],\n",
       "       [  0, 100],\n",
       "       [  0, 101],\n",
       "       [  0, 102],\n",
       "       [  0, 103],\n",
       "       [  0, 104],\n",
       "       [  0, 105],\n",
       "       [  0, 106],\n",
       "       [  0, 107],\n",
       "       [  0, 108],\n",
       "       [  0, 109],\n",
       "       [  0, 110],\n",
       "       [  0, 111],\n",
       "       [  0, 112],\n",
       "       [  0, 113],\n",
       "       [  0, 114],\n",
       "       [  0, 115],\n",
       "       [  0, 116],\n",
       "       [  0, 117],\n",
       "       [  0, 118],\n",
       "       [  0, 119],\n",
       "       [  0, 120],\n",
       "       [  0, 121],\n",
       "       [  0, 122],\n",
       "       [  0, 123],\n",
       "       [  0, 124],\n",
       "       [  0, 125],\n",
       "       [  0, 126],\n",
       "       [  0, 127],\n",
       "       [  0, 128],\n",
       "       [  0, 129],\n",
       "       [  0, 130],\n",
       "       [  0, 131],\n",
       "       [  0, 132],\n",
       "       [  0, 133],\n",
       "       [  0, 134],\n",
       "       [  0, 135],\n",
       "       [  0, 136],\n",
       "       [  0, 137],\n",
       "       [  0, 138],\n",
       "       [  0, 139],\n",
       "       [  0, 140],\n",
       "       [  0, 141],\n",
       "       [  0, 142],\n",
       "       [  0, 143],\n",
       "       [  0, 144],\n",
       "       [  0, 145],\n",
       "       [  0, 146],\n",
       "       [  0, 147],\n",
       "       [  0, 148],\n",
       "       [  0, 149],\n",
       "       [  0, 150],\n",
       "       [  0, 151],\n",
       "       [  0, 152],\n",
       "       [  0, 153],\n",
       "       [  0, 154],\n",
       "       [  0, 155],\n",
       "       [  0, 156],\n",
       "       [  0, 157],\n",
       "       [  0, 158],\n",
       "       [  0, 159],\n",
       "       [  0, 160],\n",
       "       [  0, 161],\n",
       "       [  0, 162],\n",
       "       [  0, 163],\n",
       "       [  0, 164],\n",
       "       [  0, 165],\n",
       "       [  0, 166],\n",
       "       [  0, 167],\n",
       "       [  0, 168],\n",
       "       [  0, 169],\n",
       "       [  0, 170],\n",
       "       [  0, 171],\n",
       "       [  0, 172],\n",
       "       [  0, 173],\n",
       "       [  0, 174],\n",
       "       [  0, 175],\n",
       "       [  0, 176],\n",
       "       [  0, 177],\n",
       "       [  0, 178],\n",
       "       [  0, 179],\n",
       "       [  0, 180],\n",
       "       [  0, 181],\n",
       "       [  0, 182],\n",
       "       [  0, 183],\n",
       "       [  0, 184],\n",
       "       [  0, 185],\n",
       "       [  0, 186],\n",
       "       [  0, 187],\n",
       "       [  0, 188],\n",
       "       [  0, 189],\n",
       "       [  0, 190],\n",
       "       [  0, 191],\n",
       "       [  0, 192],\n",
       "       [  0, 193],\n",
       "       [  0, 194],\n",
       "       [  0, 195],\n",
       "       [  0, 196],\n",
       "       [  0, 197],\n",
       "       [  0, 198],\n",
       "       [  0, 199],\n",
       "       [  0, 200],\n",
       "       [  0, 201],\n",
       "       [  0, 202],\n",
       "       [  0, 203],\n",
       "       [  0, 204],\n",
       "       [  0, 205],\n",
       "       [  0, 206],\n",
       "       [  0, 207],\n",
       "       [  0, 208],\n",
       "       [  0, 209],\n",
       "       [  0, 210],\n",
       "       [  0, 211],\n",
       "       [  0, 212],\n",
       "       [  0, 213],\n",
       "       [  0, 214],\n",
       "       [  0, 215],\n",
       "       [  0, 216],\n",
       "       [  0, 217],\n",
       "       [  0, 218],\n",
       "       [  0, 219],\n",
       "       [  0, 220],\n",
       "       [  0, 221],\n",
       "       [  0, 222],\n",
       "       [  0, 223],\n",
       "       [  0, 224],\n",
       "       [  0, 225],\n",
       "       [  0, 226],\n",
       "       [  0, 227],\n",
       "       [  0, 228],\n",
       "       [  0, 229],\n",
       "       [  0, 230],\n",
       "       [  0, 231],\n",
       "       [  0, 232],\n",
       "       [  0, 233],\n",
       "       [  0, 234],\n",
       "       [  0, 235],\n",
       "       [  0, 236],\n",
       "       [  0, 237],\n",
       "       [  0, 238],\n",
       "       [  0, 239],\n",
       "       [  0, 240],\n",
       "       [  0, 241],\n",
       "       [  0, 242],\n",
       "       [  0, 243],\n",
       "       [  0, 244],\n",
       "       [  0, 245],\n",
       "       [  0, 246],\n",
       "       [  0, 247],\n",
       "       [  0, 248],\n",
       "       [  0, 249],\n",
       "       [  0, 250],\n",
       "       [  0, 251],\n",
       "       [  0, 252],\n",
       "       [  0, 253],\n",
       "       [  0, 254],\n",
       "       [  0, 255],\n",
       "       [  0, 256],\n",
       "       [  0, 257],\n",
       "       [  0, 258],\n",
       "       [  0, 259],\n",
       "       [  0, 260],\n",
       "       [  0, 261],\n",
       "       [  0, 262],\n",
       "       [  0, 263],\n",
       "       [  0, 264],\n",
       "       [  0, 265],\n",
       "       [  0, 266],\n",
       "       [  0, 267],\n",
       "       [  0, 268],\n",
       "       [  0, 269],\n",
       "       [  0, 270],\n",
       "       [  0, 271],\n",
       "       [  0, 272],\n",
       "       [  0, 273],\n",
       "       [  0, 274],\n",
       "       [  0, 275],\n",
       "       [  0, 276],\n",
       "       [  0, 277],\n",
       "       [  0, 278],\n",
       "       [  0, 279],\n",
       "       [  0, 280],\n",
       "       [  0, 281],\n",
       "       [  0, 282],\n",
       "       [  0, 283],\n",
       "       [  0, 284],\n",
       "       [  0, 285],\n",
       "       [  0, 286],\n",
       "       [  0, 287],\n",
       "       [  0, 288],\n",
       "       [  0, 289],\n",
       "       [  0, 290],\n",
       "       [  0, 291],\n",
       "       [  0, 292],\n",
       "       [  0, 293],\n",
       "       [  0, 294],\n",
       "       [  0, 295],\n",
       "       [  0, 296],\n",
       "       [  0, 297],\n",
       "       [  0, 298],\n",
       "       [  0, 299],\n",
       "       [  0, 300],\n",
       "       [  0, 301],\n",
       "       [  0, 302],\n",
       "       [  0, 303],\n",
       "       [  0, 304],\n",
       "       [  0, 305],\n",
       "       [  0, 306],\n",
       "       [  0, 307],\n",
       "       [  0, 308],\n",
       "       [  0, 309],\n",
       "       [  0, 310],\n",
       "       [  0, 311],\n",
       "       [  0, 312],\n",
       "       [  0, 313],\n",
       "       [  0, 314],\n",
       "       [  0, 315],\n",
       "       [  0, 316],\n",
       "       [  0, 317],\n",
       "       [  0, 318],\n",
       "       [  0, 319],\n",
       "       [  0, 320],\n",
       "       [  0, 321],\n",
       "       [  0, 322],\n",
       "       [  0, 323],\n",
       "       [  0, 324],\n",
       "       [  0, 325],\n",
       "       [  0, 326],\n",
       "       [  0, 327],\n",
       "       [  0, 328],\n",
       "       [  0, 329],\n",
       "       [  0, 330],\n",
       "       [  0, 331],\n",
       "       [  0, 332],\n",
       "       [  0, 333],\n",
       "       [  0, 334],\n",
       "       [  0, 335],\n",
       "       [  0, 336],\n",
       "       [  0, 337],\n",
       "       [  0, 338],\n",
       "       [  0, 339],\n",
       "       [  0, 340],\n",
       "       [  0, 341],\n",
       "       [  0, 342],\n",
       "       [  0, 343],\n",
       "       [  0, 344],\n",
       "       [  0, 345],\n",
       "       [  0, 346],\n",
       "       [  0, 347],\n",
       "       [  0, 348],\n",
       "       [  0, 349],\n",
       "       [  0, 350],\n",
       "       [  0, 351],\n",
       "       [  0, 352],\n",
       "       [  0, 353],\n",
       "       [  0, 354],\n",
       "       [  0, 355],\n",
       "       [  0, 356],\n",
       "       [  0, 357],\n",
       "       [  0, 358],\n",
       "       [  0, 359],\n",
       "       [  0, 360],\n",
       "       [  0, 361],\n",
       "       [  0, 362],\n",
       "       [  0, 363],\n",
       "       [  0, 364],\n",
       "       [  0, 365],\n",
       "       [  0, 366],\n",
       "       [  0, 367],\n",
       "       [  0, 368],\n",
       "       [  0, 369],\n",
       "       [  0, 370],\n",
       "       [  0, 371],\n",
       "       [  0, 372],\n",
       "       [  0, 373],\n",
       "       [  0, 374],\n",
       "       [  0, 375],\n",
       "       [  0, 376],\n",
       "       [  0, 377],\n",
       "       [  0, 378],\n",
       "       [  0, 379],\n",
       "       [  0, 380],\n",
       "       [  0, 381],\n",
       "       [  0, 382],\n",
       "       [  0, 383],\n",
       "       [  0, 384],\n",
       "       [  0, 385],\n",
       "       [  0, 386],\n",
       "       [  0, 387],\n",
       "       [  0, 388],\n",
       "       [  0, 389],\n",
       "       [  0, 390],\n",
       "       [  0, 391],\n",
       "       [  0, 392],\n",
       "       [  0, 393],\n",
       "       [  0, 394],\n",
       "       [  0, 395],\n",
       "       [  0, 396],\n",
       "       [  0, 397],\n",
       "       [  0, 398],\n",
       "       [  0, 399],\n",
       "       [  0, 400],\n",
       "       [  0, 401],\n",
       "       [  0, 402],\n",
       "       [  0, 403],\n",
       "       [  0, 404],\n",
       "       [  0, 405],\n",
       "       [  0, 406],\n",
       "       [  0, 407],\n",
       "       [  0, 408],\n",
       "       [  0, 409],\n",
       "       [  0, 410],\n",
       "       [  0, 411],\n",
       "       [  0, 412],\n",
       "       [  0, 413],\n",
       "       [  0, 414],\n",
       "       [  0, 415],\n",
       "       [  0, 416],\n",
       "       [  0, 417],\n",
       "       [  0, 418],\n",
       "       [  0, 419],\n",
       "       [  0, 420],\n",
       "       [  0, 421],\n",
       "       [  0, 422],\n",
       "       [  0, 423],\n",
       "       [  0, 424],\n",
       "       [  0, 425],\n",
       "       [  0, 426],\n",
       "       [  0, 427],\n",
       "       [  0, 428],\n",
       "       [  0, 429],\n",
       "       [  0, 430],\n",
       "       [  0, 431],\n",
       "       [  0, 432],\n",
       "       [  0, 433],\n",
       "       [  0, 434],\n",
       "       [  0, 435],\n",
       "       [  0, 436],\n",
       "       [  0, 437],\n",
       "       [  0, 438],\n",
       "       [  0, 439],\n",
       "       [  0, 440],\n",
       "       [  0, 441],\n",
       "       [  0, 442],\n",
       "       [  0, 443],\n",
       "       [  0, 444],\n",
       "       [  0, 445],\n",
       "       [  0, 446],\n",
       "       [  0, 447],\n",
       "       [  0, 448],\n",
       "       [  0, 449],\n",
       "       [  0, 450],\n",
       "       [  0, 451],\n",
       "       [  0, 452],\n",
       "       [  0, 453],\n",
       "       [  0, 454],\n",
       "       [  0, 455],\n",
       "       [  0, 456],\n",
       "       [  0, 457],\n",
       "       [  0, 458],\n",
       "       [  0, 459],\n",
       "       [  0, 460],\n",
       "       [  0, 461],\n",
       "       [  0, 462],\n",
       "       [  0, 463],\n",
       "       [  0, 464],\n",
       "       [  0, 465],\n",
       "       [  0, 466],\n",
       "       [  0, 467],\n",
       "       [  0, 468]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = torch.tensor(data, dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].long(), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConceptDataset = OntologyDataset(X_concepts, y_concepts)\n",
    "\n",
    "dataset_size = len(ConceptDataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainConceptDataset, testConceptDataset = random_split(ConceptDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoleDataset = OntologyDataset(X_roles, y_roles)\n",
    "\n",
    "dataset_size = len(RoleDataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainRoleDataset, testRoleDataset = torch.utils.data.random_split(RoleDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_ConceptDataLoader = DataLoader(trainConceptDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_ConceptDataLoader = DataLoader(testConceptDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_RoleDataLoader = DataLoader(trainRoleDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_RoleDataLoader = DataLoader(testRoleDataset, batch_size = BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithEL(nn.Module):\n",
    "    def __init__(self, emb_dim, gamma, individual_vocabulary, concept_vocabulary, role_vocabulary):\n",
    "        super(FaithEL, self).__init__()\n",
    "        self.margin_gamma = gamma\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.individual_embedding_dict = nn.Embedding(len(individual_vocabulary),\n",
    "                                                      emb_dim,\n",
    "                                                      )\n",
    "        \n",
    "        self.concept_embedding_dict = nn.Embedding(len(concept_vocabulary),\n",
    "                                                   emb_dim,\n",
    "                                                   )\n",
    "\n",
    "        self.role_embedding_dict = nn.Embedding(len(role_vocabulary),\n",
    "                                                emb_dim * 2,\n",
    "                                                )\n",
    "    \n",
    "    def forward(self, data):\n",
    "    \n",
    "        # Concept assertions are of the form ['Concept', 'Entity']\n",
    "        # Role assertions are of the form ['SubjectEntity', 'Role', 'ObjectEntity']\n",
    "        \n",
    "        subj_entity_idx = 1 if len(data[0]) == 2 else 0\n",
    "\n",
    "        if subj_entity_idx == 1:\n",
    "            concept_idx = 0\n",
    "\n",
    "            subj_entity = data[:, subj_entity_idx]\n",
    "            concept = data[:, concept_idx]\n",
    "\n",
    "            c_assertion_out1 = self.individual_embedding_dict(subj_entity) # Outputs the embedding for the individual\n",
    "            c_assertion_out2 = self.concept_embedding_dict(concept) # Outputs the moving parameter for the concept\n",
    "            \n",
    "            out = c_assertion_out2\n",
    "            out2 = c_assertion_out1\n",
    "            \n",
    "            return out, out2\n",
    "\n",
    "        elif subj_entity_idx == 0:\n",
    "            role_idx = 1\n",
    "            obj_entity_idx = 2\n",
    "        \n",
    "            subject_entity = self.individual_embedding_dict(data[:, subj_entity_idx])\n",
    "            object_entity = self.individual_embedding_dict(data[:, obj_entity_idx])\n",
    "            role = self.role_embedding_dict(data[:, role_idx])\n",
    "\n",
    "            r_assertion_out1 = torch.cat((subject_entity, object_entity), 1)\n",
    "            r_assertion_out2 = role\n",
    "\n",
    "            out = r_assertion_out1\n",
    "            out2 = r_assertion_out2\n",
    "            \n",
    "            return out, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.1\n",
    "EMB_DIM = 465\n",
    "model = FaithEL(EMB_DIM, GAMMA, entity_to_idx_vocab, concept_to_idx_vocab, role_to_idx_vocab)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, concept_dataloader, role_dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    for i, data in enumerate(concept_dataloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = model(inputs)\n",
    "        loss = loss_fn(outputs1, labels) + loss_fn(outputs2, outputs1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    for i, data in enumerate(role_dataloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = model(inputs)\n",
    "        loss = loss_fn(outputs1, labels) + loss_fn(outputs2, outputs1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, concept_dataloader, role_dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            loss = loss_fn(outputs1, labels) + loss_fn(outputs2, outputs1)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            loss = loss_fn(outputs1, labels) + loss_fn(outputs2, outputs1)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, concept_dataloader, role_dataloader, scoring_function):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "log_epoch = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_ConceptDataLoader, train_RoleDataLoader, loss_fn, optimizer)\n",
    "    test_loss = test(model, test_ConceptDataLoader, test_RoleDataLoader, loss_fn)\n",
    "\n",
    "    if epoch % log_epoch == 0:\n",
    "        print(f'Epoch {epoch}/{EPOCHS} -> Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Outputs: ranked list of predictions, with indication of whether the original fact is in the topk ones.\n",
    "\n",
    "For this I need the full list of entities, I need to choose whether to corrput the tail or the head, I also need the golden label.\n",
    "\n",
    "For example, lets say I have C(a), then the golden label is just y. Then I create several (how many?) corrupted triples + the correct one.\n",
    "I then pass them through the model, get the final embedding, take the distance of all of them from the golden label, rank them, and check whether\n",
    "the correct one is on the top. This is quite straightforward.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_concept_assertions(test_data_concept_assertions, entity_vocab=dict):\n",
    "    pass\n",
    "\n",
    "def corrupt_role_assertions():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k(model, test_data_concept_assertions, test_data_role_assertions,\n",
    "                  k, scoring_function, entity_vocab=dict, concept_vocab=dict,\n",
    "                  role_vocab=dict):\n",
    "    \n",
    "    top1 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "\n",
    "    corrupted_concept_assertions = corrupt_concept_assertions(test_data_concept_assertions, entity_vocab, concept_vocab)\n",
    "    corrupted_role_assertions = corrupt_role_assertions(test_data_role_assertions, entity_vocab, role_vocab)\n",
    "\n",
    "    c_assertion_scores = []\n",
    "    r_assertion_scores = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_data_concept_assertions:\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            scores = scoring_function(outputs1, labels) + scoring_function(outputs2, outputs1)\n",
    "            assertion_score = (inputs, scores, 'True')\n",
    "            c_assertion_scores.append(assertion_score)\n",
    "\n",
    "        for data in corrupted_concept_assertions:\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            scores = scoring_function(outputs1, labels) + scoring_function(outputs2, outputs1)\n",
    "            assertion_score = (inputs, scores, 'False')\n",
    "            c_assertion_scores.append(assertion_score)\n",
    "\n",
    "        for i, data in test_data_role_assertions:\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            scores = scoring_function(outputs1, labels) + scoring_function(outputs2, outputs1)\n",
    "            assertion_score = (inputs, scores, 'True')\n",
    "            r_assertion_scores.append(assertion_score)\n",
    "        for i, data in corrupted_role_assertions:\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            scores = scoring_function(outputs1, labels) + scoring_function(outputs2, outputs1)\n",
    "            assertion_score = (inputs, scores, 'False')\n",
    "            r_assertion_scores.append(assertion_score)\n",
    "\n",
    "    c_assertion_scores = torch.sort(torch.tensor(c_assertion_scores)[], dim=1)\n",
    "    r_assertion_scores = torch.tensor(r_assertion_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ConceptDataLoader.dataset.dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
