{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import owlready2 as owl\n",
    "from owlready2 import *\n",
    "import types\n",
    "\n",
    "import scipy\n",
    "#from scipy.spatial import ConvexHull\n",
    "#import cdd\n",
    "#from cdd import RepType, Matrix, Polyhedron\n",
    "#from fractions import Fraction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/goslimyeast.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/galennorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/gonorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELH Implementation/NormalizedOntologies/'\n",
    "dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/family_ontology.owl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class for creating entities to\n",
    "populate the creation of the\n",
    "canonical models.\n",
    "\n",
    "The .name attribute is used to\n",
    "create a single representation\n",
    "for concepts like A and B / \n",
    "B and A, as they are the same.\n",
    "'''\n",
    "\n",
    "class CanonicalModelElements:\n",
    "\n",
    "    concept_names = {}\n",
    "    concept_intersections = {}\n",
    "    concept_restrictions = {}\n",
    "    all_concepts = {}\n",
    "\n",
    "    def __init__(self, concept):\n",
    "        self.concept = concept\n",
    "        self.name = self.get_name()\n",
    "        self.get_element_dict()\n",
    "\n",
    "    def get_name(self):\n",
    "\n",
    "        # add \\Top\n",
    "        \n",
    "        if type(self.concept) == ThingClass:\n",
    "            return self.concept.name\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            return 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "        \n",
    "        else:\n",
    "            return 'And_' + ''.join(sorted(self.concept.Classes[0].name + self.concept.Classes[1].name)) # The name is sorted to avoid that (e.g) (A \\and B) and (B \\and A) are treated as different concepts\n",
    "        \n",
    "    def get_element_dict(self):\n",
    "\n",
    "        if type(self.concept) == ThingClass:\n",
    "            CanonicalModelElements.concept_names[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            CanonicalModelElements.concept_restrictions[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == And:\n",
    "            CanonicalModelElements.concept_intersections[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_model_elements(concept_names_iter, role_names_iter, ontology):\n",
    "    \n",
    "    onto = ontology\n",
    "    top = owl.Thing\n",
    "    bottom = owl.Nothing\n",
    "\n",
    "    CanonicalModelElements(top)\n",
    "    CanonicalModelElements(bottom)\n",
    "\n",
    "    for concept_name in concept_names_iter:\n",
    "        \n",
    "        CanonicalModelElements(concept_name)\n",
    "        for concept_name2 in concept_names_iter:\n",
    "        \n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(concept_name & concept_name2)\n",
    "                gca.is_a.append(concept_name & concept_name2)\n",
    "            \n",
    "            CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.')\n",
    "\n",
    "    concept_names_iter.append(top)\n",
    "    # concept_names_iter.append(bottom)\n",
    "\n",
    "    for role_name in role_names_iter:\n",
    "        for concept_name in concept_names_iter:\n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(role_name.some(concept_name))\n",
    "                gca.is_a.append(role_name.some(concept_name))\n",
    "\n",
    "            CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('All restrictions have been preprocessed for the creation of the canonical model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main class for creating the canonical model for the ontology.\n",
    "\n",
    "The canonical model is stored in dictionaries available as class variables 'concept_canonical_interpretation'\n",
    "and 'role_canonical_interpretation'. \n",
    "\n",
    "Args:\n",
    "    concept_names_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_intersection_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_restrictions_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    all_concepts_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    role_names_iter (list): a list containing all role names in the loaded ontology.\n",
    "'''\n",
    "\n",
    "class CanonicalModel:\n",
    "\n",
    "    concept_canonical_interpretation = {}\n",
    "    role_canonical_interpretation = {}\n",
    "\n",
    "    def __init__(self, concept_names_dict, concept_intersections_dict, concept_restrictions_dict, all_concepts_dict, role_names_iter):\n",
    "        \n",
    "        self.domain = all_concepts_dict\n",
    "        self.concept_names_dict = concept_names_dict\n",
    "        self.concept_restrictions_dict = concept_restrictions_dict\n",
    "        self.concept_intersections_dict = concept_intersections_dict\n",
    "\n",
    "        self.role_names_iter = role_names_iter\n",
    "\n",
    "        self.concept_canonical_interp = self.get_concept_name_caninterp() # These are only used to build the concept_canonical_interpretation and role_canonical_interpretation class attributes\n",
    "        self.role_canonical_interp = self.get_role_name_caninterp()       # The functions do not return anything, they just update the class variables\n",
    "\n",
    "    def get_concept_name_caninterp(self):\n",
    "\n",
    "        # The variable concept is a string containing the name of an element of the domain of the canonical model\n",
    "        # The key to the concept_names_dict variable corresponds to concept.name\n",
    "        # This name can be used to access the concept in owlready2's format\n",
    "\n",
    "        for concept in self.concept_names_dict.keys():\n",
    "\n",
    "            CanonicalModel.concept_canonical_interpretation[concept] = []\n",
    "            superclasses = self.domain[concept].concept.ancestors(include_self=True, include_constructs=True) # The self.domain[concept] is used to access the CanonicalModelElements type of object,\n",
    "                                                                                                               # and the attribute .concept is used to access the concept in owlready2 format\n",
    "                                                                                                              \n",
    "            for superclass in superclasses:\n",
    "\n",
    "                if type(superclass) == ThingClass:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append(superclass.name)\n",
    "\n",
    "                elif type(superclass) == Restriction:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append('exists_' + superclass.property.name + '.' + superclass.value.name)\n",
    "\n",
    "                elif type(superclass) == And:\n",
    "                    if 'And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)) in CanonicalModel.concept_canonical_interpretation[concept]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        CanonicalModel.concept_canonical_interpretation[concept].append('And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)))\n",
    "\n",
    "    def get_role_name_caninterp(self):\n",
    "\n",
    "        # First case from Definition 10\n",
    "\n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            role_name_str = role_name.name # Accesses the property type object's name as a string\n",
    "            CanonicalModel.role_canonical_interpretation[role_name_str] = []\n",
    "\n",
    "            for restriction_name in self.concept_restrictions_dict.keys(): # Where restriction_name denotes a \\exists r.B type of concept 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "                c_B = self.concept_restrictions_dict[restriction_name].concept.value.name\n",
    "\n",
    "                if role_name_str in restriction_name:\n",
    "                    superclasses = self.domain[restriction_name].concept.ancestors(include_self=True, include_constructs=False)\n",
    "\n",
    "                    for superclass in superclasses:\n",
    "                        super_superclasses = superclass.ancestors(include_self=True, include_constructs=True)\n",
    "\n",
    "                        for super_superclass in super_superclasses:\n",
    "\n",
    "                            if type(super_superclass) == ThingClass:\n",
    "                                c_D = super_superclass.name\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append(tuple(c_D, c_B))\n",
    "\n",
    "                            elif type(super_superclass) == Restriction:\n",
    "                                c_D = 'exists_' + super_superclass.property.name + '.' + super_superclass.value.name\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append(tuple(c_D, c_B))\n",
    "\n",
    "                            elif type(super_superclass) == And:\n",
    "                                c_D = 'And_' + ''.join(sorted(super_superclass.Classes[0].name + super_superclass.Classes[1].name))\n",
    "                                CanonicalModel.role_canonical_interpretation[role_name_str].append(tuple(c_D, c_B))\n",
    "\n",
    "        # Second case from Definition 10: r \\sqsubset s \n",
    "            \n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            superroles = role_name.ancestors(include_self=True)\n",
    "            role_name_str = role_name.name\n",
    "            \n",
    "            for superrole in superroles:\n",
    "                for restriction_name in self.concept_restrictions_dict.keys():\n",
    "                    if superrole.name in restriction_name:\n",
    "                        pair = tuple((restriction_name, self.domain[restriction_name].concept.value.name))\n",
    "                        CanonicalModel.role_canonical_interpretation[role_name_str].append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main function for creating the canonical model.\n",
    "\n",
    "    Args:\n",
    "        onto_dir (str): a string pointing to the directory where the ontology is stored.\n",
    "\n",
    "    Returns:\n",
    "        canmodel (CanonicalModel): returns a variable containing the canonical model. \n",
    "        \n",
    "        Attention: the interpretations of concept names and role names can also be accessed via class variables\n",
    "        from the CanonicalModel class.\n",
    "'''\n",
    "\n",
    "def create_canonical_model(onto_dir):\n",
    "\n",
    "    onto = get_ontology(onto_dir)\n",
    "    onto = onto.load()\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "\n",
    "    #for concept in concept_names_iter:\n",
    "    #    concept.namespace = onto.ontology\n",
    "\n",
    "    #for role in role_names_iter:\n",
    "    #    role.namespace = onto.ontology\n",
    "\n",
    "    get_canonical_model_elements(concept_names_iter, role_names_iter, onto)\n",
    "\n",
    "    print('')\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Starting to reason.')\n",
    "    print('')\n",
    "\n",
    "    with onto:\n",
    "        sync_reasoner()\n",
    "        \n",
    "    # onto.save(\"inferences_goslimyeast.owl\")\n",
    "\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "\n",
    "    print('')\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Done reasoning. Creating the canonical model.')\n",
    "    print('')\n",
    "    canmodel = CanonicalModel(CanonicalModelElements.concept_names, CanonicalModelElements.concept_intersections, CanonicalModelElements.concept_restrictions, CanonicalModelElements.all_concepts, role_names_iter)\n",
    "    print('============================================================================')\n",
    "    print('')\n",
    "    print('Concluded creating canonical model.')\n",
    "\n",
    "    return canmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.\n",
      "\n",
      "\n",
      "All restrictions have been preprocessed for the creation of the canonical model.\n",
      "\n",
      "============================================================================\n",
      "\n",
      "Starting to reason.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx2000M -cp /opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit:/opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////var/folders/wg/g5861gcs6k5d3rbq_rncztjw0000gn/T/tmphnw8cq4b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================\n",
      "\n",
      "Done reasoning. Creating the canonical model.\n",
      "\n",
      "============================================================================\n",
      "\n",
      "Concluded creating canonical model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * HermiT took 0.8157849311828613 seconds\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the canonical model\n",
    "\n",
    "canmodel = create_canonical_model(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for initializing\n",
    "the class EntityEmbedding. They\n",
    "allow us to access dictionaries\n",
    "containing indexes and canonical\n",
    "interpretation of concepts\n",
    "and roles as class.\n",
    "'''\n",
    "\n",
    "def get_concept_names_idx_dict(canmodel):\n",
    "   conceptnames_idx_dict = {concept_name: idx for idx, concept_name in enumerate(CanonicalModel.concept_canonical_interpretation.keys())}\n",
    "   return conceptnames_idx_dict\n",
    "\n",
    "def get_role_names_idx_dict(canmodel):\n",
    "    rolenames_idx_dict = {role_name: idx for idx, role_name in enumerate(CanonicalModel.role_canonical_interpretation.keys())}\n",
    "    return rolenames_idx_dict\n",
    "\n",
    "def get_entities_idx_dict(canmodel):\n",
    "    entities_idx_dict = {entity: idx for idx, entity in enumerate(canmodel.domain.keys())}\n",
    "    return entities_idx_dict\n",
    "\n",
    "def get_domain_dict(canmodel):\n",
    "    return canmodel.domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atenção: a função mu está com complexidade alta devido aos for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Class for obtaining the positional \n",
    "embedding for each entity in the domain\n",
    "of the canonical interpretation.\n",
    "It represents the Mu Function from the\n",
    "paper.\n",
    "'''\n",
    "\n",
    "class EntityEmbedding:\n",
    "\n",
    "    # Dictionaries for storing the indices of concept names and role names, entities pairs, respectively\n",
    "    # Keys are strings and values are integers\n",
    "    \n",
    "    concept_names_idx_dict = get_concept_names_idx_dict(canmodel)\n",
    "    role_names_idx_dict = get_role_names_idx_dict(canmodel)\n",
    "    entities_idx_dict = get_entities_idx_dict(canmodel)\n",
    "\n",
    "    # Dictionaries accessing the canonical interpretation of concepts and roles\n",
    "    # Keys and values are strings\n",
    "    \n",
    "    concept_canonical_interpretation_dict = CanonicalModel.concept_canonical_interpretation\n",
    "    role_canonical_interpretation_dict = CanonicalModel.role_canonical_interpretation\n",
    "\n",
    "    # Dictionary storing the domain of the canonical model being embedded\n",
    "    # IMPORTANT: Keys are strings and values are CanonicalModelElements type objects\n",
    "    \n",
    "    domain_dict = get_domain_dict(canmodel)\n",
    "\n",
    "    # Dictionary for easy access to entity embeddings\n",
    "    # It is initialized with empty values, iteratively built by the .get_embedding_vector() method\n",
    "    # Key (str): Domain Entity / Value (np.array): EntityEmbedding.embedding_vector\n",
    "\n",
    "    entity_entityvector_dict = dict.fromkeys(domain_dict.keys())\n",
    "\n",
    "    def __init__(self, entity_name, emb_dim):\n",
    "        self.name = entity_name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.in_interpretation_of = []\n",
    "        self.embedding_vector = self.get_embedding_vector()\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        \n",
    "        embedding_vector = np.zeros((self.emb_dim,))\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = []\n",
    "\n",
    "        # Applies the embedding function to the concept names portion of the definition\n",
    "\n",
    "        for concept_name in EntityEmbedding.concept_canonical_interpretation_dict:\n",
    "            concept_name_idx = EntityEmbedding.concept_names_idx_dict[concept_name]\n",
    "        \n",
    "            if self.name in EntityEmbedding.concept_canonical_interpretation_dict[concept_name]:\n",
    "                embedding_vector[concept_name_idx] = 1 * 10\n",
    "                self.in_interpretation_of.append(concept_name)\n",
    "\n",
    "        # Applies the embedding function to the role names portion of the definition\n",
    "\n",
    "        for role_name in EntityEmbedding.role_canonical_interpretation_dict:\n",
    "            \n",
    "            role_name_idx = len(EntityEmbedding.concept_names_idx_dict) + (EntityEmbedding.role_names_idx_dict[role_name] * len(EntityEmbedding.entities_idx_dict))\n",
    "            role_name_caninterp = EntityEmbedding.role_canonical_interpretation_dict[role_name]\n",
    "\n",
    "            for pair in role_name_caninterp:\n",
    "\n",
    "                entity_2 = pair[1]\n",
    "\n",
    "                if (self.name, entity_2) == pair:\n",
    "                    entity_2_idx = EntityEmbedding.entities_idx_dict[entity_2]\n",
    "                    final_role_entity_pair_idx = role_name_idx + entity_2_idx\n",
    "                    embedding_vector[final_role_entity_pair_idx] = 1 * 10\n",
    "\n",
    "        # EntityEmbedding.entity_entityvector_dict[self.name].append(embedding_vector)\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = embedding_vector\n",
    "\n",
    "        return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the binary vectors representing\n",
    "each element of the domain of the canonical interpretation.\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int/float): the number of dimensions of the embedding space.\n",
    "\n",
    "    Returns:\n",
    "        embedded_entities (list): a list containing all embeddings of the entities\n",
    "                                  in the domain. \n",
    "    \n",
    "    The embedded_entities are also available in the dictionary EntityEmbeddings.entity_entityvector_dict\n",
    "'''\n",
    "\n",
    "def get_domain_embeddings(emb_dim):\n",
    "\n",
    "    embedded_entities = []\n",
    "    counter = 0\n",
    "\n",
    "   # The entities in the domain are strings\n",
    "    \n",
    "    for entity_name in EntityEmbedding.domain_dict:\n",
    "       embedded_entity = EntityEmbedding(entity_name, emb_dim)\n",
    "       embedded_entities.append(embedded_entity)\n",
    "       counter += 1\n",
    "       \n",
    "       if counter % 1000 == 0:\n",
    "           print(counter)\n",
    "       \n",
    "    return embedded_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final class for creating the dataset.\n",
    "\n",
    "Inputs: concept or role names, generated\n",
    "embeddings for entities in the domain of\n",
    "the canonical model.\n",
    "\n",
    "Outputs: geometrical interpretation of\n",
    "concepts and role names, represented\n",
    "by vertices defining a region.\n",
    "\n",
    "One can access the GeometricInterpretation\n",
    "objects either as elements in a list, or as\n",
    "values in a class variable dictionary.\n",
    "'''\n",
    "\n",
    "class GeometricInterpretation:\n",
    "\n",
    "    concept_geointerps_dict = dict.fromkeys(CanonicalModel.concept_canonical_interpretation.keys())\n",
    "    role_geointerps_dict = dict.fromkeys(CanonicalModel.role_canonical_interpretation.keys())\n",
    "\n",
    "    def __init__(self, name, emb_dim):\n",
    "        self.name = name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vertices = []\n",
    "        self.centroid = None\n",
    "        self.bounding_box_vertices = None\n",
    "\n",
    "    def get_centroid_naive(self):\n",
    "        if len(self.vertices) == 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) == 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim * 2,)) # The centroid for the regions needs to be doubled due to the concat operation\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "\n",
    "    def get_bounding_box_vertices(self):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There has to be a more efficient way of doing the creating of geometric interpretations for concepts and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_finder(emb_dim, concept_name_idx_dict, role_name_idx_dict, domain_idx_dict):\n",
    "\n",
    "    index_dict = {k: None for k in range(emb_dim)}\n",
    "\n",
    "    for k,v in concept_name_idx_dict.items():\n",
    "\n",
    "        index_dict[v] = k\n",
    "\n",
    "    for role in role_name_idx_dict:\n",
    "        role_init_idx = len(concept_name_idx_dict) + (role_name_idx_dict[role] * len(domain_idx_dict))\n",
    "\n",
    "        for entity in domain_idx_dict:\n",
    "            entity_init_idx = domain_idx_dict[entity]\n",
    "            final_role_entity_pair_idx = role_init_idx + entity_init_idx\n",
    "            index_dict[final_role_entity_pair_idx] = (role, entity)\n",
    "        \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "\n",
    "    faithful_concept_geometric_interps = []\n",
    "\n",
    "    for concept_name in concept_names_interps.keys():\n",
    "        concept_name = GeometricInterpretation(concept_name, emb_dim)\n",
    "\n",
    "        for embedding in domain_embeddings_list:\n",
    "            if concept_name.name in embedding.in_interpretation_of:\n",
    "                concept_name.vertices.append(embedding.embedding_vector)\n",
    "            \n",
    "        GeometricInterpretation.concept_geointerps_dict[concept_name.name] = concept_name\n",
    "        concept_name.centroid = concept_name.get_centroid_naive()\n",
    "        \n",
    "        faithful_concept_geometric_interps.append(concept_name)\n",
    "\n",
    "    return faithful_concept_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_role_geometric_interps(role_names_interps, entity_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "    \n",
    "    faithful_role_geometric_interps = []\n",
    "    idx_entity_dict = entity_dims_index_dict\n",
    "    #entity_idx_dict = {v: k for k,v in entity_dims_index_dict}\n",
    "\n",
    "    relevant_idxs = len(canmodel.concept_names_dict)-1\n",
    "\n",
    "    for role_name in role_names_interps.keys():\n",
    "        role_name_str = role_name\n",
    "        role_name = GeometricInterpretation(role_name_str, emb_dim)\n",
    "\n",
    "        for entity in entity_embeddings_list:\n",
    "\n",
    "            onehot_idx_list = np.where(entity.embedding_vector == 1)[0]\n",
    "            #print(f'This is the entity: {entity} and this is the onehot_idx_list: {onehot_idx_list}')\n",
    "\n",
    "            for idx in onehot_idx_list: # I could just look at the TRULY relevant indexes\n",
    "                if idx > relevant_idxs:\n",
    "                    role_entity_pair = idx_entity_dict[idx]\n",
    "                    r_name_str = role_entity_pair[0]\n",
    "                    e_name_str = role_entity_pair[1]\n",
    "\n",
    "                    if r_name_str == role_name_str:\n",
    "                        e_embedding = EntityEmbedding.entity_entityvector_dict[e_name_str]\n",
    "                        role_name.vertices.append(np.concatenate((entity.embedding_vector, e_embedding)))\n",
    "\n",
    "        GeometricInterpretation.role_geointerps_dict[role_name_str] = role_name\n",
    "        role_name.centroid = role_name.get_centroid_naive()\n",
    "        faithful_role_geometric_interps.append(role_name)\n",
    "\n",
    "    return faithful_role_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tbox_embeddings(canonical_model: CanonicalModel):\n",
    "\n",
    "    domain = canonical_model.domain # Keys are strings and values are CanonicalModelElements type objects\n",
    "    concept_names_interps = canonical_model.concept_canonical_interpretation # Keys are strings and values are lists of strings.\n",
    "    role_names_interps = canonical_model.role_canonical_interpretation # Keys are strings and values are lists of tuples. Tuples are of form ('C', 'D'), with C and D strings.\n",
    "\n",
    "    EMB_DIM = len(concept_names_interps) + len(role_names_interps) * len(domain)\n",
    "\n",
    "    print('================EMBEDDING DIMENSION================')\n",
    "    print(f'Concept Name dimensions: {len(concept_names_interps)}')\n",
    "    print(f'The number of role names is: {len(role_names_interps)}')\n",
    "    print(f'The size of the domain is: {len(domain)}')\n",
    "    print(f'Role names dimensions: {len(role_names_interps) * len(domain)}')\n",
    "    print('===================================================')\n",
    "    print('')\n",
    "    print(f'Final embedding dimension: {EMB_DIM}')\n",
    "    print(f'The final dimension for role regions is: {EMB_DIM * 2}')\n",
    "\n",
    "    domain_embeddings_list = get_domain_embeddings(EMB_DIM)\n",
    "    \n",
    "    concept_names_ordering = EntityEmbedding.concept_names_idx_dict\n",
    "    role_names_ordering = EntityEmbedding.role_names_idx_dict\n",
    "    entities_ordering = EntityEmbedding.entities_idx_dict\n",
    "    \n",
    "    print('')\n",
    "    print('===============FINISHED EMBEDDINGS===============')\n",
    "    print(f'There are {len(domain_embeddings_list)} vector embeddings.')\n",
    "    print('')\n",
    "\n",
    "    index_finder_dict = index_finder(EMB_DIM, concept_names_ordering, role_names_ordering, entities_ordering)\n",
    "\n",
    "    faithful_concept_geometric_interps = get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('============FINISHED INTERPS CONCEPT=============')\n",
    "    print(f'There are {len(faithful_concept_geometric_interps)} regions for concept names.')\n",
    "    print('')\n",
    "\n",
    "    faithful_role_geometric_interps = get_faithful_role_geometric_interps(role_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('=============FINISHED INTERPS ROLES==============')\n",
    "    print(f'There are {len(faithful_role_geometric_interps)} regions for role names.')\n",
    "    print('')\n",
    "\n",
    "    return domain_embeddings_list, faithful_concept_geometric_interps, faithful_role_geometric_interps, index_finder_dict # Returns the faithful geometric interpretations for concepts and roles as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================EMBEDDING DIMENSION================\n",
      "Concept Name dimensions: 10\n",
      "The number of role names is: 5\n",
      "The size of the domain is: 91\n",
      "Role names dimensions: 455\n",
      "===================================================\n",
      "\n",
      "Final embedding dimension: 465\n",
      "The final dimension for role regions is: 930\n",
      "\n",
      "===============FINISHED EMBEDDINGS===============\n",
      "There are 91 vector embeddings.\n",
      "\n",
      "============FINISHED INTERPS CONCEPT=============\n",
      "There are 10 regions for concept names.\n",
      "\n",
      "=============FINISHED INTERPS ROLES==============\n",
      "There are 5 regions for role names.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "domain_embeddings, concept_geointerps, role_geointerps, idx_finder_dict = create_tbox_embeddings(canmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the pre-split dataset containing facts from the ontology.\n",
    "Distinguishes between concept assertions and role assertions.\n",
    "\n",
    "\n",
    "    Args: ontology_dir (str): the directory from the ontology\n",
    "          concept_geointerps_dict (dict): the geometrical interpretations for concepts generated by create_tbox_embeddings()\n",
    "          role_geointerps_dict (dict): the geometrical interpretations for roles generated by create_tbox_embeddings()\n",
    "\n",
    "    Returns:\n",
    "          X_concepts (np.array): A dataframe with columns 'Concept', 'Entity', 'y_true' (equivalent to concept.centroid())\n",
    "          X_roles (np.array): A dataframe with columns 'SubjectEntity', 'Role', 'ObjectEntity', 'y_true' (equivalent to role.centroid())\n",
    "          y_concepts (np.array):\n",
    "          y_roles (np.array):\n",
    "          vocabulary_dict (dict): A vocabulary with key (int): value (str) for entities in the domain.\n",
    "'''\n",
    "\n",
    "def get_abox_dataset(ontology_dir: str, concept_geointerps_dict: dict, role_geointerps_dict: dict, concept_to_idx: dict, role_to_idx: dict):\n",
    "    \n",
    "    ontology = get_ontology(ontology_dir)\n",
    "    ontology = ontology.load()\n",
    "    \n",
    "    X_concepts = []\n",
    "    X_roles = []\n",
    "    y_concepts = []\n",
    "    y_roles = []\n",
    "\n",
    "    entities = [entity.name for entity in list(ontology.individuals())]\n",
    "    \n",
    "    concept_to_idx_vocab = concept_to_idx\n",
    "    idx_to_concept_vocab = {value: key for key, value in concept_to_idx_vocab.items()}\n",
    "\n",
    "    role_to_idx_vocab = role_to_idx\n",
    "    idx_to_role_vocab = {value: key for key, value in role_to_idx_vocab.items()}\n",
    "    \n",
    "    entity_to_idx_vocab = {value: index for index, value in enumerate(entities)}\n",
    "    idx_to_entity_vocab = {value: key for key, value in entity_to_idx_vocab.items()}\n",
    "\n",
    "    for individual in list(ontology.individuals()):\n",
    "\n",
    "        all_facts = individual.is_a # Is this actually getting all assertions?\n",
    "\n",
    "        for concept in all_facts:\n",
    "            if type(concept) == ThingClass:\n",
    "                concept = concept_geointerps_dict[concept.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "\n",
    "            # There are actually no assertions of the type A and B(a)\n",
    "            # Check later how to get them\n",
    "                \n",
    "            elif type(concept) == And:\n",
    "                print('And') # remove print statement\n",
    "                concept1 = concept_geointerps_dict[concept.Classes[0].name]\n",
    "                concept2 = concept_geointerps_dict[concept.Classes[1].name]\n",
    "                concept_name = 'And_' + ''.join(sorted(concept1.name + concept2.name))\n",
    "                fact = np.array([concept_to_idx_vocab[concept_name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "\n",
    "            # There are actually no assertions of the type exists r.B\n",
    "            # Check later how to get them\n",
    "            \n",
    "            elif type(concept) == Restriction:\n",
    "                print('restriction') # remove print statement\n",
    "                concept = concept_geointerps_dict['exists_' + concept.property.name + '.' + concept.value.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "\n",
    "        relevant_roles = individual.get_properties()\n",
    "        individual_name = individual.name\n",
    "\n",
    "        for role in relevant_roles:\n",
    "            role_geo = role_geointerps_dict[role.name]\n",
    "            subject_list = role[individual] # This syntax is from the owlready2 library\n",
    "            for subject in subject_list:\n",
    "                fact = np.array([entity_to_idx_vocab[individual.name], role_to_idx_vocab[role.name], entity_to_idx_vocab[subject.name]])\n",
    "                X_roles.append(fact)\n",
    "                y_label = y_roles.append(np.array(role_geo.centroid))\n",
    "\n",
    "    return np.array(X_concepts), np.array(X_roles), np.array(y_concepts), np.array(y_roles), entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concepts, X_roles, y_concepts, y_roles, entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab = get_abox_dataset(dir,\n",
    "                                                                                                                                                                                        GeometricInterpretation.concept_geointerps_dict,\n",
    "                                                                                                                                                                                        GeometricInterpretation.role_geointerps_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.concept_names_idx_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.role_names_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = torch.tensor(data, dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].long(), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE_PROPORTION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConceptDataset = OntologyDataset(X_concepts, y_concepts)\n",
    "\n",
    "dataset_size = len(ConceptDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainConceptDataset, testConceptDataset = random_split(ConceptDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "RoleDataset = OntologyDataset(X_roles, y_roles)\n",
    "\n",
    "dataset_size = len(RoleDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainRoleDataset, testRoleDataset = torch.utils.data.random_split(RoleDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_ConceptDataLoader = DataLoader(trainConceptDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_ConceptDataLoader = DataLoader(testConceptDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_RoleDataLoader = DataLoader(trainRoleDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_RoleDataLoader = DataLoader(testRoleDataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithEL(nn.Module):\n",
    "    def __init__(self, emb_dim, phi, individual_vocabulary, concept_vocabulary, role_vocabulary):\n",
    "        super(FaithEL, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.phi = phi\n",
    "        \n",
    "        self.individual_embedding_dict = nn.Embedding(len(individual_vocabulary),\n",
    "                                                      emb_dim,\n",
    "                                                      max_norm=1 * 100\n",
    "                                                      )\n",
    "        \n",
    "        self.concept_embedding_dict = nn.Embedding(len(concept_vocabulary),\n",
    "                                                   emb_dim,\n",
    "                                                   max_norm=1 * 100\n",
    "                                                   )\n",
    "\n",
    "        self.role_embedding_dict = nn.Embedding(len(role_vocabulary),\n",
    "                                                emb_dim * 2,\n",
    "                                                max_norm=1 * 100)\n",
    "    \n",
    "    def forward(self, data):\n",
    "    \n",
    "        # Concept assertions are of the form ['Concept', 'Entity']\n",
    "        # Role assertions are of the form ['SubjectEntity', 'Role', 'ObjectEntity']\n",
    "        \n",
    "        subj_entity_idx = 1 if len(data[0]) == 2 else 0 # Performs a check to see whether the model has received a C assert or R assert\n",
    "\n",
    "        if subj_entity_idx == 1:\n",
    "            concept_idx = 0\n",
    "\n",
    "            subj_entity = data[:, subj_entity_idx]\n",
    "            concept = data[:, concept_idx]\n",
    "\n",
    "            c_assertion_out1 = self.concept_embedding_dict(concept) # Outputs the moving parameter for the concept\n",
    "            c_assertion_out2 = self.individual_embedding_dict(subj_entity) # Outputs the embedding for the individual\n",
    "            \n",
    "            out1 = c_assertion_out1\n",
    "            out2 = c_assertion_out2\n",
    "            \n",
    "            return out1, out2\n",
    "\n",
    "        elif subj_entity_idx == 0:\n",
    "            role_idx = 1\n",
    "            obj_entity_idx = 2\n",
    "        \n",
    "            subject_entity = self.individual_embedding_dict(data[:, subj_entity_idx])\n",
    "            object_entity = self.individual_embedding_dict(data[:, obj_entity_idx])\n",
    "            role = self.role_embedding_dict(data[:, role_idx])\n",
    "\n",
    "            r_assertion_out1 = role # Role parameter embedding\n",
    "            r_assertion_out2 = torch.cat((subject_entity, object_entity), 1) # Concatenation of subject and object\n",
    "\n",
    "            out1 = r_assertion_out1\n",
    "            out2 = r_assertion_out2\n",
    "            \n",
    "            return out1, out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "PHI = 0\n",
    "EMB_DIM = 465\n",
    "model = FaithEL(EMB_DIM, PHI, entity_to_idx_vocab, concept_to_idx_vocab, role_to_idx_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, concept_dataloader, role_dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    for i, data in enumerate(concept_dataloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "        loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    for i, data in enumerate(role_dataloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "        loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    for i, data in enumerate(role_dataloader):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "        loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, concept_dataloader, role_dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_epoch = 10\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500 -> Train Loss: 12.8823 | Test Loss: 7.9096\n",
      "Epoch 20/500 -> Train Loss: 11.9268 | Test Loss: 7.4275\n",
      "Epoch 30/500 -> Train Loss: 11.0544 | Test Loss: 6.9835\n",
      "Epoch 40/500 -> Train Loss: 10.2552 | Test Loss: 6.5734\n",
      "Epoch 50/500 -> Train Loss: 9.5227 | Test Loss: 6.1942\n",
      "Epoch 60/500 -> Train Loss: 8.8515 | Test Loss: 5.8435\n",
      "Epoch 70/500 -> Train Loss: 8.2373 | Test Loss: 5.5193\n",
      "Epoch 80/500 -> Train Loss: 7.6757 | Test Loss: 5.2199\n",
      "Epoch 90/500 -> Train Loss: 7.1627 | Test Loss: 4.9434\n",
      "Epoch 100/500 -> Train Loss: 6.6944 | Test Loss: 4.6880\n",
      "Epoch 110/500 -> Train Loss: 6.2670 | Test Loss: 4.4523\n",
      "Epoch 120/500 -> Train Loss: 5.8769 | Test Loss: 4.2346\n",
      "Epoch 130/500 -> Train Loss: 5.5207 | Test Loss: 4.0333\n",
      "Epoch 140/500 -> Train Loss: 5.1953 | Test Loss: 3.8472\n",
      "Epoch 150/500 -> Train Loss: 4.8976 | Test Loss: 3.6748\n",
      "Epoch 160/500 -> Train Loss: 4.6249 | Test Loss: 3.5150\n",
      "Epoch 170/500 -> Train Loss: 4.3748 | Test Loss: 3.3666\n",
      "Epoch 180/500 -> Train Loss: 4.1449 | Test Loss: 3.2286\n",
      "Epoch 190/500 -> Train Loss: 3.9332 | Test Loss: 3.1001\n",
      "Epoch 200/500 -> Train Loss: 3.7379 | Test Loss: 2.9802\n",
      "Epoch 210/500 -> Train Loss: 3.5574 | Test Loss: 2.8682\n",
      "Epoch 220/500 -> Train Loss: 3.3903 | Test Loss: 2.7634\n",
      "Epoch 230/500 -> Train Loss: 3.2352 | Test Loss: 2.6652\n",
      "Epoch 240/500 -> Train Loss: 3.0910 | Test Loss: 2.5730\n",
      "Epoch 250/500 -> Train Loss: 2.9568 | Test Loss: 2.4864\n",
      "Epoch 260/500 -> Train Loss: 2.8315 | Test Loss: 2.4048\n",
      "Epoch 270/500 -> Train Loss: 2.7145 | Test Loss: 2.3280\n",
      "Epoch 280/500 -> Train Loss: 2.6050 | Test Loss: 2.2555\n",
      "Epoch 290/500 -> Train Loss: 2.5022 | Test Loss: 2.1870\n",
      "Epoch 300/500 -> Train Loss: 2.4057 | Test Loss: 2.1222\n",
      "Epoch 310/500 -> Train Loss: 2.3150 | Test Loss: 2.0609\n",
      "Epoch 320/500 -> Train Loss: 2.2295 | Test Loss: 2.0028\n",
      "Epoch 330/500 -> Train Loss: 2.1490 | Test Loss: 1.9477\n",
      "Epoch 340/500 -> Train Loss: 2.0730 | Test Loss: 1.8954\n",
      "Epoch 350/500 -> Train Loss: 2.0013 | Test Loss: 1.8457\n",
      "Epoch 360/500 -> Train Loss: 1.9336 | Test Loss: 1.7986\n",
      "Epoch 370/500 -> Train Loss: 1.8697 | Test Loss: 1.7538\n",
      "Epoch 380/500 -> Train Loss: 1.8093 | Test Loss: 1.7113\n",
      "Epoch 390/500 -> Train Loss: 1.7522 | Test Loss: 1.6709\n",
      "Epoch 400/500 -> Train Loss: 1.6982 | Test Loss: 1.6325\n",
      "Epoch 410/500 -> Train Loss: 1.6472 | Test Loss: 1.5960\n",
      "Epoch 420/500 -> Train Loss: 1.5990 | Test Loss: 1.5613\n",
      "Epoch 430/500 -> Train Loss: 1.5533 | Test Loss: 1.5283\n",
      "Epoch 440/500 -> Train Loss: 1.5101 | Test Loss: 1.4970\n",
      "Epoch 450/500 -> Train Loss: 1.4693 | Test Loss: 1.4671\n",
      "Epoch 460/500 -> Train Loss: 1.4307 | Test Loss: 1.4388\n",
      "Epoch 470/500 -> Train Loss: 1.3941 | Test Loss: 1.4118\n",
      "Epoch 480/500 -> Train Loss: 1.3595 | Test Loss: 1.3861\n",
      "Epoch 490/500 -> Train Loss: 1.3268 | Test Loss: 1.3617\n",
      "Epoch 500/500 -> Train Loss: 1.2958 | Test Loss: 1.3385\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train(model, train_ConceptDataLoader, train_RoleDataLoader, loss_fn, optimizer)\n",
    "    test_loss = test(model, test_ConceptDataLoader, test_RoleDataLoader, loss_fn)\n",
    "\n",
    "    if epoch % log_epoch == 0:\n",
    "        print(f'Epoch {epoch}/{EPOCHS} -> Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOutputs: ranked list of predictions, with indication of whether the original fact is in the topk ones.\\n\\nFor this I need the full list of entities, I need to choose whether to corrput the tail or the head, I also need the golden label.\\n\\nFor example, lets say I have C(a), then the golden label is just y. Then I create several (how many?) corrupted triples + the correct one.\\nI then pass them through the model, get the final embedding, take the distance of all of them from the golden label, rank them, and check whether\\nthe correct one is on the top. This is quite straightforward.\\n'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Outputs: ranked list of predictions, with indication of whether the original fact is in the topk ones.\n",
    "\n",
    "For this I need the full list of entities, I need to choose whether to corrput the tail or the head, I also need the golden label.\n",
    "\n",
    "For example, lets say I have C(a), then the golden label is just y. Then I create several (how many?) corrupted triples + the correct one.\n",
    "I then pass them through the model, get the final embedding, take the distance of all of them from the golden label, rank them, and check whether\n",
    "the correct one is on the top. This is quite straightforward.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(hit_ratios, hit_list):\n",
    "\n",
    "    k_values = [1, 3, 10, 100, 469]\n",
    "    \n",
    "    for i in range(len(hit_ratios)):\n",
    "            print(f'Top{str(k_values[i])} hits: {hit_list[i]}')\n",
    "            print(f'Hits@{str(k_values[i])}: {hit_ratios[i]}')\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 3, 10, 100, 469]\n",
    "CENTROID_SCORE = False # When set to True, model scores assertion w.r.t distance to the centroid instead of to the moving parameter for concepts/roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_concept_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    relevant_concept_idx = []\n",
    "\n",
    "    # Gathers only concepts appearing in the test set (it is not guaranteed that if a concept appears in the dataset, then it appears here)\n",
    "\n",
    "    for assertion in test_concept_assertions:\n",
    "        inputs, _ = assertion\n",
    "        if inputs[0] not in relevant_concept_idx:\n",
    "            relevant_concept_idx.append(inputs[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # iterate over concepts appearing on test set\n",
    "        # rank scores by individual name\n",
    "        # incrementar os counters se tiver um hit@k\n",
    "        # dividir os counters de hits pelo total dos sets\n",
    "        \n",
    "        print(f'Number of concepts appearing on the test set: {len(relevant_concept_idx)}')\n",
    "        print('')\n",
    "\n",
    "        for concept_idx in relevant_concept_idx:\n",
    "            assertion_scores = []\n",
    "\n",
    "            for _, entity_idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([concept_idx, entity_idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample) # out1 = Concept parameter, out2 = Individual parameter\n",
    "\n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                else:\n",
    "                    assertion_score = torch.dist(GeometricInterpretation.concept_geointerps_dict[idx_to_entity_vocab[entity_idx]].centroid, outputs2, p=2) # Spaghetti, Italians should be proud of my code BUGGED DO NOT USE\n",
    "\n",
    "                assertion_scores.append((torch.tensor([concept_idx, entity_idx]), assertion_score.item()))\n",
    "            \n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "            hit_k_values = []\n",
    "\n",
    "            true_samples = [inputs for inputs, _ in test_concept_assertions if inputs[0] == concept_idx] # This is problematic when dealing with big datasets\n",
    "\n",
    "            #print(f'Dealing with concept idx: {concept_idx}')\n",
    "            #print(f'True samples: {true_samples}')\n",
    "            #print(f'Len true_samples: {len(true_samples)}')\n",
    "            #print(f'Sorted scores: {sorted_scores}')\n",
    "            #print(f'Len sorted scores: {len(sorted_scores)}')\n",
    "            #print('')\n",
    "\n",
    "            for k in k_list:\n",
    "                hit_k = any(torch.equal(scored_sample[0], true_sample) for true_sample in true_samples for scored_sample in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "            \n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            top1 += int(hit_k_values[0])\n",
    "            top3 += int(hit_k_values[1])\n",
    "            top10 += int(hit_k_values[2])\n",
    "            top100 += int(hit_k_values[3])\n",
    "            top_all += int(hit_k_values[4])\n",
    "\n",
    "            #if hit_k_values[0]:  # Check if top1 hit\n",
    "            #    top1 += 1\n",
    "            #if hit_k_values[1]:  # Check if top3 hit\n",
    "            #    top3 += 1\n",
    "            #if hit_k_values[2]:  # Check if top10 hit\n",
    "            #    top10 += 1\n",
    "            #if hit_k_values[3]:  # Check if top100 hit\n",
    "            #    top100 += 1\n",
    "            #if hit_k_values[4]:\n",
    "            #    top130 += 1\n",
    "\n",
    "    hits_at_k = [sum(hit_values) / len(hit_values) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    return hits_at_k, [top1, top3, top10, top100, top_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of concepts appearing on the test set: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hits_at_k_concept, hit_list_concept = get_hits_at_k_concept_assertions(model,\n",
    "                                             testConceptDataset, testRoleDataset,\n",
    "                                             entity_to_idx_vocab, idx_to_entity_vocab,\n",
    "                                             idx_to_concept_vocab, role_to_idx_vocab,\n",
    "                                             centroid_score = CENTROID_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 0\n",
      "Hits@1: 0.0\n",
      "\n",
      "Top3 hits: 0\n",
      "Hits@3: 0.0\n",
      "\n",
      "Top10 hits: 3\n",
      "Hits@10: 0.5\n",
      "\n",
      "Top100 hits: 5\n",
      "Hits@100: 0.8333333333333334\n",
      "\n",
      "Top469 hits: 6\n",
      "Hits@469: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(hits_at_k_concept, hit_list_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 0\n",
      "Hits@1: 0.0\n",
      "\n",
      "Top3 hits: 1\n",
      "Hits@3: 0.16666666666666666\n",
      "\n",
      "Top10 hits: 4\n",
      "Hits@10: 0.6666666666666666\n",
      "\n",
      "Top100 hits: 5\n",
      "Hits@100: 0.8333333333333334\n",
      "\n",
      "Top469 hits: 6\n",
      "Hits@469: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(hits_at_k_concept, hit_list_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 0\n",
      "Hits@1: 0.0\n",
      "\n",
      "Top3 hits: 2\n",
      "Hits@3: 0.3333333333333333\n",
      "\n",
      "Top10 hits: 2\n",
      "Hits@10: 0.3333333333333333\n",
      "\n",
      "Top100 hits: 5\n",
      "Hits@100: 0.8333333333333334\n",
      "\n",
      "Top469 hits: 6\n",
      "Hits@469: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(hits_at_k_concept, hit_list_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_role_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "    \n",
    "    relevant_head_role_queries = []\n",
    "    relevant_role_tail_queries = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for assertion in test_role_assertions:\n",
    "            inputs, _ = assertion\n",
    "            head_entity_idx = inputs[0]\n",
    "            role_entity_idx = inputs[1]\n",
    "            assertion_scores = []\n",
    "\n",
    "            for _, tail_entity_idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample)\n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            assertion_scores.append((torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]), assertion_score.item()))\n",
    "\n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "            hit_k_values = []\n",
    "\n",
    "            for k in k_list:\n",
    "                hit_k = any(torch.equal(scored_sample[0], inputs) for scored_sample in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "            \n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            top1 += int(hit_k_values[0])\n",
    "            top3 += int(hit_k_values[1])\n",
    "            top10 += int(hit_k_values[2])\n",
    "            top100 += int(hit_k_values[3])\n",
    "            top_all += int(hit_k_values[4])\n",
    "\n",
    "\n",
    "    hits_at_k = [sum(hit_values) / len(hit_values) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    return hits_at_k, [top1, top3, top10, top100, top_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k_role, hit_list_role = get_hits_at_k_role_assertions(model,\n",
    "                                             testConceptDataset, testRoleDataset,\n",
    "                                             entity_to_idx_vocab, idx_to_entity_vocab,\n",
    "                                             idx_to_concept_vocab, role_to_idx_vocab,\n",
    "                                             centroid_score = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 0\n",
      "Hits@1: 0.0\n",
      "\n",
      "Top3 hits: 0\n",
      "Hits@3: 0.0\n",
      "\n",
      "Top10 hits: 0\n",
      "Hits@10: 0.0\n",
      "\n",
      "Top100 hits: 0\n",
      "Hits@100: 0.0\n",
      "\n",
      "Top469 hits: 0\n",
      "Hits@469: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is with PHI = 0\n",
    "print_results(hits_at_k_role, hit_list_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_concept_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top469 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # iterate over concepts appearing on test set\n",
    "        # rank scores by individual name\n",
    "        # incrementar os counters se tiver um hit@k\n",
    "        # dividir os counters de hits pelo total dos sets\n",
    "\n",
    "        for assertion in test_concept_assertions:\n",
    "            inputs, labels = assertion\n",
    "            true_idx = inputs[1] # Inputs are of the form ['Concept', 'Individual']\n",
    "            assertion_scores = [] # Stores the scores for all individuals\n",
    "\n",
    "            for _, idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([inputs[0], idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample) # out1 = Concept parameter, out2 = Individual parameter\n",
    "                \n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2) # The scoring function is just the euclidean distance\n",
    "                else:\n",
    "                    assertion_score = torch.dist(labels, outputs2, p=2)\n",
    "    \n",
    "                assertion_scores.append((idx, assertion_score.item()))\n",
    "                \n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            k_list = [1, 3, 10, 100, 469]\n",
    "            hit_k_values = []  # Store hit_k values for each k\n",
    "\n",
    "            for k in k_list:\n",
    "                hit_k = any(idx == true_idx for idx, _ in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "\n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            if hit_k_values[0]:  # Check if top1 hit\n",
    "                top1 += 1\n",
    "            if hit_k_values[1]:  # Check if top3 hit\n",
    "                top3 += 1\n",
    "            if hit_k_values[2]:  # Check if top10 hit\n",
    "                top10 += 1\n",
    "            if hit_k_values[3]:  # Check if top100 hit\n",
    "                top100 += 1\n",
    "            if hit_k_values[4]:\n",
    "                top469 += 1\n",
    "\n",
    "    hits_at_k = [sum(hit_values) / len(hit_values) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    return hits_at_k, [top1, top3, top10, top100, top469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k_concept, hit_list_concept = get_hits_at_k_concept_assertions(model,\n",
    "                                             testConceptDataset, testRoleDataset,\n",
    "                                             entity_to_idx_vocab, idx_to_entity_vocab,\n",
    "                                             idx_to_concept_vocab, role_to_idx_vocab,\n",
    "                                             centroid_score = CENTROID_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.038461538461538464, 0.4, 1.0] [0, 0, 5, 52, 130]\n"
     ]
    }
   ],
   "source": [
    "print(hits_at_k_concept, hit_list_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_role_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top469 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for assertion in test_role_assertions:\n",
    "            inputs, labels = assertion\n",
    "            true_idx = inputs[2] # Inputs are of the form ['Head', 'Role', 'Tail']\n",
    "            assertion_scores = [] # Stores the scores for all individuals\n",
    "\n",
    "            for _, idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([inputs[0], inputs[1], idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample) # out1 = Role parameter, out2 = Tail parameter\n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2) # The scoring function is just the euclidean distance\n",
    "                else:\n",
    "                    assertion_score = torch.dist(labels, outputs2, p=2)\n",
    "                assertion_scores.append((idx, assertion_score.item()))\n",
    "                \n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            k_list = [1, 3, 10, 100, 469]\n",
    "            hit_k_values = []  # Store hit_k values for each k\n",
    "            for k in k_list:\n",
    "                hit_k = any(idx == true_idx for idx, _ in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "\n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            if hit_k_values[0]:  # Check if top1 hit\n",
    "                top1 += 1\n",
    "            if hit_k_values[1]:  # Check if top3 hit\n",
    "                top3 += 1\n",
    "            if hit_k_values[2]:  # Check if top10 hit\n",
    "                top10 += 1\n",
    "            if hit_k_values[3]:  # Check if top100 hit\n",
    "                top100 += 1\n",
    "            if hit_k_values[4]:\n",
    "                top469 += 1\n",
    "\n",
    "    hits_at_k = [sum(hit_values) / len(hit_values) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    return hits_at_k, [top1, top3, top10, top100, top469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_at_k_role, hit_list_role = get_hits_at_k_role_assertions(model,\n",
    "                                             testConceptDataset, testRoleDataset,\n",
    "                                             entity_to_idx_vocab, idx_to_entity_vocab,\n",
    "                                             idx_to_concept_vocab, role_to_idx_vocab,\n",
    "                                             centroid_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 0\n",
      "Hits@1: 0.0\n",
      "\n",
      "Top3 hits: 11\n",
      "Hits@3: 0.049107142857142856\n",
      "\n",
      "Top10 hits: 35\n",
      "Hits@10: 0.15625\n",
      "\n",
      "Top100 hits: 107\n",
      "Hits@100: 0.47767857142857145\n",
      "\n",
      "Top469 hits: 224\n",
      "Hits@469: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(hits_at_k_role, hit_list_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 0\n",
      "Hits@1: 0.0\n",
      "\n",
      "Top3 hits: 11\n",
      "Hits@3: 0.049107142857142856\n",
      "\n",
      "Top10 hits: 35\n",
      "Hits@10: 0.15625\n",
      "\n",
      "Top100 hits: 107\n",
      "Hits@100: 0.47767857142857145\n",
      "\n",
      "Top469 hits: 224\n",
      "Hits@469: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(hits_at_k_role, hit_list_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top1 hits: 2\n",
      "Hits@1: 0.008928571428571428\n",
      "\n",
      "Top3 hits: 3\n",
      "Hits@3: 0.013392857142857142\n",
      "\n",
      "Top10 hits: 11\n",
      "Hits@10: 0.049107142857142856\n",
      "\n",
      "Top100 hits: 38\n",
      "Hits@100: 0.16964285714285715\n",
      "\n",
      "Top469 hits: 224\n",
      "Hits@469: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_results(hits_at_k_role, hit_list_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_concept_assertions(test_data_concept_assertions,\n",
    "                               num_corrupt = int,\n",
    "                               entity_to_idx_vocab = dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "    \n",
    "    original_assertions = torch.tensor([sample[0] for sample, label in list(test_data_concept_assertions)]) # Gets rid of the individual\n",
    "\n",
    "    num_samples = len(original_assertions)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 2), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_assertions\n",
    "    corrupted_assertions[:, 1] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_role_assertions(test_data_role_assertions,\n",
    "                            num_corrupt = int,\n",
    "                            entity_to_idx_vocab=dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "\n",
    "    original_head_entities = torch.tensor([sample[0] for sample, label in list(test_data_role_assertions)])\n",
    "    original_roles = torch.tensor([sample[1] for sample, label in list(test_data_role_assertions)])\n",
    "    original_tail_entities = torch.tensor([sample[2] for sample, label in list(test_data_role_assertions)])\n",
    "\n",
    "    num_samples = len(original_head_entities)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 3), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_head_entities # The original head entities\n",
    "    corrupted_assertions[:, 1] = original_roles # The original roles\n",
    "    corrupted_assertions[:, 2] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
