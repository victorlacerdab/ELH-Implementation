{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import owlready2 as owl\n",
    "from owlready2 import *\n",
    "import types\n",
    "owlready2.reasoning.JAVA_MEMORY = 200000\n",
    "\n",
    "#import scipy\n",
    "#from scipy.spatial import ConvexHull\n",
    "#import cdd\n",
    "#from cdd import RepType, Matrix, Polyhedron\n",
    "#from fractions import Fraction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/goslimyeast.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/galennorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/gonorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Desktop/toyontology.owl'\n",
    "dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/family_ontology.owl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canmodelcreation import CanonicalModelElements, get_canonical_model_elements, CanonicalModel, create_canonical_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================================================================================\n",
      "All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.\n",
      "===========================================================================================================\n",
      "\n",
      "\n",
      "All restrictions have been preprocessed for the creation of the canonical model.\n",
      "============================================================================\n",
      "Starting to reason.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Running HermiT...\n",
      "    java -Xmx200000M -cp /opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit:/opt/homebrew/Caskroom/miniforge/base/envs/kgenv/lib/python3.11/site-packages/owlready2/hermit/HermiT.jar org.semanticweb.HermiT.cli.CommandLine -c -O -D -I file:////var/folders/wg/g5861gcs6k5d3rbq_rncztjw0000gn/T/tmp58ysyk0b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================================\n",
      "Done reasoning. Creating the canonical model.\n",
      "============================================================================\n",
      "\n",
      "Concluded creating canonical model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * HermiT took 1.040266990661621 seconds\n",
      "* Owlready * Reparenting entity.Q16019673: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q2397531: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q57209: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q829669: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q11090991: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q10323203: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2840038: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q60610: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q213716: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q312110: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q22876077: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2446902: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q1096611: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q15139243: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q706827: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q261809: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q140001: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1216049: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2559721: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q215903: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q5353322: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q138048: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1887943: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q266025: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q157812: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q580624: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2086454: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q391053: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1277671: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q1290518: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1770677: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q153586: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3044: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q511832: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q217098: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q437271: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q121973: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q234410: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q2724084: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q3637283: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q380380: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q7324278: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q10323222: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1138595: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q241179: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q350769: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q1241338: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q3638489: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q180920: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q4924916: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q312325: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q359215: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father}\n",
      "* Owlready * Reparenting entity.Q187550: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2131262: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q77067: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q5353300: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q241165: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q329708: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q185126: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q946829: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q819778: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q518569: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3345941: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q199647: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q295850: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q838180: {owl.Thing} => {Class.Child, Class.Spouse, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q314457: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q6827105: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q6116483: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q60657: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q706866: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q5730011: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q946939: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q5383161: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q292698: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q12971: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q1697035: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q269691: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2622086: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q10323773: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2399640: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q4722030: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1883786: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q242598: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q513091: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q131203: {owl.Thing} => {Class.Sibling, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q204077: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q2555139: {owl.Thing} => {Class.Child, Class.Spouse, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2658379: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1058383: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q465518: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q316859: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q5362448: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q5362322: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q3002934: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q547970: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1065802: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q314550: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q60796: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q178525: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q5353737: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q131122: {owl.Thing} => {Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q555362: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q302212: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1191183: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q70244: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q588360: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q693480: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q96836: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1183310: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q163043: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q12967: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q47595: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q313333: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q560939: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q578584: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q4356466: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q356145: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1063751: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1369215: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q466947: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q189527: {owl.Thing} => {Class.Spouse, Class.Father}\n",
      "* Owlready * Reparenting entity.Q367387: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q79999: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q468968: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q7184745: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q44530: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q320937: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q16859543: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q18754786: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q5048978: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q723791: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q102140: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q1121970: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q215988: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q235238: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3306659: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q170197: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q238609: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q16149641: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q712674: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q976797: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q130834: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q2453051: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q44785: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q506415: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q10323292: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q264368: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q213696: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q184216: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q93711: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q269928: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q2279884: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q609650: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q643456: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q170182: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q164504: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q975581: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2266324: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q7332058: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q138232: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q898947: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q4858486: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q12812195: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q4722123: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q66698: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q44319: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q711798: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q19975809: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q324085: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q571756: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q234249: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q1755340: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q11989934: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q470713: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q1934858: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q313015: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q527673: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q827676: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3035357: {owl.Thing} => {Class.Child, Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q1357271: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2467554: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q183226: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q165938: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q184437: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q947511: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3178707: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q645857: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q15829810: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q954274: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q18378893: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q10323244: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1365927: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q618316: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2159281: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q297194: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q4605238: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1091124: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q247408: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q6276792: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q129308: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q753294: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q706905: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1979505: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2026887: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q235418: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q16146696: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q942280: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q1699735: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q3011344: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q359596: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q434760: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q57531: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q57653: {owl.Thing} => {Class.Spouse, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q194012: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q57654: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q232155: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q10323269: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q977455: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q237706: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q350330: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q808503: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q463174: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q236981: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q557261: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q735448: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q57662: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3429250: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q698890: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q212890: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q232144: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q235637: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q44361: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q6347693: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q390099: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q154971: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q236970: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q2469933: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q610738: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q19282024: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q824532: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q299612: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q4763175: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q936486: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q4893550: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q112104: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q167173: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2574542: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q3144886: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q6531068: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q2375184: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q274428: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q9385079: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q157491: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q276742: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q204797: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q169123: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2084728: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q446544: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q14946987: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q261129: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q5623747: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q961790: {owl.Thing} => {Class.Spouse, Class.Father}\n",
      "* Owlready * Reparenting entity.Q2411371: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q152923: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q43974: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q163936: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2013986: {owl.Thing} => {Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q699893: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3726692: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1226617: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2640044: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q2411381: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q732899: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q15439085: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2630473: {owl.Thing} => {Class.Child, Class.Spouse, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q926544: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q28531: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q2059605: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2616482: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2411396: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q189175: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2015941: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2060421: {owl.Thing} => {Class.Child, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q808498: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q84276: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q285102: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q808499: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q4755558: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q349097: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q53438: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q229702: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q1464: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q229826: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2411320: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q180073: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q714803: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q983155: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q706068: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2006191: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q41610: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q1091520: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q183579: {owl.Thing} => {Class.Child, Class.Spouse, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q62036: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q7342246: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q170394: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q16186637: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q319638: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q364688: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1259232: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q14356: {owl.Thing} => {Class.Spouse, Class.Father}\n",
      "* Owlready * Reparenting entity.Q685306: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q719148: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q299645: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2411332: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q41849: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q192107: {owl.Thing} => {Class.Sibling, Class.Child, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q11091019: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q185747: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q559253: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q16851600: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1091530: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q64462: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1536527: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q240604: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q672470: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q16148349: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q603352: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q403964: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q11091100: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q349062: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q374220: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q373015: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q158248: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q66535: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q180287: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q237318: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q328005: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q233198: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q276304: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q57943: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q234047: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q4126528: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q51056: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1186147: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q1406351: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q130818: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1305838: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q707386: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1282676: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q161419: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q235484: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q287563: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1146603: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q3738408: {owl.Thing} => {Class.Child, Class.Father}\n",
      "* Owlready * Reparenting entity.Q78519: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q36234: {owl.Thing} => {Class.Spouse, Class.Father}\n",
      "* Owlready * Reparenting entity.Q233187: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q84192: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q326198: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q169512: {owl.Thing} => {Class.Child, Class.Sibling, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q4877223: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q1709891: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q447103: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q63605: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3624266: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q452932: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q242687: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q521487: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q61548: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q86846: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1283158: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q726279: {owl.Thing} => {Class.Child, Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q1236399: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2325634: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q2109015: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1380850: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q261769: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q7144855: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q356564: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q93390: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2253832: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q355112: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q58020: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q72795: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q920516: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1272142: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q189347: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2058801: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q262989: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1480: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q3294442: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q690327: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q156659: {owl.Thing} => {Class.Child, Class.Spouse, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q3601360: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q604419: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q222805: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q605744: {owl.Thing} => {Class.Child, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q231755: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1247148: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q617977: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q3904375: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q446240: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q37084: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q93377: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q2519772: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q921825: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1052301: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q252724: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q313641: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q691678: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q42305: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q761597: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q283107: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q4460880: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q1104636: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q5537157: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q452984: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q61221: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q180222: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q318091: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q380513: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q561182: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q565780: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q343101: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q478057: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q963719: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q441850: {owl.Thing} => {Class.Spouse, Class.Mother}\n",
      "* Owlready * Reparenting entity.Q269058: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q61234: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q659437: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q16200213: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q5389893: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q182637: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q215036: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q5899444: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q887170: {owl.Thing} => {Class.Child, Class.Father, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q6193704: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q355348: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q4726369: {owl.Thing} => {Class.Child, Class.Spouse, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q171977: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q157799: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q918990: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1445064: {owl.Thing} => {Class.Mother}\n",
      "* Owlready * Reparenting entity.Q61241: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q426677: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q11091072: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q154041: {owl.Thing} => {Class.Father}\n",
      "* Owlready * Reparenting entity.Q312641: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * Reparenting entity.Q93172: {owl.Thing} => {Class.Child, Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q84239: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q236293: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q6612: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q333319: {owl.Thing} => {Class.Child}\n",
      "* Owlready * Reparenting entity.Q101322: {owl.Thing} => {Class.Sibling}\n",
      "* Owlready * Reparenting entity.Q1891427: {owl.Thing} => {Class.Spouse}\n",
      "* Owlready * (NB: only changes on entities loaded in Python are shown, other changes are done but not listed)\n"
     ]
    }
   ],
   "source": [
    "# Instantiates the canonical model\n",
    "\n",
    "canmodel = create_canonical_model(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atenção: a função mu está com complexidade alta devido aos for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'canmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTBoxcreation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntityEmbedding, get_domain_embeddings, GeometricInterpretation, index_finder, get_faithful_concept_geometric_interps, get_faithful_role_geometric_interps\n",
      "File \u001b[0;32m~/Documents/VSCode/ELH-Implementation/TBoxcreation.py:38\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m canmodel\u001b[38;5;241m.\u001b[39mdomain\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m''' \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mClass for obtaining the positional \u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03membedding for each entity in the domain\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mpaper.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mEntityEmbedding\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Dictionaries for storing the indices of concept names and role names, entities pairs, respectively\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Keys are strings and values are integers\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconcept_names_idx_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_concept_names_idx_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrole_names_idx_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_role_names_idx_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/VSCode/ELH-Implementation/TBoxcreation.py:43\u001b[0m, in \u001b[0;36mEntityEmbedding\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEntityEmbedding\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Dictionaries for storing the indices of concept names and role names, entities pairs, respectively\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Keys are strings and values are integers\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     concept_names_idx_dict \u001b[38;5;241m=\u001b[39m get_concept_names_idx_dict(\u001b[43mcanmodel\u001b[49m)\n\u001b[1;32m     44\u001b[0m     role_names_idx_dict \u001b[38;5;241m=\u001b[39m get_role_names_idx_dict(canmodel)\n\u001b[1;32m     45\u001b[0m     entities_idx_dict \u001b[38;5;241m=\u001b[39m get_entities_idx_dict(canmodel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'canmodel' is not defined"
     ]
    }
   ],
   "source": [
    "from TBoxcreation import EntityEmbedding, get_domain_embeddings, GeometricInterpretation, index_finder, get_faithful_concept_geometric_interps, get_faithful_role_geometric_interps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There has to be a more efficient way of doing the creating of geometric interpretations for concepts and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default for the scale_factor is 1 to create binary vectors.\n",
    "\n",
    "def create_tbox_embeddings(canonical_model: CanonicalModel, scale_factor = 1):\n",
    "\n",
    "    domain = canonical_model.domain # Keys are strings and values are CanonicalModelElements type objects\n",
    "    concept_names_interps = canonical_model.concept_canonical_interpretation # Keys are strings and values are lists of strings.\n",
    "    role_names_interps = canonical_model.role_canonical_interpretation # Keys are strings and values are lists of tuples. Tuples are of form ('C', 'D'), with C and D strings.\n",
    "\n",
    "    EMB_DIM = len(concept_names_interps) + len(role_names_interps) * len(domain)\n",
    "    SCALE_FACTOR = scale_factor\n",
    "\n",
    "    print('================EMBEDDING DIMENSION================')\n",
    "    print(f'Concept Name dimensions: {len(concept_names_interps)}')\n",
    "    print(f'The number of role names is: {len(role_names_interps)}')\n",
    "    print(f'The size of the domain is: {len(domain)}')\n",
    "    print(f'Role names dimensions: {len(role_names_interps) * len(domain)}')\n",
    "    print('===================================================')\n",
    "    print('')\n",
    "    print(f'Final embedding dimension: {EMB_DIM}')\n",
    "    print(f'The final dimension for role regions is: {EMB_DIM * 2}')\n",
    "\n",
    "    domain_embeddings_list = get_domain_embeddings(EMB_DIM, SCALE_FACTOR)\n",
    "    \n",
    "    concept_names_ordering = EntityEmbedding.concept_names_idx_dict\n",
    "    role_names_ordering = EntityEmbedding.role_names_idx_dict\n",
    "    entities_ordering = EntityEmbedding.entities_idx_dict\n",
    "    \n",
    "    print('')\n",
    "    print('===============FINISHED EMBEDDINGS===============')\n",
    "    print(f'There are {len(domain_embeddings_list)} vector embeddings.')\n",
    "    print('')\n",
    "\n",
    "    index_finder_dict = index_finder(EMB_DIM, concept_names_ordering, role_names_ordering, entities_ordering)\n",
    "\n",
    "    faithful_concept_geometric_interps = get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('============FINISHED INTERPS CONCEPT=============')\n",
    "    print(f'There are {len(faithful_concept_geometric_interps)} regions for concept names.')\n",
    "    print('')\n",
    "\n",
    "    faithful_role_geometric_interps = get_faithful_role_geometric_interps(role_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('=============FINISHED INTERPS ROLES==============')\n",
    "    print(f'There are {len(faithful_role_geometric_interps)} regions for role names.')\n",
    "    print('')\n",
    "\n",
    "    return domain_embeddings_list, faithful_concept_geometric_interps, faithful_role_geometric_interps, index_finder_dict, int(EMB_DIM), int(scale_factor) # Returns the faithful geometric interpretations for concepts and roles as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_embeddings, concept_geointerps, role_geointerps, idx_finder_dict, EMB_DIM, SCALE_FACTOR = create_tbox_embeddings(canmodel, SCALE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restriction_vertices(restriction_concept, concept_geointerps_dict, role_geointerps_dict, index_finder_dict, CanonicalModel: CanonicalModel, EntityEmbedding: EntityEmbedding):\n",
    "    \n",
    "    concept_name = restriction_concept.value\n",
    "    role_name = restriction_concept.property\n",
    "\n",
    "    concept_name_str = concept_name.name\n",
    "    role_name_str = role_name.name\n",
    "\n",
    "    vertices = []\n",
    "\n",
    "    for element in list(CanonicalModel.role_canonical_interpretation.keys()):\n",
    "        if role_name_str in element:\n",
    "            role_interpretation_set = CanonicalModel.role_canonical_interpretation[role_name_str]\n",
    "\n",
    "            for pair in role_interpretation_set:\n",
    "                element_1 = pair[0]\n",
    "                element_2 = pair[1]\n",
    "\n",
    "                if element_2 in CanonicalModel.concept_canonical_interpretation[concept_name_str]:\n",
    "                    vertices.append(EntityEmbedding.entity_entityvector_dict[element_1])\n",
    "    \n",
    "    return np.array(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_vertices(restriction_concept, concept_geointerps_dict, role_geointerps_dict, index_finder_dict, CanonicalModel: CanonicalModel, EntityEmbedding: EntityEmbedding):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the pre-split dataset containing facts from the ontology.\n",
    "Distinguishes between concept assertions and role assertions.\n",
    "\n",
    "\n",
    "    Args: ontology_dir (str): the directory from the ontology\n",
    "          concept_geointerps_dict (dict): the geometrical interpretations for concepts generated by create_tbox_embeddings()\n",
    "          role_geointerps_dict (dict): the geometrical interpretations for roles generated by create_tbox_embeddings()\n",
    "\n",
    "    Returns:\n",
    "          X_concepts (np.array): A dataframe with columns 'Concept', 'Entity', 'y_true' (equivalent to concept.centroid())\n",
    "          X_roles (np.array): A dataframe with columns 'SubjectEntity', 'Role', 'ObjectEntity', 'y_true' (equivalent to role.centroid())\n",
    "          y_concepts (np.array):\n",
    "          y_roles (np.array):\n",
    "          vocabulary_dict (dict): A vocabulary with key (int): value (str) for entities in the domain.\n",
    "'''\n",
    "\n",
    "def get_abox_dataset(ontology_dir: str,\n",
    "                     concept_geointerps_dict: dict, role_geointerps_dict: dict,\n",
    "                     concept_to_idx: dict, role_to_idx: dict,\n",
    "                     index_finder_dict: dict, emb_dim = int,\n",
    "                     CanonicalModel = CanonicalModel, EntityEmbedding=EntityEmbedding):\n",
    "    \n",
    "    ontology = get_ontology(ontology_dir)\n",
    "    ontology = ontology.load()\n",
    "    \n",
    "    X_concepts = []\n",
    "    X_roles = []\n",
    "    y_concepts = []\n",
    "    y_roles = []\n",
    "\n",
    "    entities = [entity.name for entity in list(ontology.individuals())]\n",
    "    \n",
    "    concept_to_idx_vocab = concept_to_idx\n",
    "    idx_to_concept_vocab = {value: key for key, value in concept_to_idx_vocab.items()}\n",
    "\n",
    "    role_to_idx_vocab = role_to_idx\n",
    "    idx_to_role_vocab = {value: key for key, value in role_to_idx_vocab.items()}\n",
    "    \n",
    "    entity_to_idx_vocab = {value: index for index, value in enumerate(entities)}\n",
    "    idx_to_entity_vocab = {value: key for key, value in entity_to_idx_vocab.items()}\n",
    "\n",
    "    for individual in list(ontology.individuals()):\n",
    "\n",
    "        all_facts = individual.INDIRECT_is_a\n",
    "        #all_facts = individual.is_a\n",
    "\n",
    "        for concept in all_facts:\n",
    "            # Handles concepts of the type A\n",
    "            if type(concept) == ThingClass:\n",
    "                concept = concept_geointerps_dict[concept.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "                \n",
    "            # Handles concepts of the type A \\and B\n",
    "            elif type(concept) == And:\n",
    "                concept1 = concept_geointerps_dict[concept.Classes[0]]\n",
    "                concept2 = concept_geointerps_dict[concept.Classes[1]]\n",
    "                intersection_name = 'And_' + ''.join(sorted(concept1.name + concept2.name))\n",
    "\n",
    "                if concept_to_idx_vocab.get(intersection_name) is not None:\n",
    "                    fact = np.array([concept_to_idx_vocab[intersection_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "                else:\n",
    "                    concept_to_idx_vocab[intersection_name] = len(concept_to_idx_vocab)\n",
    "                    idx_to_concept_vocab[len(concept_to_idx_vocab)] = intersection_name\n",
    "                    fact = np.array([concept_to_idx_vocab[intersection_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "            \n",
    "            # Handles concepts of the type \\exists r.B\n",
    "            elif type(concept) == Restriction:\n",
    "                concept_name = concept.value\n",
    "                role_name = concept.property\n",
    "                restriction_name = 'exists_' + role_name.name + '.' + concept_name.name\n",
    "\n",
    "                if concept_to_idx_vocab.get(restriction_name) is not None:\n",
    "                \n",
    "                    fact = np.array([concept_to_idx_vocab[restriction_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array(GeometricInterpretation.concept_geointerps_dict[restriction_name].centroid)\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "                else:\n",
    "                    concept_to_idx_vocab[restriction_name] = len(concept_to_idx_vocab)\n",
    "                    idx_to_concept_vocab[len(concept_to_idx_vocab)-1] = restriction_name\n",
    "                    restriction_concept = GeometricInterpretation(restriction_name, EMB_DIM) # Initializes a Geometric Interpretation type object\n",
    "                    restriction_concept.vertices = get_restriction_vertices(concept, concept_geointerps_dict,\n",
    "                                                                            role_geointerps_dict, index_finder_dict, CanonicalModel, EntityEmbedding)\n",
    "                    \n",
    "                    GeometricInterpretation.concept_geointerps_dict[restriction_name] = restriction_concept\n",
    "                    restriction_concept.centroid = restriction_concept.get_centroid_naive()\n",
    "                    fact = np.array([concept_to_idx_vocab[restriction_concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = restriction_concept.centroid\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "        relevant_roles = individual.get_properties()\n",
    "        individual_name = individual.name\n",
    "\n",
    "        for role in relevant_roles:\n",
    "            role_geo = role_geointerps_dict[role.name]\n",
    "            subject_list = role[individual] # This syntax is from the owlready2 library\n",
    "            for subject in subject_list:\n",
    "                fact = np.array([entity_to_idx_vocab[individual.name], role_to_idx_vocab[role.name], entity_to_idx_vocab[subject.name]])\n",
    "                X_roles.append(fact)\n",
    "                y_label = y_roles.append(np.array(role_geo.centroid))\n",
    "\n",
    "    return np.array(X_concepts), np.array(X_roles), np.array(y_concepts), np.array(y_roles), entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concepts, X_roles, y_concepts, y_roles, entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab = get_abox_dataset(dir,\n",
    "                                                                                                                                                                                        GeometricInterpretation.concept_geointerps_dict,\n",
    "                                                                                                                                                                                        GeometricInterpretation.role_geointerps_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.concept_names_idx_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.role_names_idx_dict,\n",
    "                                                                                                                                                                                        idx_finder_dict,\n",
    "                                                                                                                                                                                        EMB_DIM,\n",
    "                                                                                                                                                                                        CanonicalModel, EntityEmbedding\n",
    "                                                                                                                                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = torch.tensor(data, dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].long(), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE_PROPORTION = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "ConceptDataset = OntologyDataset(X_concepts, y_concepts)\n",
    "\n",
    "dataset_size = len(ConceptDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainConceptDataset, testConceptDataset = torch.utils.data.random_split(ConceptDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "RoleDataset = OntologyDataset(X_roles, y_roles)\n",
    "\n",
    "dataset_size = len(RoleDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainRoleDataset, testRoleDataset = torch.utils.data.random_split(RoleDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_ConceptDataLoader = DataLoader(trainConceptDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_ConceptDataLoader = DataLoader(testConceptDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "\n",
    "train_RoleDataLoader = DataLoader(trainRoleDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_RoleDataLoader = DataLoader(testRoleDataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithEL(nn.Module):\n",
    "    def __init__(self, emb_dim, phi, individual_vocabulary, concept_vocabulary, role_vocabulary, init_individual_param_to_centroid = False, init_concept_param_to_centroid = False, init_role_param_to_centroid = False):\n",
    "        super(FaithEL, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.phi = phi\n",
    "        \n",
    "        self.individual_embedding_dict = nn.Embedding(len(individual_vocabulary),\n",
    "                                                      emb_dim\n",
    "                                                      )\n",
    "        \n",
    "        if init_individual_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for individual, individual_idx in individual_vocabulary.items():\n",
    "                    value = SCALE_FACTOR/2\n",
    "                    individual_centroid = torch.full((emb_dim,), value)\n",
    "                    self.individual_embedding_dict.weight[individual_idx] = individual_centroid\n",
    "        \n",
    "        self.concept_embedding_dict = nn.Embedding(len(concept_vocabulary),\n",
    "                                                   emb_dim\n",
    "                                                   )\n",
    "        \n",
    "        # Initializes the moving parameter for concepts at the concept's respective centroid\n",
    "        if init_concept_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for concept_name, concept_idx in concept_vocabulary.items():\n",
    "                    self.concept_embedding_dict.weight[concept_idx] = torch.tensor(GeometricInterpretation.concept_geointerps_dict[concept_name].centroid)\n",
    "\n",
    "        self.role_embedding_dict = nn.Embedding(len(role_vocabulary),\n",
    "                                                emb_dim * 2\n",
    "                                                )\n",
    "        \n",
    "        # Initializes the moving parameter for roles at the role's respective centroid\n",
    "        if init_role_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for role_name, role_idx in role_vocabulary.items():\n",
    "                    self.role_embedding_dict.weight[role_idx] = torch.tensor(GeometricInterpretation.role_geointerps_dict[role_name].centroid)\n",
    "        \n",
    "    \n",
    "    def forward(self, data):\n",
    "    \n",
    "        # Concept assertions are of the form ['Concept', 'Entity']\n",
    "        # Role assertions are of the form ['SubjectEntity', 'Role', 'ObjectEntity']\n",
    "        \n",
    "        subj_entity_idx = 1 if len(data[0]) == 2 else 0 # Checks whether the model has received a C assert or R assert\n",
    "\n",
    "        if subj_entity_idx == 1:\n",
    "            concept_idx = 0\n",
    "\n",
    "            subj_entity = data[:, subj_entity_idx]\n",
    "            concept = data[:, concept_idx]\n",
    "\n",
    "            c_assertion_out1 = self.concept_embedding_dict(concept) # Outputs the moving parameter for the concept\n",
    "            c_assertion_out2 = self.individual_embedding_dict(subj_entity) # Outputs the embedding for the individual\n",
    "            \n",
    "            out1 = c_assertion_out1\n",
    "            out2 = c_assertion_out2\n",
    "            \n",
    "            return out1, out2\n",
    "\n",
    "        elif subj_entity_idx == 0:\n",
    "            role_idx = 1\n",
    "            obj_entity_idx = 2\n",
    "        \n",
    "            subject_entity = self.individual_embedding_dict(data[:, subj_entity_idx])\n",
    "            object_entity = self.individual_embedding_dict(data[:, obj_entity_idx])\n",
    "            role = self.role_embedding_dict(data[:, role_idx])\n",
    "\n",
    "            r_assertion_out1 = role # Role parameter embedding\n",
    "            r_assertion_out2 = torch.cat((subject_entity, object_entity), 1) # Concatenation of subject and object\n",
    "\n",
    "            out1 = r_assertion_out1\n",
    "            out2 = r_assertion_out2\n",
    "            \n",
    "            return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_dists(model, dictionary_concept_to_idx, role = False):\n",
    "\n",
    "    dist_dict = {}\n",
    "\n",
    "    for predicate, idx in dictionary_concept_to_idx.items():\n",
    "        with torch.no_grad():\n",
    "            if role == False:\n",
    "                dist = torch.dist(torch.tensor(GeometricInterpretation.concept_geointerps_dict[predicate].centroid), model.concept_embedding_dict.weight[idx])\n",
    "                dist_dict[predicate] = dist\n",
    "            else:\n",
    "                dist = torch.dist(torch.tensor(GeometricInterpretation.role_geointerps_dict[predicate].centroid), model.concept_embedding_dict.weight[idx])\n",
    "                dist_dict[predicate] = dist\n",
    "\n",
    "    return dist_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, concept_dataloader, role_dataloader, loss_fn, optimizer, alternate_training = False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "    \n",
    "    if alternate_training == False:\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "            loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2)  + model.phi * loss_fn(outputs1, labels) # Phi describe\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "            loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, concept_dataloader, role_dataloader, loss_fn, alternate_training = False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    if alternate_training == False:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(concept_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs)\n",
    "                loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            for i, data in enumerate(role_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs)\n",
    "                loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, data in enumerate(concept_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "                loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2)  + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "                loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_concept_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    relevant_concept_idx = []\n",
    "\n",
    "    # Gathers only concepts appearing in the test set (it is not guaranteed that if a concept appears in the dataset, then it appears here)\n",
    "\n",
    "    for assertion in test_concept_assertions:\n",
    "        inputs, _ = assertion\n",
    "        if inputs[0] not in relevant_concept_idx:\n",
    "            relevant_concept_idx.append(inputs[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for concept_idx in relevant_concept_idx:\n",
    "            assertion_scores = []\n",
    "\n",
    "            for _, entity_idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([concept_idx, entity_idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample) # out1 = Concept parameter, out2 = Individual parameter\n",
    "\n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                else:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2) + torch.dist(outputs2, torch.tensor(GeometricInterpretation.concept_geointerps_dict[idx_to_concept_vocab[int(concept_idx)]].centroid))\n",
    "\n",
    "                assertion_scores.append((torch.tensor([concept_idx, entity_idx]), assertion_score.item()))\n",
    "            \n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "            hit_k_values = []\n",
    "\n",
    "            true_samples = [inputs for inputs, _ in test_concept_assertions if inputs[0] == concept_idx] # This is problematic when dealing with big datasets\n",
    "\n",
    "            for k in k_list:\n",
    "                hit_k = any(torch.equal(scored_sample[0], true_sample) for true_sample in true_samples for scored_sample in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "            \n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            top1 += int(hit_k_values[0])\n",
    "            top3 += int(hit_k_values[1])\n",
    "            top10 += int(hit_k_values[2])\n",
    "            top100 += int(hit_k_values[3])\n",
    "            top_all += int(hit_k_values[4])\n",
    "\n",
    "    hits_at_k = [round(sum(hit_values) / len(hit_values), 3) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    # return hits_at_k, [top1, top3, top10, top100, top_all]\n",
    "    return hits_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_role_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "    relevant_assertions = []\n",
    "\n",
    "    # Convert PyTorch dataset to a numpy array for vectorization\n",
    "    assertions_array = [assertion[0].numpy() for assertion in test_role_assertions]\n",
    "    assertions_array = np.stack(assertions_array)\n",
    "\n",
    "    ''' \n",
    "    The array below is used to disregard duplicate queries.\n",
    "    For ex., if we have two assertions r(a,b) and r(a,c), the function\n",
    "    will treat r(a, ?) as a query with b and c as positive answers. It\n",
    "    will then disregard any other.\n",
    "    '''\n",
    "\n",
    "    filter_array = np.ones((assertions_array.shape), dtype=int)\n",
    "    filter_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for assertion_idx, assertion in enumerate(assertions_array):\n",
    "\n",
    "            filter_counter = assertion_idx\n",
    "\n",
    "            if np.all(filter_array[filter_counter] == 1):\n",
    "\n",
    "                head_entity_idx = assertion[0]\n",
    "                role_entity_idx = assertion[1]\n",
    "                filter_arr = (assertions_array[:, 0] == head_entity_idx) & (assertions_array[:, 1] == role_entity_idx)\n",
    "                relevant_assertions_idcs = np.where(filter_arr)[0]\n",
    "                relevant_assertions = torch.tensor(np.array([assertions_array[idx] for idx in relevant_assertions_idcs]))\n",
    "                filter_array[relevant_assertions_idcs] = 0\n",
    "\n",
    "                assertion_scores = []\n",
    "\n",
    "                for _, tail_entity_idx in entity_to_idx_vocab.items():\n",
    "                    eval_sample = torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]).unsqueeze(0)\n",
    "                    outputs1, outputs2 = model(eval_sample)\n",
    "                    if centroid_score == False:\n",
    "                        assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                    else:\n",
    "                        assertion_score = torch.dist(outputs1, outputs2, p=2) + torch.dist(outputs2, torch.tensor(GeometricInterpretation.role_geointerps_dict[idx_to_role_vocab[role_entity_idx]].centroid))\n",
    "\n",
    "                    assertion_scores.append((torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]), assertion_score.item()))\n",
    "\n",
    "                sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "                k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "                hit_k_values = []\n",
    "\n",
    "                for k in k_list:\n",
    "                    hit_k = any(torch.equal(scored_sample[0], assertion) for assertion in relevant_assertions for scored_sample in sorted_scores[:k])\n",
    "                    hit_k_values.append(hit_k)\n",
    "            \n",
    "                hits.append(hit_k_values)\n",
    "\n",
    "                top1 += int(hit_k_values[0])\n",
    "                top3 += int(hit_k_values[1])\n",
    "                top10 += int(hit_k_values[2])\n",
    "                top100 += int(hit_k_values[3])\n",
    "                top_all += int(hit_k_values[4])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "    hits_at_k = [round(sum(hit_values) / len(hit_values), 3) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "    # print(f'Hits at 1, 3, 10, 100 and all: {hits_at_k}')\n",
    "\n",
    "    # return hits_at_k, [top1, top3, top10, top100, top_all]\n",
    "    return hits_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss, test_loss, num_epoch):\n",
    "    \n",
    "    plt.plot(range(1, num_epoch+1), train_loss, 'b-', label='Train Loss')\n",
    "    plt.plot(range(1, num_epoch+1), test_loss, 'r-', label='Test Loss')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Test Loss per Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_hak(hits_at_k_concept, hits_at_k_roles, topk, num_epoch, eval_freq):\n",
    "\n",
    "    concept_hits_at_topk = [score_list[topk] for score_list in hits_at_k_concept]\n",
    "    roles_hits_at_topk = [scores[topk] for scores in hits_at_k_roles]\n",
    "\n",
    "    hak_dict = {0: 1,\n",
    "                1: 3,\n",
    "                2: 10,\n",
    "                3: 100,\n",
    "                4: 'all'}\n",
    "    \n",
    "    plt.plot(range(1, num_epoch+1, eval_freq), concept_hits_at_topk, 'b-', label=f'H@{hak_dict[topk]} concepts')\n",
    "\n",
    "    plt.plot(range(1, num_epoch+1, eval_freq), roles_hits_at_topk, 'r-', label=f'H@{hak_dict[topk]} roles')\n",
    "\n",
    "    plt.ylim(0, 1.02)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f'hits@{hak_dict[topk]}')\n",
    "    plt.title(f'Hits@{hak_dict[topk]} every {eval_freq} epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(centroid_score, lr,\n",
    "               phi, emb_dim, epochs,\n",
    "               log_epoch, eval_freq,\n",
    "               eval_test, alt_train,\n",
    "               entity_centroid_init, concept_centroid_init,\n",
    "               role_centroid_init, loss_fn, model, optimizer,\n",
    "               train_loss_list, test_loss_list,\n",
    "               train_hits_at_k_concept, test_hits_at_k_concept,\n",
    "               train_hits_at_k_role, test_hits_at_k_role):\n",
    "    \n",
    "    model_hparams = {'centroid_score': centroid_score,\n",
    "                     'lr': lr,\n",
    "                     'phi': phi,\n",
    "                     'emb_dim': emb_dim,\n",
    "                     'epochs': epochs,\n",
    "                     'log_epoch': log_epoch,\n",
    "                     'eval_freq': eval_freq,\n",
    "                     'eval_test': eval_test,\n",
    "                     'alt_train': alt_train,\n",
    "                     'entity_centroid_init': entity_centroid_init,\n",
    "                     'concept_centroid_init': concept_centroid_init,\n",
    "                     'role_centroid_init': role_centroid_init,\n",
    "                     'loss_fn': loss_fn,\n",
    "                     'model': model,\n",
    "                     'optimizer': optimizer,\n",
    "                     'train_loss_list': train_loss_list,\n",
    "                     'test_loss_list': test_loss_list,\n",
    "                     'train_hits_at_k_concept': train_hits_at_k_concept,\n",
    "                     'test_hits_at_k_concept': test_hits_at_k_concept,\n",
    "                     'train_hits_at_k_role': train_hits_at_k_role,\n",
    "                     'test_hits_at_k_role': test_hits_at_k_role,\n",
    "                     'misc notes': []\n",
    "                     }\n",
    "    \n",
    "    return model_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, individual_vocab_idcs, concept_vocab_idcs, scaling_factor):\n",
    "\n",
    "    individual_embeddings = model.individual_embedding_dict.weight\n",
    "    concept_parameter_embeddings = model.concept_embedding_dict.weight\n",
    "\n",
    "    individuals_for_plotting = []\n",
    "    concept_parameters_for_plotting = []\n",
    "    centroid_for_plotting = []\n",
    "\n",
    "    for idx, individual in enumerate(individual_embeddings[:]):\n",
    "        individual = individual[1:3].detach().numpy()\n",
    "        individual_label = individual_vocab_idcs[idx]\n",
    "        final_representation = (individual, individual_label)\n",
    "        individuals_for_plotting.append(final_representation)\n",
    "\n",
    "    for idx, concept in enumerate(concept_parameter_embeddings):\n",
    "        concept_param = concept[1:3].detach().numpy()\n",
    "        concept_label = concept_vocab_idcs[idx]\n",
    "        final_representation = (concept_param, concept_label)\n",
    "        concept_parameters_for_plotting.append(final_representation)\n",
    "\n",
    "    for idx, key in enumerate(GeometricInterpretation.concept_geointerps_dict.keys()):\n",
    "        concept_centroid = GeometricInterpretation.concept_geointerps_dict[key].centroid[1:3]\n",
    "        concept_label = key + '_centroid'\n",
    "        final_representation = (concept_centroid, concept_label)\n",
    "        centroid_for_plotting.append(final_representation)\n",
    "\n",
    "    # Create a figure and axis object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlim(-1, scaling_factor + scaling_factor/10)\n",
    "    ax.set_ylim(-1, scaling_factor + scaling_factor/10)\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.plot(0, 0, 'yo')\n",
    "\n",
    "    # Plot individual points in blue\n",
    "    for individual, label in individuals_for_plotting:\n",
    "        ax.plot(individual[0], individual[1], 'bo', label=label)\n",
    "        ax.annotate(label, xy=(individual[0], individual[1]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    # Plot concept points in red\n",
    "    for concept_param, label in concept_parameters_for_plotting:\n",
    "        ax.plot(concept_param[0], concept_param[1], 'r+', label=label)\n",
    "        ax.annotate(label, xy=(concept_param[0], concept_param[1]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    for concept_centroid, label in centroid_for_plotting:\n",
    "        ax.plot(concept_centroid[0], concept_centroid[1], 'go', label=label)\n",
    "        ax.annotate(label, xy=(concept_centroid[0], concept_centroid[1]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_concept_loader, train_role_loader,\n",
    "                test_concept_loader, test_role_loader,\n",
    "                train_concept_dset, test_concept_dset,\n",
    "                train_role_dset, test_role_dset,\n",
    "                num_epochs, loss_log_freq,\n",
    "                eval_freq, eval_train,\n",
    "                loss_function, optimizer,\n",
    "                idx_to_entity: dict, entity_to_idx: dict,\n",
    "                idx_to_concept: dict, concept_to_idx: dict,\n",
    "                idx_to_role: dict, role_to_idx: dict,\n",
    "                centroid_score = False, alt_training = True,\n",
    "                ):\n",
    "\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    train_hits_at_k_concept = []\n",
    "    test_hits_at_k_concept = []\n",
    "\n",
    "    train_hits_at_k_role = []\n",
    "    test_hits_at_k_role = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train(model, train_concept_loader, train_role_loader, loss_function, optimizer, alt_training)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss = test(model, test_concept_loader, test_role_loader, loss_function, alt_training)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if epoch % loss_log_freq == 0:\n",
    "            print(f'Epoch {epoch}/{num_epochs} -> Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\\n')\n",
    "\n",
    "        if epoch % eval_freq == 0:\n",
    "            print(f'Epoch {epoch}: Initiating evaluation. \\n')\n",
    "            \n",
    "            test_hak_concept = get_hits_at_k_concept_assertions(model, test_concept_dset, test_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "            test_hits_at_k_concept.append(test_hak_concept)\n",
    "\n",
    "            if eval_train == True:\n",
    "                train_hak_concept = get_hits_at_k_concept_assertions(model, train_concept_dset, train_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "                train_hits_at_k_concept.append(train_hak_concept)\n",
    "\n",
    "            test_hak_role = get_hits_at_k_role_assertions(model, test_concept_dset, test_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "            test_hits_at_k_role.append(test_hak_role)\n",
    "            \n",
    "            if eval_train == True:\n",
    "                train_hak_role = get_hits_at_k_role_assertions(model, train_concept_dset, train_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "                train_hits_at_k_role.append(train_hak_role)\n",
    "\n",
    "    plot_loss(train_loss_list, test_loss_list, num_epochs)\n",
    "\n",
    "    return train_loss_list, test_loss_list, train_hits_at_k_concept, test_hits_at_k_concept, train_hits_at_k_role, test_hits_at_k_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "\n",
    "CENTROID_SCORE = True # When set to True, model scores assertion w.r.t distance to the centroid instead of to the moving parameter for concepts/roles\n",
    "LR = 0.001\n",
    "PHI = 0.0\n",
    "EMB_DIM = 465\n",
    "EPOCHS = 200\n",
    "LOG_EPOCH = 25\n",
    "EVAL_FREQ = 10\n",
    "EVAL_TEST = True\n",
    "ALT_TRAIN = False\n",
    "ENTITY_CENTROID_INIT = False\n",
    "CONCEPT_CENTROID_INIT = False\n",
    "ROLE_CENTROID_INIT = False\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model = FaithEL(EMB_DIM, PHI,\n",
    "                entity_to_idx_vocab, concept_to_idx_vocab, role_to_idx_vocab,\n",
    "                init_individual_param_to_centroid = ENTITY_CENTROID_INIT,\n",
    "                init_concept_param_to_centroid = CONCEPT_CENTROID_INIT,\n",
    "                init_role_param_to_centroid = ROLE_CENTROID_INIT)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, test_loss_list, train_hits_at_k_concept, test_hits_at_k_concept, train_hits_at_k_role, test_hits_at_k_role, = train_model(model,\n",
    "                                                                                                                                           train_ConceptDataLoader, train_RoleDataLoader, test_ConceptDataLoader, test_RoleDataLoader,\n",
    "                                                                                                                                           trainConceptDataset, testConceptDataset, trainRoleDataset, testRoleDataset,\n",
    "                                                                                                                                           EPOCHS, LOG_EPOCH, EVAL_FREQ, EVAL_TEST, loss_fn, optimizer,\n",
    "                                                                                                                                           idx_to_entity_vocab, entity_to_idx_vocab,\n",
    "                                                                                                                                           idx_to_concept_vocab, concept_to_idx_vocab,\n",
    "                                                                                                                                           idx_to_role_vocab, role_to_idx_vocab,\n",
    "                                                                                                                                           CENTROID_SCORE, ALT_TRAIN \n",
    "                                                                                                                                           )\n",
    "\n",
    "model_list.append(save_model(CENTROID_SCORE, LR, PHI, EMB_DIM, EPOCHS, LOG_EPOCH, EVAL_FREQ, EVAL_TEST, ALT_TRAIN, ENTITY_CENTROID_INIT, CONCEPT_CENTROID_INIT, ROLE_CENTROID_INIT,\n",
    "                             loss_fn, model, optimizer, train_loss_list, test_loss_list, train_hits_at_k_concept, test_hits_at_k_concept, train_hits_at_k_role, test_hits_at_k_role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without centroid score\n",
    "pos_list = 0\n",
    "TOPK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_hak(model_list[pos_list]['test_hits_at_k_concept'], model_list[pos_list]['test_hits_at_k_role'], TOPK, model_list[pos_list]['epochs'], model_list[pos_list]['eval_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With centroid score\n",
    "pos_list = 1\n",
    "TOPK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_hak(model_list[pos_list]['train_hits_at_k_concept'], model_list[pos_list]['train_hits_at_k_role'], TOPK, model_list[pos_list]['epochs'], model_list[pos_list]['eval_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corruption for training with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_concept_assertions(test_data_concept_assertions,\n",
    "                               num_corrupt = int,\n",
    "                               entity_to_idx_vocab = dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "    \n",
    "    original_assertions = torch.tensor([sample[0] for sample, label in list(test_data_concept_assertions)]) # Gets rid of the individual\n",
    "\n",
    "    num_samples = len(original_assertions)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 2), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_assertions\n",
    "    corrupted_assertions[:, 1] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_role_assertions(test_data_role_assertions,\n",
    "                            num_corrupt = int,\n",
    "                            entity_to_idx_vocab=dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "\n",
    "    original_head_entities = torch.tensor([sample[0] for sample, label in list(test_data_role_assertions)])\n",
    "    original_roles = torch.tensor([sample[1] for sample, label in list(test_data_role_assertions)])\n",
    "    original_tail_entities = torch.tensor([sample[2] for sample, label in list(test_data_role_assertions)])\n",
    "\n",
    "    num_samples = len(original_head_entities)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 3), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_head_entities # The original head entities\n",
    "    corrupted_assertions[:, 1] = original_roles # The original roles\n",
    "    corrupted_assertions[:, 2] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
