{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "\n",
      "Warning: SQLite3 version 3.40.0 and 3.41.2 have huge performance regressions; please install version 3.41.1 or 3.42!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import owlready2 as owl\n",
    "from owlready2 import *\n",
    "owlready2.reasoning.JAVA_MEMORY = 200000\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/goslimyeast.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/galennorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/gonorm.xml.owl'\n",
    "# dir = '/Users/victorlacerda/Desktop/toyontology.owl'\n",
    "# dir = '/Users/victorlacerda/Desktop/roledebug.owl'\n",
    "dir = '/Users/victorlacerda/Documents/VSCode/ELHFaithfulness/NormalizedOntologies/family_ontology.owl'\n",
    "# dir = '/Users/victorlacerda/Desktop/familiaontologia.owl'\n",
    "# dir = '/Users/victorlacerda/Desktop/dbpedia15k.owl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class for creating entities to\n",
    "populate the creation of the\n",
    "canonical models.\n",
    "\n",
    "The .name attribute is used to\n",
    "create a single representation\n",
    "for concepts like A and B / \n",
    "B and A, as they are the same.\n",
    "'''\n",
    "\n",
    "class CanonicalModelElements:\n",
    "\n",
    "    concept_names = {}\n",
    "    concept_intersections = {}\n",
    "    concept_restrictions = {}\n",
    "    all_concepts = {}\n",
    "\n",
    "    def __init__(self, concept):\n",
    "        self.concept = concept\n",
    "        self.name = self.get_name()\n",
    "        self.get_element_dict()\n",
    "\n",
    "    def get_name(self):\n",
    "        \n",
    "        if type(self.concept) == ThingClass:\n",
    "            return self.concept.name\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            return 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "        \n",
    "        else:\n",
    "            return 'And_' + ''.join(sorted(self.concept.Classes[0].name + self.concept.Classes[1].name)) # The name is sorted to avoid that (e.g) (A \\and B) and (B \\and A) are treated as different concepts\n",
    "        \n",
    "    def get_element_dict(self):\n",
    "\n",
    "        if type(self.concept) == ThingClass:\n",
    "            CanonicalModelElements.concept_names[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == Restriction:\n",
    "            CanonicalModelElements.concept_restrictions[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self\n",
    "\n",
    "        elif type(self.concept) == And:\n",
    "            CanonicalModelElements.concept_intersections[self.name] = self\n",
    "            CanonicalModelElements.all_concepts[self.name] = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_canonical_model_elements(concept_names_iter, role_names_iter, ontology, restrict_language = False):\n",
    "    \n",
    "    onto = ontology\n",
    "    top = owl.Thing\n",
    "    #bottom = owl.Nothing\n",
    "\n",
    "    CanonicalModelElements(top)\n",
    "    #CanonicalModelElements(bottom)\n",
    "\n",
    "    for concept_name in concept_names_iter:\n",
    "        \n",
    "        CanonicalModelElements(concept_name)\n",
    "\n",
    "        if restrict_language == False:\n",
    "\n",
    "            for concept_name2 in concept_names_iter:\n",
    "        \n",
    "                with onto:\n",
    "                    gca = GeneralClassAxiom(concept_name & concept_name2)\n",
    "                    gca.is_a.append(concept_name & concept_name2)\n",
    "            \n",
    "                CanonicalModelElements(gca.left_side)\n",
    "\n",
    "    print('')\n",
    "    print('===========================================================================================================')\n",
    "    print('All Concept Names and Concept Intersections have been preprocessed for the creation of the canonical model.')\n",
    "    print('===========================================================================================================')\n",
    "\n",
    "    concept_names_iter.append(top)\n",
    "    #concept_names_iter.append(bottom)\n",
    "\n",
    "    if restrict_language == False:\n",
    "\n",
    "        for role_name in role_names_iter:\n",
    "            for concept_name in concept_names_iter:\n",
    "                with onto:\n",
    "                    gca = GeneralClassAxiom(role_name.some(concept_name))\n",
    "                    gca.is_a.append(role_name.some(concept_name))\n",
    "\n",
    "                CanonicalModelElements(gca.left_side)\n",
    "    \n",
    "    else:\n",
    "\n",
    "        for role_name in role_names_iter:\n",
    "            with onto:\n",
    "                gca = GeneralClassAxiom(role_name.some(owl.Thing))\n",
    "                gca.is_a.append(role_name.some(owl.Thing))\n",
    "\n",
    "                CanonicalModelElements(gca.left_side)\n",
    "            \n",
    "    print('')\n",
    "    print('All restrictions have been preprocessed for the creation of the canonical model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The main class for creating the canonical model for the ontology.\n",
    "\n",
    "The canonical model is stored in dictionaries available as class variables 'concept_canonical_interpretation'\n",
    "and 'role_canonical_interpretation'. \n",
    "\n",
    "Args:\n",
    "    concept_names_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_intersection_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    concept_restrictions_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    all_concepts_dict: a dictionary stored in the CanonicalModelElement class.\n",
    "    role_names_iter (list): a list containing all role names in the loaded ontology.\n",
    "'''\n",
    "\n",
    "class CanonicalModel:\n",
    "\n",
    "    concept_canonical_interpretation = {}\n",
    "    role_canonical_interpretation = {}\n",
    "\n",
    "    def __init__(self, concept_names_dict, concept_intersections_dict, concept_restrictions_dict, all_concepts_dict, role_names_iter):\n",
    "        \n",
    "        self.domain = all_concepts_dict\n",
    "        self.concept_names_dict = concept_names_dict\n",
    "        self.concept_restrictions_dict = concept_restrictions_dict\n",
    "        self.concept_intersections_dict = concept_intersections_dict\n",
    "\n",
    "        self.role_names_iter = role_names_iter\n",
    "\n",
    "        self.concept_canonical_interp = self.get_concept_name_caninterp() # These are only used to build the concept_canonical_interpretation and role_canonical_interpretation class attributes\n",
    "        self.role_canonical_interp = self.get_role_name_caninterp()       # The functions do not return anything, they just update their corresponding class variables \n",
    "\n",
    "    def get_concept_name_caninterp(self):\n",
    "\n",
    "        # The variable concept is a string containing the name of an element of the domain of the canonical model\n",
    "        # The key to the concept_names_dict variable corresponds to concept.name\n",
    "        # This name can be used to access the concept in owlready2's format\n",
    "\n",
    "        for concept in self.concept_names_dict.keys():\n",
    "\n",
    "            CanonicalModel.concept_canonical_interpretation[concept] = []\n",
    "            superclasses = self.domain[concept].concept.ancestors(include_self=True, include_constructs=True) # The self.domain[concept] is used to access the CanonicalModelElements type of object,\n",
    "                                                                                                               # and the attribute .concept is used to access the concept in owlready2 format                                                            \n",
    "            for superclass in superclasses:\n",
    "\n",
    "                if type(superclass) == ThingClass:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append(superclass.name)\n",
    "\n",
    "                elif type(superclass) == Restriction:\n",
    "                    CanonicalModel.concept_canonical_interpretation[concept].append('exists_' + superclass.property.name + '.' + superclass.value.name)\n",
    "\n",
    "                elif type(superclass) == And:\n",
    "                    if 'And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)) in CanonicalModel.concept_canonical_interpretation[concept]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        CanonicalModel.concept_canonical_interpretation[concept].append('And_' + ''.join(sorted(superclass.Classes[0].name + superclass.Classes[1].name)))\n",
    "\n",
    "            \n",
    "    def get_role_name_caninterp(self):\n",
    "\n",
    "        # Initialize the dictionary storing the canonical interpretation of roles\n",
    "\n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            role_name_str = role_name.name # Accesses the property type object's name as a string\n",
    "            CanonicalModel.role_canonical_interpretation[role_name_str] = []\n",
    "\n",
    "        # First case from Definition 10\n",
    "                                \n",
    "        for role_name in self.role_names_iter:\n",
    "\n",
    "            superroles = role_name.ancestors(include_self=True)\n",
    "\n",
    "            for superrole in superroles:\n",
    "                for restriction_name, restriction_concept in self.concept_restrictions_dict.items():\n",
    "\n",
    "                    if superrole == restriction_concept.concept.property:\n",
    "                        concept_name_str = restriction_concept.concept.value.name\n",
    "                        pair = (restriction_name, concept_name_str)\n",
    "                        CanonicalModel.role_canonical_interpretation[role_name.name].append(pair)\n",
    "                        \n",
    "        \n",
    "        # Second case from Definition 10\n",
    "\n",
    "        for restriction_name in self.concept_restrictions_dict.keys(): # Where restriction_name denotes a \\exists r.B type of concept 'exists_' + self.concept.property.name + '.' + self.concept.value.name\n",
    "\n",
    "            #print(f'Restriction name init for loop: {restriction_name}')\n",
    "            restriction_concept = self.concept_restrictions_dict[restriction_name].concept\n",
    "            c_B = self.concept_restrictions_dict[restriction_name].concept.value.name\n",
    "            #print(f'c_B: {c_B}\\n')\n",
    "\n",
    "            superclasses = restriction_concept.ancestors(include_self=True, include_constructs=False)\n",
    "\n",
    "            for superclass in superclasses:\n",
    "\n",
    "                super_superclasses = superclass.ancestors(include_self=True, include_constructs=True)\n",
    "\n",
    "                for super_superclass in super_superclasses:\n",
    "                    if type(super_superclass) == ThingClass:\n",
    "                        c_D = super_superclass.name\n",
    "                        CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "                    elif type(super_superclass) == Restriction:\n",
    "                        c_D = 'exists_' + super_superclass.property.name + '.' + super_superclass.value.name\n",
    "                        CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "                    elif type(super_superclass) == And:\n",
    "                        c_D = 'And_' + ''.join(sorted(super_superclass.Classes[0].name + super_superclass.Classes[1].name))\n",
    "                        CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "                    \n",
    "                    \n",
    "\n",
    "            if role_name_str in restriction_name:\n",
    "\n",
    "                #print(f'It is true that {role_name_str} is in {restriction_name}.')\n",
    "                    \n",
    "                superclasses = self.domain[restriction_name].concept.ancestors(include_self=True, include_constructs=False) # Include_constructs is turned to false due to the definition of canonical model\n",
    "\n",
    "                #print(f'These are the superclasses of the restriction_name {restriction_name}:\" {superclasses}')\n",
    "\n",
    "                for superclass in superclasses:\n",
    "                    super_superclasses = superclass.ancestors(include_self=True, include_constructs=True)\n",
    "\n",
    "                    for super_superclass in super_superclasses:\n",
    "\n",
    "                        if type(super_superclass) == ThingClass:\n",
    "                            c_D = super_superclass.name\n",
    "                            CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "                        elif type(super_superclass) == Restriction:\n",
    "                            c_D = 'exists_' + super_superclass.property.name + '.' + super_superclass.value.name\n",
    "                            CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n",
    "\n",
    "                        elif type(super_superclass) == And:\n",
    "                            c_D = 'And_' + ''.join(sorted(super_superclass.Classes[0].name + super_superclass.Classes[1].name))\n",
    "                            CanonicalModel.role_canonical_interpretation[role_name_str].append((c_D, c_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main function for creating the canonical model.\n",
    "\n",
    "    Args:\n",
    "        onto_dir (str): a string pointing to the directory where the ontology is stored.\n",
    "\n",
    "    Returns:\n",
    "        canmodel (CanonicalModel): returns a variable containing the canonical model. \n",
    "        \n",
    "        Attention: the interpretations of concept names and role names can also be accessed via class variables\n",
    "        from the CanonicalModel class.\n",
    "'''\n",
    "\n",
    "def create_canonical_model(onto_dir):\n",
    "\n",
    "    onto = get_ontology(onto_dir)\n",
    "    onto = onto.load()\n",
    "\n",
    "    individuals_iter = list(onto.individuals())\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "\n",
    "    get_canonical_model_elements(concept_names_iter, role_names_iter, onto)\n",
    "\n",
    "    print('============================================================================')\n",
    "    print('Starting to reason.\\n')\n",
    "\n",
    "    with onto:\n",
    "        sync_reasoner()\n",
    "        \n",
    "    #onto.save(\"inferences_goslimyeast.owl\")\n",
    "\n",
    "    gcas_iter = list(onto.general_class_axioms()) # Attention: this will not work unless the generator is converted into a list\n",
    "    concept_names_iter = list(onto.classes())\n",
    "    role_names_iter = list(onto.properties())\n",
    "    individuals_iter = list(onto.individuals())\n",
    "\n",
    "    print('')\n",
    "    print('============================================================================')\n",
    "    print('Done reasoning. Creating the canonical model.')\n",
    "    canmodel = CanonicalModel(CanonicalModelElements.concept_names, CanonicalModelElements.concept_intersections, CanonicalModelElements.concept_restrictions, CanonicalModelElements.all_concepts, role_names_iter)\n",
    "    print('============================================================================\\n')\n",
    "    print('Concluded creating canonical model.')\n",
    "\n",
    "    return canmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates the canonical model\n",
    "\n",
    "canmodel = create_canonical_model(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atenção: a função mu está com complexidade alta devido aos for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Utility functions for initializing\n",
    "the class EntityEmbedding. They\n",
    "allow us to access dictionaries\n",
    "containing indexes and canonical\n",
    "interpretation of concepts\n",
    "and roles as class.\n",
    "'''\n",
    "\n",
    "def get_concept_names_idx_dict(canmodel):\n",
    "   conceptnames_idx_dict = {concept_name: idx for idx, concept_name in enumerate(CanonicalModel.concept_canonical_interpretation.keys())}\n",
    "   return conceptnames_idx_dict\n",
    "\n",
    "def get_role_names_idx_dict(canmodel):\n",
    "    rolenames_idx_dict = {role_name: idx for idx, role_name in enumerate(CanonicalModel.role_canonical_interpretation.keys())}\n",
    "    return rolenames_idx_dict\n",
    "\n",
    "def get_entities_idx_dict(canmodel):\n",
    "    entities_idx_dict = {entity: idx for idx, entity in enumerate(canmodel.domain.keys())}\n",
    "    return entities_idx_dict\n",
    "\n",
    "def get_domain_dict(canmodel):\n",
    "    return canmodel.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Class for obtaining the positional \n",
    "embedding for each entity in the domain\n",
    "of the canonical interpretation.\n",
    "It represents the Mu Function from the\n",
    "paper.\n",
    "'''\n",
    "\n",
    "class EntityEmbedding:\n",
    "\n",
    "    # Dictionaries for storing the indices of concept names and role names, entities pairs, respectively\n",
    "    # Keys are strings and values are integers\n",
    "    \n",
    "    concept_names_idx_dict = get_concept_names_idx_dict(canmodel)\n",
    "    role_names_idx_dict = get_role_names_idx_dict(canmodel)\n",
    "    entities_idx_dict = get_entities_idx_dict(canmodel)\n",
    "\n",
    "    # Dictionaries accessing the canonical interpretation of concepts and roles\n",
    "    # Keys and values are strings\n",
    "    \n",
    "    concept_canonical_interpretation_dict = CanonicalModel.concept_canonical_interpretation\n",
    "    role_canonical_interpretation_dict = CanonicalModel.role_canonical_interpretation\n",
    "\n",
    "    # Dictionary storing the domain of the canonical model being embedded\n",
    "    # IMPORTANT: Keys are strings and values are CanonicalModelElements type objects\n",
    "    \n",
    "    domain_dict = get_domain_dict(canmodel)\n",
    "\n",
    "    # Dictionary for easy access to entity embeddings\n",
    "    # It is initialized with empty values, iteratively built by the .get_embedding_vector() method\n",
    "    # Key (str): Domain Entity / Value (np.array): EntityEmbedding.embedding_vector\n",
    "\n",
    "    entity_entityvector_dict = dict.fromkeys(domain_dict.keys())\n",
    "\n",
    "    def __init__(self, entity_name, emb_dim, scale_factor):\n",
    "        self.name = entity_name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.scale_factor = scale_factor\n",
    "        self.in_interpretation_of = []\n",
    "        self.embedding_vector = self.get_embedding_vector()\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        \n",
    "        embedding_vector = np.zeros((self.emb_dim,))\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = []\n",
    "\n",
    "        # Applies the embedding function to the concept names portion of the definition\n",
    "\n",
    "        for concept_name in EntityEmbedding.concept_canonical_interpretation_dict:\n",
    "            concept_name_idx = EntityEmbedding.concept_names_idx_dict[concept_name]\n",
    "        \n",
    "            if self.name in EntityEmbedding.concept_canonical_interpretation_dict[concept_name]:\n",
    "                embedding_vector[concept_name_idx] = 1 * self.scale_factor\n",
    "                self.in_interpretation_of.append(concept_name)\n",
    "\n",
    "        # Applies the embedding function to the role names portion of the definition\n",
    "                \n",
    "        #print(f'Now embedding the role section of entity {self.name}')\n",
    "\n",
    "        for role_name in EntityEmbedding.role_canonical_interpretation_dict:\n",
    "            \n",
    "            role_name_idx = len(EntityEmbedding.concept_names_idx_dict) + (EntityEmbedding.role_names_idx_dict[role_name] * len(EntityEmbedding.entities_idx_dict)) # Entities dict indexes on the domain of the canonical model\n",
    "            \n",
    "            #print(f'This is the role name {role_name}\\n This is the role_name_idx {role_name_idx}')\n",
    "\n",
    "            role_name_caninterp = EntityEmbedding.role_canonical_interpretation_dict[role_name]\n",
    "       \n",
    "            #print(f'This is the caninterp of {role_name}: {role_name_caninterp}.')\n",
    "\n",
    "            for pair in role_name_caninterp:\n",
    "                #print(f'The pair {pair} is in consideration.')\n",
    "\n",
    "                entity_2 = pair[1] # This is a string\n",
    "\n",
    "                if (self.name, entity_2) == pair:\n",
    "                    #print(f'!!!!!!!!!!!! The condition was triggered. !!!!!!!!!!!!')\n",
    "                    entity_2_idx = EntityEmbedding.entities_idx_dict[entity_2]\n",
    "                    #print(f'Index of entity_2: {entity_2_idx}')\n",
    "                    final_role_entity_pair_idx = role_name_idx + entity_2_idx\n",
    "                    #print(f'Final computed index for the {pair}: {final_role_entity_pair_idx}')\n",
    "                    embedding_vector[final_role_entity_pair_idx] = 1 * self.scale_factor\n",
    "\n",
    "        #print(f'========================================================\\n')\n",
    "        # EntityEmbedding.entity_entityvector_dict[self.name].append(embedding_vector)\n",
    "        EntityEmbedding.entity_entityvector_dict[self.name] = embedding_vector\n",
    "\n",
    "        return embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the binary vectors representing\n",
    "each element of the domain of the canonical interpretation.\n",
    "\n",
    "    Args:\n",
    "        emb_dim (int/float): the number of dimensions of the embedding space.\n",
    "\n",
    "    Returns:\n",
    "        embedded_entities (list): a list containing all embeddings of the entities\n",
    "                                  in the domain. \n",
    "    \n",
    "    The embedded_entities are also available in the dictionary EntityEmbeddings.entity_entityvector_dict\n",
    "'''\n",
    "\n",
    "def get_domain_embeddings(emb_dim, scale_factor):\n",
    "\n",
    "    embedded_entities = []\n",
    "    counter = 0\n",
    "\n",
    "   # The entities in the domain are strings\n",
    "    \n",
    "    for entity_name in EntityEmbedding.domain_dict:\n",
    "       embedded_entity = EntityEmbedding(entity_name, emb_dim, scale_factor)\n",
    "       embedded_entities.append(embedded_entity)\n",
    "       counter += 1\n",
    "       \n",
    "       if counter % 1000 == 0:\n",
    "           print(counter)\n",
    "       \n",
    "    return embedded_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final class for creating the dataset.\n",
    "\n",
    "Inputs: concept or role names, generated\n",
    "embeddings for entities in the domain of\n",
    "the canonical model.\n",
    "\n",
    "Outputs: geometrical interpretation of\n",
    "concepts and role names, represented\n",
    "by vertices defining a region.\n",
    "\n",
    "One can access the GeometricInterpretation\n",
    "objects either as elements in a list, or as\n",
    "values in a class variable dictionary.\n",
    "'''\n",
    "\n",
    "class GeometricInterpretation:\n",
    "\n",
    "    concept_geointerps_dict = dict.fromkeys(CanonicalModel.concept_canonical_interpretation.keys())\n",
    "    role_geointerps_dict = dict.fromkeys(CanonicalModel.role_canonical_interpretation.keys())\n",
    "\n",
    "    def __init__(self, name, emb_dim):\n",
    "        self.name = name\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vertices = []\n",
    "        self.centroid = None\n",
    "\n",
    "        if GeometricInterpretation.concept_geointerps_dict.get(name) is not None:\n",
    "            GeometricInterpretation.concept_geointerps_dict[name] = []\n",
    "\n",
    "    def get_centroid_naive(self):\n",
    "        if len(self.vertices) == 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) == 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            centroid = np.zeros((self.emb_dim * 2,)) # The centroid for the regions needs to be doubled due to the concat operation\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.concept_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        \n",
    "        elif len(self.vertices) > 0 and self.name in self.role_geointerps_dict.keys():\n",
    "            n = len(self.vertices)\n",
    "            centroid = np.zeros((self.emb_dim,))\n",
    "            matrix = np.vstack(self.vertices)\n",
    "            centroid = 1/n * np.sum(matrix, axis=0)\n",
    "            return centroid\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There has to be a more efficient way of doing the creating of geometric interpretations for concepts and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_finder(emb_dim, concept_name_idx_dict, role_name_idx_dict, domain_idx_dict):\n",
    "\n",
    "    index_dict = {k: None for k in range(emb_dim)}\n",
    "\n",
    "    for k,v in concept_name_idx_dict.items():\n",
    "\n",
    "        index_dict[v] = k\n",
    "\n",
    "    for role in role_name_idx_dict:\n",
    "        role_init_idx = len(concept_name_idx_dict) + (role_name_idx_dict[role] * len(domain_idx_dict))\n",
    "\n",
    "        for entity in domain_idx_dict:\n",
    "            entity_init_idx = domain_idx_dict[entity]\n",
    "            final_role_entity_pair_idx = role_init_idx + entity_init_idx\n",
    "            index_dict[final_role_entity_pair_idx] = (role, entity)\n",
    "        \n",
    "    return index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, entity_dims_index_dict, emb_dim, canmodel: CanonicalModel):\n",
    "\n",
    "    faithful_concept_geometric_interps = []\n",
    "\n",
    "    for concept_name in concept_names_interps.keys():\n",
    "        concept_name = GeometricInterpretation(concept_name, emb_dim)\n",
    "\n",
    "        for embedding in domain_embeddings_list:\n",
    "            if concept_name.name in embedding.in_interpretation_of:\n",
    "                concept_name.vertices.append(embedding.embedding_vector)\n",
    "            \n",
    "        GeometricInterpretation.concept_geointerps_dict[concept_name.name] = concept_name\n",
    "        concept_name.centroid = concept_name.get_centroid_naive()\n",
    "        \n",
    "        faithful_concept_geometric_interps.append(concept_name)\n",
    "\n",
    "    return faithful_concept_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faithful_role_geometric_interps(role_names_interps, entity_embeddings_list, entity_dims_index_dict, emb_dim, scaling_factor, canmodel: CanonicalModel):\n",
    "    \n",
    "    faithful_role_geometric_interps = []\n",
    "    idx_entity_dict = entity_dims_index_dict\n",
    "    #entity_idx_dict = {v: k for k,v in entity_dims_index_dict}\n",
    "\n",
    "    relevant_idxs = len(canmodel.concept_names_dict)\n",
    "\n",
    "    for role_name in role_names_interps.keys():\n",
    "        # print(f'This is the role in consideration: {role_name}')\n",
    "        role_name_str = role_name\n",
    "        role_name = GeometricInterpretation(role_name_str, emb_dim)\n",
    "\n",
    "        for entity in entity_embeddings_list:\n",
    "\n",
    "            onehot_idx_list = np.where(entity.embedding_vector == 1 * scaling_factor)[0]\n",
    "            # print(f'This is the entity: {entity.name} and this is the onehot_idx_list: {onehot_idx_list}')\n",
    "\n",
    "            for idx in onehot_idx_list: # I could just look at the TRULY relevant indexes\n",
    "                if idx >= relevant_idxs:\n",
    "                    # print(f'!!!!!!!!!!!!!! CONDITION TRIGGERED !!!!!!!!!!!!!!')\n",
    "                    role_entity_pair = idx_entity_dict[idx]\n",
    "                    # print(f'{role_entity_pair}')\n",
    "                    r_name_str = role_entity_pair[0]\n",
    "                    e_name_str = role_entity_pair[1]\n",
    "\n",
    "                    if r_name_str == role_name_str:\n",
    "                        e_embedding = EntityEmbedding.entity_entityvector_dict[e_name_str]\n",
    "                        role_name.vertices.append(np.concatenate((entity.embedding_vector, e_embedding)))\n",
    "\n",
    "        GeometricInterpretation.role_geointerps_dict[role_name_str] = role_name\n",
    "        role_name.centroid = role_name.get_centroid_naive()\n",
    "        faithful_role_geometric_interps.append(role_name)\n",
    "\n",
    "    return faithful_role_geometric_interps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default for the scale_factor is 1 to create binary vectors.\n",
    "\n",
    "def create_tbox_embeddings(canonical_model: CanonicalModel, scale_factor = 1):\n",
    "\n",
    "    domain = canonical_model.domain # Keys are strings and values are CanonicalModelElements type objects\n",
    "    concept_names_interps = canonical_model.concept_canonical_interpretation # Keys are strings and values are lists of strings.\n",
    "    role_names_interps = canonical_model.role_canonical_interpretation # Keys are strings and values are lists of tuples. Tuples are of form ('C', 'D'), with C and D strings.\n",
    "\n",
    "    EMB_DIM = len(concept_names_interps) + len(role_names_interps) * len(domain)\n",
    "    SCALE_FACTOR = scale_factor\n",
    "\n",
    "    print('================EMBEDDING DIMENSION================')\n",
    "    print(f'Concept Name dimensions: {len(concept_names_interps)}')\n",
    "    print(f'The number of role names is: {len(role_names_interps)}')\n",
    "    print(f'The size of the domain is: {len(domain)}')\n",
    "    print(f'Role names dimensions: {len(role_names_interps) * len(domain)}')\n",
    "    print('===================================================')\n",
    "    print('')\n",
    "    print(f'Final embedding dimension: {EMB_DIM}')\n",
    "    print(f'The final dimension for role regions is: {EMB_DIM * 2}')\n",
    "\n",
    "    domain_embeddings_list = get_domain_embeddings(EMB_DIM, SCALE_FACTOR) # This function initializes the vectors with 0 and one-hot encodes them according to \\mu\n",
    "    \n",
    "    concept_names_ordering = EntityEmbedding.concept_names_idx_dict\n",
    "    role_names_ordering = EntityEmbedding.role_names_idx_dict\n",
    "    entities_ordering = EntityEmbedding.entities_idx_dict\n",
    "    \n",
    "    print('')\n",
    "    print('===============FINISHED EMBEDDINGS===============')\n",
    "    print(f'There are {len(domain_embeddings_list)} vector embeddings.')\n",
    "    print('')\n",
    "\n",
    "    index_finder_dict = index_finder(EMB_DIM, concept_names_ordering, role_names_ordering, entities_ordering)\n",
    "\n",
    "    faithful_concept_geometric_interps = get_faithful_concept_geometric_interps(concept_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, canonical_model)\n",
    "\n",
    "    print('============FINISHED INTERPS CONCEPT=============')\n",
    "    print(f'There are {len(faithful_concept_geometric_interps)} regions for concept names.')\n",
    "    print('')\n",
    "\n",
    "    faithful_role_geometric_interps = get_faithful_role_geometric_interps(role_names_interps, domain_embeddings_list, index_finder_dict, EMB_DIM, SCALE_FACTOR, canonical_model)\n",
    "\n",
    "    print('=============FINISHED INTERPS ROLES==============')\n",
    "    print(f'There are {len(faithful_role_geometric_interps)} regions for role names.')\n",
    "    print('')\n",
    "\n",
    "    return domain_embeddings_list, faithful_concept_geometric_interps, faithful_role_geometric_interps, index_finder_dict, int(EMB_DIM), int(scale_factor) # Returns the faithful geometric interpretations for concepts and roles as lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALE_FACTOR = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_embeddings, concept_geointerps, role_geointerps, idx_finder_dict, EMB_DIM, SCALE_FACTOR = create_tbox_embeddings(canmodel, SCALE_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding in domain_embeddings:\n",
    "    print(f'Embedding name: {embedding.name}')\n",
    "    print(f'In the interpretation of: {embedding.in_interpretation_of}')\n",
    "    print(f'Embedding vector: {embedding.embedding_vector}')\n",
    "    print(f'These are the one-hot indices: {np.where(embedding.embedding_vector != 0)}')\n",
    "    print(f'The indices above correspond to these dimensions\" {[idx_finder_dict[int(idx)] for idx in np.where(embedding.embedding_vector != 0)[0]]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for role_name, geointerp in GeometricInterpretation.role_geointerps_dict.items():\n",
    "    print(f'Considering role: {role_name}')\n",
    "    print(f'There are {len(geointerp.vertices)} elements in the vertex set for {role_name}')\n",
    "    for vertex in geointerp.vertices:\n",
    "        one_hot_idx_one = np.where(vertex[:459] != 0)[0]\n",
    "        one_hot_idx_two = np.where(vertex[459:] != 0)[0]\n",
    "        print(f'First vector one_hot_idx: {one_hot_idx_one}')\n",
    "        first_vector_correspondence = [idx_finder_dict[int(idx)] for idx in one_hot_idx_one]\n",
    "        print(f'First vector correspondence indices: {first_vector_correspondence}')\n",
    "        print(f'Second vector one_hot_idx: {one_hot_idx_two}')\n",
    "        second_vector_correspondence = [idx_finder_dict[int(idx)] for idx in one_hot_idx_two]\n",
    "        print(f'Second vector correspondence indices{second_vector_correspondence}\\n')\n",
    "    print('======================NEXT ROLE======================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_restriction_vertices(restriction_concept, concept_geointerps_dict, role_geointerps_dict, index_finder_dict, CanonicalModel: CanonicalModel, EntityEmbedding: EntityEmbedding):\n",
    "    \n",
    "    concept_name = restriction_concept.value\n",
    "    role_name = restriction_concept.property\n",
    "\n",
    "    concept_name_str = concept_name.name\n",
    "    role_name_str = role_name.name\n",
    "\n",
    "    vertices = []\n",
    "\n",
    "    for element in list(CanonicalModel.role_canonical_interpretation.keys()):\n",
    "        if role_name_str in element:\n",
    "            role_interpretation_set = CanonicalModel.role_canonical_interpretation[role_name_str]\n",
    "\n",
    "            for pair in role_interpretation_set:\n",
    "                element_1 = pair[0]\n",
    "                element_2 = pair[1]\n",
    "\n",
    "                if element_2 in CanonicalModel.concept_canonical_interpretation[concept_name_str]:\n",
    "                    vertices.append(EntityEmbedding.entity_entityvector_dict[element_1])\n",
    "    \n",
    "    return np.array(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection_vertices(restriction_concept, concept_geointerps_dict, role_geointerps_dict, index_finder_dict, CanonicalModel: CanonicalModel, EntityEmbedding: EntityEmbedding):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function for creating the pre-split dataset containing facts from the ontology.\n",
    "Distinguishes between concept assertions and role assertions.\n",
    "\n",
    "\n",
    "    Args: ontology_dir (str): the directory from the ontology\n",
    "          concept_geointerps_dict (dict): the geometrical interpretations for concepts generated by create_tbox_embeddings()\n",
    "          role_geointerps_dict (dict): the geometrical interpretations for roles generated by create_tbox_embeddings()\n",
    "\n",
    "    Returns:\n",
    "          X_concepts (np.array): A dataframe with columns 'Concept', 'Entity', 'y_true' (equivalent to concept.centroid())\n",
    "          X_roles (np.array): A dataframe with columns 'SubjectEntity', 'Role', 'ObjectEntity', 'y_true' (equivalent to role.centroid())\n",
    "          y_concepts (np.array):\n",
    "          y_roles (np.array):\n",
    "          vocabulary_dict (dict): A vocabulary with key (int): value (str) for entities in the domain.\n",
    "'''\n",
    "\n",
    "def get_abox_dataset(ontology_dir: str,\n",
    "                     concept_geointerps_dict: dict, role_geointerps_dict: dict,\n",
    "                     concept_to_idx: dict, role_to_idx: dict,\n",
    "                     index_finder_dict: dict, emb_dim = int,\n",
    "                     CanonicalModel = CanonicalModel, EntityEmbedding=EntityEmbedding):\n",
    "    \n",
    "    ontology = get_ontology(ontology_dir)\n",
    "    ontology = ontology.load()\n",
    "    \n",
    "    X_concepts = []\n",
    "    X_roles = []\n",
    "    y_concepts = []\n",
    "    y_roles = []\n",
    "\n",
    "    entities = [entity.name for entity in list(ontology.individuals())]\n",
    "    \n",
    "    concept_to_idx_vocab = concept_to_idx\n",
    "    idx_to_concept_vocab = {value: key for key, value in concept_to_idx_vocab.items()}\n",
    "\n",
    "    role_to_idx_vocab = role_to_idx\n",
    "    idx_to_role_vocab = {value: key for key, value in role_to_idx_vocab.items()}\n",
    "    \n",
    "    entity_to_idx_vocab = {value: index for index, value in enumerate(entities)}\n",
    "    idx_to_entity_vocab = {value: key for key, value in entity_to_idx_vocab.items()}\n",
    "\n",
    "    for individual in list(ontology.individuals()):\n",
    "\n",
    "        all_facts = individual.INDIRECT_is_a\n",
    "\n",
    "        for concept in all_facts:\n",
    "            # Handles concepts of the type A\n",
    "            if type(concept) == ThingClass:\n",
    "                concept = concept_geointerps_dict[concept.name]\n",
    "                fact = np.array([concept_to_idx_vocab[concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                y_label = np.array(concept.centroid)\n",
    "                X_concepts.append(fact)\n",
    "                y_concepts.append(y_label)\n",
    "                \n",
    "            # Handles concepts of the type A \\and B\n",
    "            elif type(concept) == And:\n",
    "                concept1 = concept_geointerps_dict[concept.Classes[0]]\n",
    "                concept2 = concept_geointerps_dict[concept.Classes[1]]\n",
    "                intersection_name = 'And_' + ''.join(sorted(concept1.name + concept2.name))\n",
    "\n",
    "                if concept_to_idx_vocab.get(intersection_name) is not None:\n",
    "                    fact = np.array([concept_to_idx_vocab[intersection_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "                else:\n",
    "                    concept_to_idx_vocab[intersection_name] = len(concept_to_idx_vocab)\n",
    "                    idx_to_concept_vocab[len(concept_to_idx_vocab)] = intersection_name\n",
    "                    fact = np.array([concept_to_idx_vocab[intersection_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array((concept1.centroid + concept2.centroid)/2) # The golden label for an intersection is just the average of the centroid of the two regions\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "            \n",
    "            # Handles concepts of the type \\exists r.B\n",
    "            elif type(concept) == Restriction:\n",
    "                concept_name = concept.value\n",
    "                role_name = concept.property\n",
    "                restriction_name = 'exists_' + role_name.name + '.' + concept_name.name\n",
    "\n",
    "                if concept_to_idx_vocab.get(restriction_name) is not None:\n",
    "                \n",
    "                    fact = np.array([concept_to_idx_vocab[restriction_name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = np.array(GeometricInterpretation.concept_geointerps_dict[restriction_name].centroid)\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "                else:\n",
    "                    concept_to_idx_vocab[restriction_name] = len(concept_to_idx_vocab)\n",
    "                    idx_to_concept_vocab[len(concept_to_idx_vocab)-1] = restriction_name\n",
    "                    restriction_concept = GeometricInterpretation(restriction_name, EMB_DIM) # Initializes a Geometric Interpretation type object\n",
    "                    restriction_concept.vertices = get_restriction_vertices(concept, concept_geointerps_dict,\n",
    "                                                                            role_geointerps_dict, index_finder_dict, CanonicalModel, EntityEmbedding)\n",
    "                    \n",
    "                    GeometricInterpretation.concept_geointerps_dict[restriction_name] = restriction_concept\n",
    "                    restriction_concept.centroid = restriction_concept.get_centroid_naive()\n",
    "                    fact = np.array([concept_to_idx_vocab[restriction_concept.name], entity_to_idx_vocab[individual.name]])\n",
    "                    y_label = restriction_concept.centroid\n",
    "                    X_concepts.append(fact)\n",
    "                    y_concepts.append(y_label)\n",
    "\n",
    "        relevant_roles = individual.get_properties()\n",
    "        individual_name = individual.name\n",
    "\n",
    "        for role in relevant_roles:\n",
    "\n",
    "            role_geo = role_geointerps_dict[role.name]\n",
    "            subject_list = role[individual] # This syntax is from the owlready2 library\n",
    "\n",
    "            for subject in subject_list:\n",
    "                fact = np.array([entity_to_idx_vocab[individual.name], role_to_idx_vocab[role.name], entity_to_idx_vocab[subject.name]])\n",
    "\n",
    "                X_roles.append(fact)\n",
    "                y_label = y_roles.append(np.array(role_geo.centroid))\n",
    "\n",
    "    return np.array(X_concepts), np.array(X_roles), np.array(y_concepts), np.array(y_roles), entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_concepts, X_roles, y_concepts, y_roles, entity_to_idx_vocab, idx_to_entity_vocab, concept_to_idx_vocab, idx_to_concept_vocab, role_to_idx_vocab, idx_to_role_vocab = get_abox_dataset(dir,\n",
    "                                                                                                                                                                                        GeometricInterpretation.concept_geointerps_dict,\n",
    "                                                                                                                                                                                        GeometricInterpretation.role_geointerps_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.concept_names_idx_dict,\n",
    "                                                                                                                                                                                        EntityEmbedding.role_names_idx_dict,\n",
    "                                                                                                                                                                                        idx_finder_dict,\n",
    "                                                                                                                                                                                        EMB_DIM,\n",
    "                                                                                                                                                                                        CanonicalModel, EntityEmbedding\n",
    "                                                                                                                                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OntologyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.X = torch.tensor(data, dtype=torch.long)\n",
    "        self.y = torch.tensor(labels, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx].long(), self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE_PROPORTION = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(200)\n",
    "ConceptDataset = OntologyDataset(X_concepts, y_concepts)\n",
    "\n",
    "dataset_size = len(ConceptDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainConceptDataset, testConceptDataset = torch.utils.data.random_split(ConceptDataset, [train_size, test_size])\n",
    "RoleDataset = OntologyDataset(X_roles, y_roles)\n",
    "\n",
    "dataset_size = len(RoleDataset)\n",
    "train_size = int(TRAIN_SIZE_PROPORTION * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "trainRoleDataset, testRoleDataset = torch.utils.data.random_split(RoleDataset, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_ConceptDataLoader = DataLoader(trainConceptDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "test_ConceptDataLoader = DataLoader(testConceptDataset, batch_size = BATCH_SIZE, shuffle=True)\n",
    "\n",
    "train_RoleDataLoader = DataLoader(trainRoleDataset, batch_size = BATCH_SIZE, shuffle=False)\n",
    "test_RoleDataLoader = DataLoader(testRoleDataset, batch_size = BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaithEL(nn.Module):\n",
    "    def __init__(self, emb_dim, phi, radius,\n",
    "                 individual_vocabulary,\n",
    "                 concept_vocabulary,\n",
    "                 role_vocabulary,\n",
    "                 init_individual_param_to_centroid = False,\n",
    "                 init_concept_param_to_centroid = False,\n",
    "                 init_role_param_to_centroid = False):\n",
    "        \n",
    "        super(FaithEL, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.phi = phi\n",
    "        self.radius = radius\n",
    "\n",
    "        self.individual_embedding_dict = nn.Embedding(len(individual_vocabulary),\n",
    "                                                      emb_dim\n",
    "                                                      )\n",
    "        \n",
    "        if init_individual_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                value = SCALE_FACTOR/2\n",
    "                std_dev = 0.3\n",
    "                self.individual_embedding_dict.weight.data.normal_(mean=value, std=std_dev)\n",
    "        \n",
    "        self.concept_embedding_dict = nn.Embedding(len(concept_vocabulary),\n",
    "                                                   emb_dim\n",
    "                                                   )\n",
    "        \n",
    "        # Initializes the moving parameter for concepts at the concept's respective centroid\n",
    "        if init_concept_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for concept_name, concept_idx in concept_vocabulary.items():\n",
    "                    self.concept_embedding_dict.weight[concept_idx] = torch.tensor(GeometricInterpretation.concept_geointerps_dict[concept_name].centroid)\n",
    "\n",
    "        self.role_embedding_dict = nn.Embedding(len(role_vocabulary),\n",
    "                                                emb_dim * 2\n",
    "                                                )\n",
    "        \n",
    "        # Initializes the moving parameter for roles at the role's respective centroid\n",
    "        if init_role_param_to_centroid == True:\n",
    "            with torch.no_grad():\n",
    "                for role_name, role_idx in role_vocabulary.items():\n",
    "                    self.role_embedding_dict.weight[role_idx] = torch.tensor(GeometricInterpretation.role_geointerps_dict[role_name].centroid)\n",
    "        \n",
    "    \n",
    "    def forward(self, data):\n",
    "    \n",
    "        # Concept assertions are of the form ['Concept', 'Entity']\n",
    "        # Role assertions are of the form ['SubjectEntity', 'Role', 'ObjectEntity']\n",
    "        \n",
    "        subj_entity_idx = 1 if len(data[0]) == 2 else 0 # Checks whether the model has received a C assert or R assert\n",
    "\n",
    "        if subj_entity_idx == 1:\n",
    "        \n",
    "            concept_idx = 0\n",
    "\n",
    "            subj_entity = data[:, subj_entity_idx]\n",
    "            concept = data[:, concept_idx]\n",
    "\n",
    "            c_assertion_out1 = self.concept_embedding_dict(concept) # Outputs the moving parameter for the concept\n",
    "            c_assertion_out2 = self.individual_embedding_dict(subj_entity) # Outputs the embedding for the individual\n",
    "            \n",
    "            out1 = c_assertion_out1\n",
    "            out2 = c_assertion_out2\n",
    "            \n",
    "            return out1, out2\n",
    "\n",
    "        elif subj_entity_idx == 0:\n",
    "            role_idx = 1\n",
    "            obj_entity_idx = 2\n",
    "        \n",
    "            subject_entity = self.individual_embedding_dict(data[:, subj_entity_idx])\n",
    "            object_entity = self.individual_embedding_dict(data[:, obj_entity_idx])\n",
    "            role = self.role_embedding_dict(data[:, role_idx])\n",
    "\n",
    "            r_assertion_out1 = role # Role parameter embedding\n",
    "            r_assertion_out2 = torch.cat((subject_entity.detach(), object_entity), 1) # Concatenation of subject and object\n",
    "\n",
    "            out1 = r_assertion_out1\n",
    "            out2 = r_assertion_out2\n",
    "            \n",
    "            return out1, out2\n",
    "        \n",
    "    def concept_parameter_constraint(self):\n",
    "        with torch.no_grad():\n",
    "            for idx, weight in enumerate(self.concept_embedding_dict.weight):\n",
    "                centroid = torch.tensor(GeometricInterpretation.concept_geointerps_dict[list(GeometricInterpretation.concept_geointerps_dict.keys())[idx]].centroid)\n",
    "                distance = torch.dist(weight, centroid, p=2)\n",
    "                if distance > self.radius:\n",
    "                    self.concept_embedding_dict.weight[idx] = centroid + self.radius * (weight - centroid) / distance\n",
    "\n",
    "    def role_parameter_constraint(self):\n",
    "        with torch.no_grad():\n",
    "            for idx, weight in enumerate(self.role_embedding_dict.weight):\n",
    "                centroid = torch.tensor(GeometricInterpretation.role_geointerps_dict[list(GeometricInterpretation.role_geointerps_dict.keys())[idx]].centroid)\n",
    "                distance = torch.dist(weight, centroid, p=2)\n",
    "                if distance > self.radius:\n",
    "                    self.role_embedding_dict.weight[idx] = centroid + self.radius * (weight - centroid) / distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param_dists(model, dictionary_concept_to_idx, role = False):\n",
    "\n",
    "    dist_dict = {}\n",
    "\n",
    "    for predicate, idx in dictionary_concept_to_idx.items():\n",
    "        with torch.no_grad():\n",
    "            if role == False:\n",
    "                dist = torch.dist(torch.tensor(GeometricInterpretation.concept_geointerps_dict[predicate].centroid), model.concept_embedding_dict.weight[idx])\n",
    "                dist_dict[predicate] = dist\n",
    "            else:\n",
    "                dist = torch.dist(torch.tensor(GeometricInterpretation.role_geointerps_dict[predicate].centroid), model.concept_embedding_dict.weight[idx])\n",
    "                dist_dict[predicate] = dist\n",
    "\n",
    "    return dist_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, concept_dataloader, role_dataloader, loss_fn, optimizer, alternate_training = False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "    \n",
    "    if alternate_training == False:\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            model.train()\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            # loss = torch.dist(outputs2, labels, p=2) + torch.dist(outputs1, outputs2, p=2) + model.phi * torch.dist(outputs1, labels, p=2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # model.role_parameter_constraint()\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            model.train()\n",
    "            \n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            #loss = torch.dist(outputs2, labels, p=2) + torch.dist(outputs1, outputs2, p=2) + model.phi * torch.dist(outputs1, labels, p=2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #model.concept_parameter_constraint()\n",
    "\n",
    "    else:\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "            model.train()\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "            loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2)  + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            model.concept_parameter_constraint()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            model.train()\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "            loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2) + model.phi * loss_fn(outputs1, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            model.role_parameter_constraint()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, concept_dataloader, role_dataloader, loss_fn, alternate_training = False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(concept_dataloader)\n",
    "\n",
    "    if alternate_training == False:\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "            inputs, labels = data\n",
    "            outputs1, outputs2 = model(inputs)\n",
    "            loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "            #loss = torch.dist(outputs2, labels, p=2) + torch.dist(outputs1, outputs2, p=2) + model.phi * torch.dist(outputs1, labels, p=2)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(concept_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs)\n",
    "                loss = loss_fn(outputs2, labels) + loss_fn(outputs1, outputs2) + model.phi * loss_fn(outputs1, labels)\n",
    "                #loss = torch.dist(outputs2, labels, p=2) + torch.dist(outputs1, outputs2, p=2) + model.phi * torch.dist(outputs1, labels, p=2)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i, data in enumerate(concept_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs) # Outputs 1 = Concept Parameter, Outputs 2 = Entity Parameter\n",
    "                loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2)  + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        for i, data in enumerate(role_dataloader):\n",
    "                inputs, labels = data\n",
    "                outputs1, outputs2 = model(inputs) # Outputs 1 = Role Parameter, Outputs 2 = Entity concat parameter\n",
    "                loss = loss_fn(outputs2, labels) + (loss_fn(outputs1, outputs2) + loss_fn(outputs1, labels) / 2) + model.phi * loss_fn(outputs1, labels)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_concept_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    relevant_concept_idx = []\n",
    "\n",
    "    # Gathers only concepts appearing in the test set (it is not guaranteed that if a concept appears in the dataset, then it appears here)\n",
    "\n",
    "    for assertion in test_concept_assertions:\n",
    "        inputs, _ = assertion\n",
    "        if inputs[0] not in relevant_concept_idx:\n",
    "            relevant_concept_idx.append(inputs[0])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    #print(f'Relevant concept idx: {relevant_concept_idx}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for concept_idx in relevant_concept_idx:\n",
    "\n",
    "            assertion_scores = []\n",
    "\n",
    "            for _, entity_idx in entity_to_idx_vocab.items():\n",
    "                eval_sample = torch.tensor([concept_idx, entity_idx]).unsqueeze(0)\n",
    "                outputs1, outputs2 = model(eval_sample) # out1 = Concept parameter, out2 = Individual parameter\n",
    "\n",
    "                if centroid_score == False:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2) # Distance from the individual embedding from the concept parameter embedding\n",
    "                else:\n",
    "                    assertion_score = torch.dist(outputs1, outputs2, p=2) + torch.dist(outputs2, torch.tensor(GeometricInterpretation.concept_geointerps_dict[idx_to_concept_vocab[int(concept_idx)]].centroid)) \n",
    "                    # Distance from the individual embedding from concept param embedding plus distance from the ind emb to the centroid of the geointerp of the concept\n",
    "\n",
    "                assertion_scores.append((torch.tensor([concept_idx, entity_idx]), assertion_score.item()))\n",
    "            \n",
    "            sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "            #print(f'Current query concept: {concept_idx}')\n",
    "            #print(f'Centroid_score = {centroid_score}')\n",
    "            #print(f'Assertion scores: {assertion_scores}')\n",
    "            #print(f'Sorted scores: {sorted_scores}\\n')\n",
    "\n",
    "            k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "            hit_k_values = []\n",
    "\n",
    "            true_samples = [inputs for inputs, _ in test_concept_assertions if inputs[0] == concept_idx] # This is problematic when dealing with big datasets\n",
    "\n",
    "            #print(f'True samples in evaluation dset: {true_samples}')\n",
    "\n",
    "            for k in k_list:\n",
    "                hit_k = any(torch.equal(scored_sample[0], true_sample) for true_sample in true_samples for scored_sample in sorted_scores[:k])\n",
    "                hit_k_values.append(hit_k)\n",
    "                #print(f'Top{k}hits: {hit_k}')\n",
    "            \n",
    "            hits.append(hit_k_values)\n",
    "\n",
    "            top1 += int(hit_k_values[0])\n",
    "            top3 += int(hit_k_values[1])\n",
    "            top10 += int(hit_k_values[2])\n",
    "            top100 += int(hit_k_values[3])\n",
    "            top_all += int(hit_k_values[4])\n",
    "\n",
    "    hits_at_k = [round(sum(hit_values) / len(hit_values), 3) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "\n",
    "    # return hits_at_k, [top1, top3, top10, top100, top_all]\n",
    "    return hits_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits_at_k_role_assertions(model,\n",
    "                  test_concept_assertions=Dataset, test_role_assertions=Dataset,\n",
    "                  entity_to_idx_vocab=dict, idx_to_entity_vocab=dict,\n",
    "                  idx_to_concept_vocab=dict, idx_to_role_vocab=dict,\n",
    "                  centroid_score = False\n",
    "                  ):\n",
    "    \n",
    "    top1 = 0\n",
    "    top3 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    top_all = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hits = []\n",
    "    relevant_assertions = []\n",
    "\n",
    "    # Convert PyTorch dataset to a numpy array for vectorization\n",
    "    assertions_array = [assertion[0].numpy() for assertion in test_role_assertions]\n",
    "    assertions_array = np.stack(assertions_array)\n",
    "\n",
    "    ''' \n",
    "    The array below is used to disregard duplicate queries.\n",
    "    For ex., if we have two assertions r(a,b) and r(a,c), the function\n",
    "    will treat r(a, ?) as a query with b and c as positive answers. It\n",
    "    will then disregard any other.\n",
    "    '''\n",
    "\n",
    "    filter_array = np.ones((assertions_array.shape), dtype=int)\n",
    "    filter_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for assertion_idx, assertion in enumerate(assertions_array):\n",
    "\n",
    "            filter_counter = assertion_idx\n",
    "\n",
    "            if np.all(filter_array[filter_counter] == 1):\n",
    "\n",
    "                head_entity_idx = assertion[0]\n",
    "                role_entity_idx = assertion[1]\n",
    "                filter_arr = (assertions_array[:, 0] == head_entity_idx) & (assertions_array[:, 1] == role_entity_idx)\n",
    "                relevant_assertions_idcs = np.where(filter_arr)[0]\n",
    "                relevant_assertions = torch.tensor(np.array([assertions_array[idx] for idx in relevant_assertions_idcs]))\n",
    "                filter_array[relevant_assertions_idcs] = 0\n",
    "\n",
    "                assertion_scores = []\n",
    "\n",
    "                for _, tail_entity_idx in entity_to_idx_vocab.items():\n",
    "                    eval_sample = torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]).unsqueeze(0)\n",
    "                    outputs1, outputs2 = model(eval_sample)\n",
    "                    if centroid_score == False:\n",
    "                        assertion_score = torch.dist(outputs1, outputs2, p=2)\n",
    "                    else:\n",
    "                        assertion_score = torch.dist(outputs1, outputs2, p=2) + torch.dist(outputs2, torch.tensor(GeometricInterpretation.role_geointerps_dict[idx_to_role_vocab[role_entity_idx]].centroid))\n",
    "\n",
    "                    assertion_scores.append((torch.tensor([head_entity_idx, role_entity_idx, tail_entity_idx]), assertion_score.item()))\n",
    "\n",
    "                sorted_scores = sorted(assertion_scores, key=lambda x: x[1])\n",
    "\n",
    "                k_list = [1, 3, 10, 100, len(assertion_scores)]\n",
    "                hit_k_values = []\n",
    "\n",
    "                for k in k_list:\n",
    "                    hit_k = any(torch.equal(scored_sample[0], assertion) for assertion in relevant_assertions for scored_sample in sorted_scores[:k])\n",
    "                    hit_k_values.append(hit_k)\n",
    "            \n",
    "                hits.append(hit_k_values)\n",
    "\n",
    "                top1 += int(hit_k_values[0])\n",
    "                top3 += int(hit_k_values[1])\n",
    "                top10 += int(hit_k_values[2])\n",
    "                top100 += int(hit_k_values[3])\n",
    "                top_all += int(hit_k_values[4])\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "\n",
    "    hits_at_k = [round(sum(hit_values) / len(hit_values), 3) for hit_values in zip(*hits)]  # Calculate hits_at_k for each k\n",
    "    # print(f'Hits at 1, 3, 10, 100 and all: {hits_at_k}')\n",
    "\n",
    "    return hits_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss, test_loss, num_epoch):\n",
    "    \n",
    "    plt.plot(range(1, num_epoch+1), train_loss, 'b-', label='Train Loss')\n",
    "    plt.plot(range(1, num_epoch+1), test_loss, 'r-', label='Test Loss')\n",
    "    \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Test Loss per Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_hak(hits_at_k_concept, hits_at_k_roles, topk, num_epoch, eval_freq):\n",
    "\n",
    "    concept_hits_at_topk = [score_list[topk] for score_list in hits_at_k_concept]\n",
    "    roles_hits_at_topk = [scores[topk] for scores in hits_at_k_roles]\n",
    "\n",
    "    hak_dict = {0: 1,\n",
    "                1: 3,\n",
    "                2: 10,\n",
    "                3: 100,\n",
    "                4: 'all'}\n",
    "    \n",
    "    plt.plot(range(1, num_epoch+1, eval_freq), concept_hits_at_topk, 'b-', label=f'H@{hak_dict[topk]} concepts')\n",
    "\n",
    "    try:\n",
    "        plt.plot(range(1, num_epoch+1, eval_freq), roles_hits_at_topk, 'r-', label=f'H@{hak_dict[topk]} roles')\n",
    "    except:\n",
    "        print('No roles to plot.')\n",
    "\n",
    "    plt.ylim(0, 1.02)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(f'hits@{hak_dict[topk]}')\n",
    "    plt.title(f'Hits@{hak_dict[topk]} every {eval_freq} epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(centroid_score, lr,\n",
    "               phi, emb_dim, epochs,\n",
    "               log_epoch, eval_freq,\n",
    "               eval_test, alt_train,\n",
    "               entity_centroid_init, concept_centroid_init,\n",
    "               role_centroid_init, loss_fn, model, optimizer,\n",
    "               train_loss_list, test_loss_list,\n",
    "               train_hits_at_k_concept, test_hits_at_k_concept,\n",
    "               train_hits_at_k_role, test_hits_at_k_role):\n",
    "    \n",
    "    model_hparams = {'centroid_score': centroid_score,\n",
    "                     'lr': lr,\n",
    "                     'phi': phi,\n",
    "                     'emb_dim': emb_dim,\n",
    "                     'epochs': epochs,\n",
    "                     'log_epoch': log_epoch,\n",
    "                     'eval_freq': eval_freq,\n",
    "                     'eval_test': eval_test,\n",
    "                     'alt_train': alt_train,\n",
    "                     'entity_centroid_init': entity_centroid_init,\n",
    "                     'concept_centroid_init': concept_centroid_init,\n",
    "                     'role_centroid_init': role_centroid_init,\n",
    "                     'loss_fn': loss_fn,\n",
    "                     'model': model,\n",
    "                     'optimizer': optimizer,\n",
    "                     'train_loss_list': train_loss_list,\n",
    "                     'test_loss_list': test_loss_list,\n",
    "                     'train_hits_at_k_concept': train_hits_at_k_concept,\n",
    "                     'test_hits_at_k_concept': test_hits_at_k_concept,\n",
    "                     'train_hits_at_k_role': train_hits_at_k_role,\n",
    "                     'test_hits_at_k_role': test_hits_at_k_role,\n",
    "                     'misc notes': []\n",
    "                     }\n",
    "    \n",
    "    return model_hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model, individual_vocab_idcs, concept_vocab_idcs, role_vocab_idcs, scaling_factor, dim1, dim2):\n",
    "\n",
    "    individual_embeddings = model.individual_embedding_dict.weight\n",
    "    concept_parameter_embeddings = model.concept_embedding_dict.weight\n",
    "    role_parameter_embeddings = model.role_embedding_dict.weight\n",
    "\n",
    "    individuals_for_plotting = []\n",
    "    concept_parameters_for_plotting = []\n",
    "    concept_centroid_for_plotting = []\n",
    "    role_parameters_for_plotting = []\n",
    "    role_centroid_for_plotting = []\n",
    "\n",
    "    for idx, individual in enumerate(individual_embeddings[:]):\n",
    "        individual = individual[:].detach().numpy()\n",
    "        individual_label = individual_vocab_idcs[idx]\n",
    "        final_representation = (individual, individual_label)\n",
    "        individuals_for_plotting.append(final_representation)\n",
    "\n",
    "    for idx, concept in enumerate(concept_parameter_embeddings):\n",
    "        concept_param = concept[:].detach().numpy()\n",
    "        concept_label = concept_vocab_idcs[idx]\n",
    "        final_representation = (concept_param, concept_label)\n",
    "        concept_parameters_for_plotting.append(final_representation)\n",
    "\n",
    "    for idx, key in enumerate(GeometricInterpretation.concept_geointerps_dict.keys()):\n",
    "        concept_centroid = GeometricInterpretation.concept_geointerps_dict[key].centroid[:]\n",
    "        concept_label = key + '_centroid'\n",
    "        final_representation = (concept_centroid, concept_label)\n",
    "        concept_centroid_for_plotting.append(final_representation)\n",
    "\n",
    "    for idx, role in enumerate(role_parameter_embeddings):\n",
    "        role_param = role[:].detach().numpy()\n",
    "        role_label = role_vocab_idcs[idx]\n",
    "        final_representation = (role_param, role_label)\n",
    "        role_parameters_for_plotting.append(final_representation)\n",
    "\n",
    "    for idx, key in enumerate(GeometricInterpretation.role_geointerps_dict.keys()):\n",
    "        role_centroid = GeometricInterpretation.role_geointerps_dict[key].centroid[:]\n",
    "        role_label = key + '_centroid'\n",
    "        final_representation = (role_centroid, role_label)\n",
    "        role_centroid_for_plotting.append(final_representation)\n",
    "\n",
    "\n",
    "    # Create a figure and axis object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlim(-1, scaling_factor + scaling_factor/10)\n",
    "    ax.set_ylim(-1, scaling_factor + scaling_factor/10)\n",
    "    ax.grid(True)\n",
    "\n",
    "    ax.plot(0, 0, 'yo')\n",
    "\n",
    "    # Plot individual points in blue\n",
    "    for individual, label in individuals_for_plotting:\n",
    "        ax.plot(individual[dim1], individual[dim2], 'bo', label=label)\n",
    "        ax.annotate(label, xy=(individual[dim1], individual[dim2]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    # Plot concept points in red\n",
    "    for concept_param, label in concept_parameters_for_plotting:\n",
    "        ax.plot(concept_param[dim1], concept_param[dim2], 'r+', label=label)\n",
    "        ax.annotate(label, xy=(concept_param[dim1], concept_param[dim2]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    for concept_centroid, label in concept_centroid_for_plotting:\n",
    "        ax.plot(concept_centroid[dim1], concept_centroid[dim2], 'go', label=label)\n",
    "        ax.annotate(label, xy=(concept_centroid[dim1], concept_centroid[dim2]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    # Plot role points in yellow\n",
    "    \n",
    "    for role_param, label in role_parameters_for_plotting:\n",
    "        ax.plot(role_param[dim1], role_param[dim2], 'y+', label=label)\n",
    "        ax.annotate(label, xy=(role_param[dim1], role_param[dim2]), xytext=(3, -3), textcoords='offset points')\n",
    "        \n",
    "    for role_centroid, label in role_centroid_for_plotting:\n",
    "        ax.plot(role_centroid[dim1], role_centroid[dim2], 'yo', label=label)\n",
    "        ax.annotate(label, xy=(role_centroid[dim1], role_centroid[dim2]), xytext=(3, -3), textcoords='offset points')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_concept_loader, train_role_loader,\n",
    "                test_concept_loader, test_role_loader,\n",
    "                train_concept_dset, test_concept_dset,\n",
    "                train_role_dset, test_role_dset,\n",
    "                num_epochs, loss_log_freq,\n",
    "                eval_freq, eval_train,\n",
    "                loss_function, optimizer,\n",
    "                idx_to_entity: dict, entity_to_idx: dict,\n",
    "                idx_to_concept: dict, concept_to_idx: dict,\n",
    "                idx_to_role: dict, role_to_idx: dict,\n",
    "                centroid_score = False, alt_training = False,\n",
    "                plot_loss_flag = False\n",
    "                ):\n",
    "\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "\n",
    "    train_hits_at_k_concept = []\n",
    "    test_hits_at_k_concept = []\n",
    "\n",
    "    train_hits_at_k_role = []\n",
    "    test_hits_at_k_role = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss = train(model, train_concept_loader, train_role_loader, loss_function, optimizer, alt_training)\n",
    "        train_loss_list.append(train_loss)\n",
    "        test_loss = test(model, test_concept_loader, test_role_loader, loss_function, alt_training)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if epoch % loss_log_freq == 0:\n",
    "            print(f'Epoch {epoch}/{num_epochs} -> Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f}\\n')\n",
    "\n",
    "        if epoch % eval_freq == 0:\n",
    "            print(f'Epoch {epoch}: Initiating evaluation. \\n')\n",
    "            \n",
    "            try:\n",
    "                test_hak_concept = get_hits_at_k_concept_assertions(model, test_concept_dset, test_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "                test_hits_at_k_concept.append(test_hak_concept)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if eval_train == True:\n",
    "                try:\n",
    "                    train_hak_concept = get_hits_at_k_concept_assertions(model, train_concept_dset, train_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "                    train_hits_at_k_concept.append(train_hak_concept)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            try:\n",
    "                test_hak_role = get_hits_at_k_role_assertions(model, test_concept_dset, test_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "                test_hits_at_k_role.append(test_hak_role)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            if eval_train == True:\n",
    "                try:\n",
    "                    train_hak_role = get_hits_at_k_role_assertions(model, train_concept_dset, train_role_dset, entity_to_idx, idx_to_entity, idx_to_concept, idx_to_role, centroid_score)\n",
    "                    train_hits_at_k_role.append(train_hak_role)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    if plot_loss_flag == True:\n",
    "        plot_loss(train_loss_list, test_loss_list, num_epochs)\n",
    "\n",
    "    return train_loss_list, test_loss_list, train_hits_at_k_concept, test_hits_at_k_concept, train_hits_at_k_role, test_hits_at_k_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "\n",
    "CENTROID_SCORE = True # When set to True, model scores assertion w.r.t distance to the centroid and to the moving parameter for concepts/roles\n",
    "LR = 0.01\n",
    "PHI = 0\n",
    "RADIUS = SCALE_FACTOR/2 + 0.1\n",
    "EMB_DIM = 459\n",
    "# EPOCHS = 200\n",
    "LOG_EPOCH = 100000\n",
    "# EVAL_FREQ = 100\n",
    "EVAL_TEST = True\n",
    "ALT_TRAIN = False\n",
    "PLOT_LOSS = False\n",
    "ENTITY_CENTROID_INIT = True\n",
    "CONCEPT_CENTROID_INIT = True\n",
    "ROLE_CENTROID_INIT = True\n",
    "\n",
    "DIM1 = 0\n",
    "DIM2 = 1\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(269)\n",
    "\n",
    "model = FaithEL(EMB_DIM, PHI, RADIUS,\n",
    "                entity_to_idx_vocab, concept_to_idx_vocab, role_to_idx_vocab,\n",
    "                init_individual_param_to_centroid = ENTITY_CENTROID_INIT,\n",
    "                init_concept_param_to_centroid = CONCEPT_CENTROID_INIT,\n",
    "                init_role_param_to_centroid = ROLE_CENTROID_INIT)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.role_embedding_dict.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.individual_embedding_dict.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, idx_to_entity_vocab, idx_to_concept_vocab, idx_to_role_vocab, SCALE_FACTOR, DIM1, DIM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000\n",
    "EVAL_FREQ = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list, test_loss_list, train_hits_at_k_concept, test_hits_at_k_concept, train_hits_at_k_role, test_hits_at_k_role, = train_model(model,\n",
    "                                                                                                                                           train_ConceptDataLoader, train_RoleDataLoader, test_ConceptDataLoader, test_RoleDataLoader,\n",
    "                                                                                                                                           trainConceptDataset, testConceptDataset, trainRoleDataset, testRoleDataset,\n",
    "                                                                                                                                           EPOCHS, LOG_EPOCH, EVAL_FREQ, EVAL_TEST, loss_fn, optimizer,\n",
    "                                                                                                                                           idx_to_entity_vocab, entity_to_idx_vocab,\n",
    "                                                                                                                                           idx_to_concept_vocab, concept_to_idx_vocab,\n",
    "                                                                                                                                           idx_to_role_vocab, role_to_idx_vocab,\n",
    "                                                                                                                                           CENTROID_SCORE, ALT_TRAIN, PLOT_LOSS\n",
    "                                                                                                                                           )\n",
    "\n",
    "model_list.append(save_model(CENTROID_SCORE, LR, PHI, EMB_DIM, EPOCHS, LOG_EPOCH, EVAL_FREQ, EVAL_TEST, ALT_TRAIN, ENTITY_CENTROID_INIT, CONCEPT_CENTROID_INIT, ROLE_CENTROID_INIT,\n",
    "                             loss_fn, model, optimizer, train_loss_list, test_loss_list, train_hits_at_k_concept, test_hits_at_k_concept, train_hits_at_k_role, test_hits_at_k_role))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, idx_to_entity_vocab, idx_to_concept_vocab, idx_to_role_vocab, SCALE_FACTOR, DIM1, DIM2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = -1\n",
    "TOPK = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score_hak(model_list[pos_list]['test_hits_at_k_concept'], model_list[pos_list]['test_hits_at_k_role'], TOPK, model_list[pos_list]['epochs'], model_list[pos_list]['eval_freq'])\n",
    "plot_score_hak(model_list[pos_list]['train_hits_at_k_concept'], model_list[pos_list]['train_hits_at_k_role'], TOPK, model_list[pos_list]['epochs'], model_list[pos_list]['eval_freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corruption for training with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_concept_assertions(train_data_concept_assertions,\n",
    "                               num_corrupt = int,\n",
    "                               entity_to_idx_vocab = dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "    \n",
    "    original_assertions = torch.tensor([sample[0] for sample, label in list(train_data_concept_assertions)]) # Gets rid of the individual\n",
    "\n",
    "    num_samples = len(original_assertions)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 2), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_assertions\n",
    "    corrupted_assertions[:, 1] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_role_assertions(train_data_role_assertions,\n",
    "                            num_corrupt = int,\n",
    "                            entity_to_idx_vocab=dict):\n",
    "\n",
    "    candidate_entities = list(entity_to_idx_vocab.keys())\n",
    "\n",
    "    original_head_entities = torch.tensor([sample[0] for sample, label in list(train_data_role_assertions)])\n",
    "    original_roles = torch.tensor([sample[1] for sample, label in list(train_data_role_assertions)])\n",
    "    original_tail_entities = torch.tensor([sample[2] for sample, label in list(train_data_role_assertions)])\n",
    "\n",
    "    num_samples = len(original_head_entities)\n",
    "\n",
    "    sampled_entities = torch.tensor([torch.tensor(entity_to_idx_vocab[random.choice(candidate_entities)], dtype=torch.long) for _ in range(num_samples)])\n",
    "\n",
    "    corrupted_assertions = torch.zeros((num_samples, 3), dtype=torch.long)\n",
    "    corrupted_assertions[:, 0] = original_head_entities # The original head entities\n",
    "    corrupted_assertions[:, 1] = original_roles # The original roles\n",
    "    corrupted_assertions[:, 2] = sampled_entities\n",
    "\n",
    "    return corrupted_assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
